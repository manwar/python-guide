## Parallel and Concurrent Programming

- [Learn parallel programming basics](#Learn-parallel-programming-basics)
- [What you should know](#What-you-should-know)
- [Sequential vs. parallel computing](#Sequential-vs-parallel-computing)
- [Parallel computing architectures](#Parallel-computing-architectures)
- [Shared vs. distributed memory](#Shared-vs-distributed-memory)
- [Thread vs. process](#Thread-vs-process)
- [Concurrent vs. parallel execution](#Concurrent-vs-parallel-execution)
- [Global interpreter lock: Python demo](#Global-interpreter-lock-Python-demo)
- [Multiple threads: Python demo](#Multiple-threads-Python-demo)
- [Multiple processes: Python demo](#Multiple-processes-Python-demo)
- [Execution scheduling](#Execution-scheduling)
- [Thread lifecycle](#Thread-lifecycle)
- [Thread lifecycle: Python demo](#Thread-lifecycle-Python-demo)
- [Daemon thread](#Daemon-thread)
- [Daemon thread: Python demo](#Daemon-thread-Python-demo)
- [Data race](#Data-race)
- [Data race: Python demo](#Data-race-Python-demo)
- [Mutual exclusion](#Mutual-exclusion)
- [Mutual exclusion: Python demo](#Mutual-exclusion-Python-demo)
- [Reentrant lock](#Reentrant-lock)
- [Rlock: Python demo](#Rlock-Python-demo)
- [Try lock](#Try-lock)
- [Non-blocking acquire: Python demo](#Non-blocking-acquire-Python-demo)
- [Read-write lock](#Read-write-lock)
- [Read-write lock: Python demo](#Read-write-lock-Python-demo)
- [Deadlock](#Deadlock)
- [Deadlock: Python demo](#Deadlock-Python-demo)
- [Abandoned lock](#Abandoned-lock)
- [Abandoned lock: Python demo](#Abandoned-lock-Python-demo)
- [Starvation](#starvation)
- [Starvation: Python demo](#Starvation-Python-demo)
- [Livelock](#livestock)
- [Livelock: Python demo](#Livelock-Python-demo)
- [Condition variable](#condition-variable)
- [Condition variable: Python demo](#Condition-variable-Python-demo)
- [Producer-consumer](#producer-consumer)
- [Producer-consumer threads: Python demo](#Producer-consumer-threads-Python-demo)
- [Producer-consumer processes: Python demo](#Producer-consumer-processes-Python-demo)
- [Semaphore](#Semaphore)
- [Semaphore: Python demo](#Semaphore-Python-demo)
- [Race condition](#race-condition)
- [Race condition: Python demo](#Race-condition-Python-demo)
- [Barrier](#barrier)
- [Barrier: Python demo](#Barrier-Python-demo)
- [Computational graph](#Computational-graph)
- [Thread pool](#thread-poo)
- [Thread pool: Python demo](#Thread-pool-Python-demo)
- [Process pool: Python demo](#Process-pool-Python-demo)
- [Future](#Future)
- [Future: Python demo](#Future-Python-demo)
- [Divide and conquer](#Divide-and-conquer)
- [Divide and conquer: Python demo](#Divide-and-conquer-Python-demo)
- [Speedup, latency, and throughput](#Speedup-latency-and-throughput)
- [Amdahl's law](#Amdahl's-law)
- [Measure speedup](#Measure-speedup)
- [Measure speedup: Python demo](#Measure-speedup-Python-demo)
- [Partitioning](#Partitioning)
- [Communication](#Communication)
- [Agglomeration](#Agglomeration)
- [Mapping](#mapping)
- [Welcome to the challenges](#Welcome-to-the-challenges)
- [Challenge: Matrix multiply in Python](#Challenge-Matrix-multiply-in-Python)
- [Solution: Matrix multiply in Python](#Solution-Matrix-multiply-in-Python)
- [Challenge: Merge sort in Python](#Challenge-Merge-sort-in-Python)
- [Solution: Merge sort in Python](#Solution-Merge-sort-in-Python)
- [Challenge: Download images in Python](#Challenge-Download-images-in-Python)
- [Solution: Download images in Python](#Solution-Download-images-in-Python)

***

Course:

    Part 1: https://www.linkedin.com/learning/python-parallel-and-concurrent-programming-part-1
    Part 2: https://www.linkedin.com/learning/python-parallel-and-concurrent-programming-part-2

## Learn parallel programming basics

- There's an old saying that two heads are better than one. - Well, when it comes to parallel programming I say two threads are better than one. I'm Barron Stone. - And I'm Olivia Stone. In this course, we'll introduce you to the fundamental concepts for concurrent and parallel programming. - These are the basic mechanisms you need to develop programs that can do multiple things at once, to take advantage of multi-core processors, and parallel hardware. - [Olivia] We'll focus on the general concepts and relate them back to everyday activities in the kitchen to make those ideas easier to understand and have some fun. - [Barron] Then, to cement those abstract ideas, we'll demonstrate them in action using the Python programming language. If you're new to concurrent and parallel programming, this is a great place to start. - [Together] Let's get to it.

## What you should know

- The purpose of this course is to give beginner and intermediate programmers a basic understanding of how to write concurrent and parallel programs. To get the most out of it you should have some programming experience but you don't need to be an expert. The videos of us working in the kitchen will demonstrate the key concepts in general terms, and then in the Python code examples I'll explain the unique terminology and nuances that make Python different than other programming languages. If you're interested in concurrent and parallel programming in other languages be on the lookout for different versions of this course. You'll find the same general concept videos but we'll show you the example code in another programming language.

## Sequential vs. parallel computing

- Let's start by looking at what parallel computing means and why it's useful. Why it's worth the extra effort to write parallel code. A computer program is just a list of instructions that tells a computer what to do. Like the steps in a recipe that tell me what to do when I'm cooking. Like a computer, I simply follow those instructions to execute the program. So to execute the program, or recipe, to make a salad, I'll start by chopping some lettuce and putting it on a plate. (knife chopping) Then I'll slice up a cucumber and add it. (knife slicing) Next, I'll slice and add a few chunks of tomato. (knife slicing) I'll try not to cry while I slice the onion. (knife slicing) And finally, I add the dressing. Done. As a single cook working alone in the kitchen, I'm a single processor, executing this program in a sequential manner. The program is broken down into a sequence of discrete instructions that I execute one after another. And I can only execute one instruction at any given moment. There's no overlap between them. This type of serial or sequential programming is how software has traditionally been written. And it's how new programmers are usually taught to code, because it's easy to understand, but it has its limitations. The time it takes for a sequential program to run is limited by the speed of the processor and how fast it can execute that series of instructions. I'll slice and chop ingredients as fast as I can, but there's a limit to how quickly I can complete all of those tasks by myself. Each step takes some amount of time and in total, it takes me about three minutes to execute this program and make a salad. That's my personal speed record and I can't make a salad any faster than that without help. - That's my cue. Two cooks in the kitchen represent a system with multiple processors. Now we can break down the salad recipe and execute some of those steps in parallel. - While I chop the lettuce, - I'll slice the cucumber. (knives chopping and slicing) - And when I'm done chopping lettuce, I'll slice the tomatoes. - And I'll chop the onion. (knives chopping and slicing) - And finally, I'll add some dressing. - Hold on. Now it's ready. - Finally, the dressing. - Working together, we broke the recipe into independent parts that can be executed simultaneously by different processors. While I was slicing cucumbers and onions, Barron was chopping lettuce and tomatoes. That final step of adding dressing was dependent on all of the previous steps being done. So we had to coordinate with each other for that step. By working together in parallel, it only took us two minutes to make the salad, which is faster than the three minutes it took Barron to do it alone. Adding a second cook in the kitchen doesn't necessarily mean we'll make the salad twice as fast, because adding extra cooks in the kitchen adds complexity. We have to spend extra effort to communicate with each other to coordinate our actions. - And there might be times when one of us has to wait for the other cook to finish a certain step before we continue on. Those coordination challenges are part of what make writing parallel programs harder than simple sequential programs. But that extra work can be worth the extra effort, because when done right, parallel execution increases the overall throughput of a program, enabling us to break down large tasks to accomplish them faster or to accomplish more tasks in a given amount of time. Some computing problems are so large or complex, that it's not practical or even possible to solve them with a single computer. Web search engines that process millions of transactions every second are only possible thanks to parallel computing. - In many industries, the time saved using parallel computing also leads to saving money. The advantages of being able to solve a problem faster often outweighs the cost of investing in parallel computing hardware.

## Parallel computing architectures

- Parallel computing requires parallel hardware with multiple processors to execute different parts of a program at the same time. But before you dive into writing software, it helps to understand how different types of parallel computers are structured. One of the most widely used systems for classifying multiprocessor architectures is Flynn's Taxonomy, which distinguishes four classes of computer architecture based on two factors: the number of concurrent instruction or control streams, and the number of data streams. The class names are usually written as four letter acronyms that indicate whether they have single or multiple instruction streams and data streams. For example, SIMD stands for Single Instruction, Multiple Data. The simplest of these four classes is the Single Instruction, Single Data or SISD architecture which is a sequential computer with a single processor unit. If I'm an SISD computer, at any given time I can only execute one series of instructions, such as chopping, and I can only act on one element of data at a time, this carrot. (knife chops) - It's simple, like an old computer. The next class in Flynn's Taxonomy is Single Instruction, Multiple Data, or SIMD, which is a type of parallel computer with multiple processing units. All of its processors execute the same instruction at any given time, but they can each operate on different data element. As an SIMD computer, our two processors are both executing the same chopping instruction. But I'm chopping celery as my data while Barron chops a carrot. - And we'll execute those instructions in sync with each other. (two knives chop) This type of SIMD architecture is well suited for applications that perform the same handful of operations on a massive set of data elements like image processing. And most modern computers use graphic processing units, or GPUs, with SIMD instructions, to do just that. Our third class is the opposite of SIMD. In a Multiple Instruction, Single Data or MISD architecture, each processing unit independently executes its own separate series of instructions, however, all of those processors are operating on the same single stream of data. That's like Olivia executing the chopping instruction while I execute a different peeling instruction, but we're both chopping and peeling the same carrot at the same tine. - Yeah, we're not doing that. - As you can see, MISD doesn't make much practical sense so it's not a commonly used architecture. - But our fourth and final architecture is. In a Multiple Instruction, Multiple Data, or MIMD computer every processing unit can be executing a different series of instructions, and at the same time each of those processors can be operating on a different set of data. Now I can slice celery while Barron peels carrots. (knives peel and slice) - MIMD is the most commonly used architecture in Flynn's Taxonomy, and you'll find it in everything from multi-core PCs to network clusters in supercomputers. Now that broad MIMD category is sometimes further subdivided into two parallel programming models which also have four letter names. Single Program, Multiple Data, or SPMD, and Multiple Program, Multiple Data, MPMD. In the SPMD model, multiple processing units are executing a copy of the same single program simultaneously, however, they can each use different data. That might sound a lot like the SIMD architecture from earlier, but it's different, because although each processor is executing the same program, they do not have to be executing the same instruction at the same time. The processors can run asynchronously and the program usually includes conditional logic that allows different tasks within the program to only execute specific parts of the overall program. If Olivia and I are both following the same recipe or program, I can execute part of it, while Olivia's processor handles a different task. This SPMD model is the most common style of parallel programming, and when we show you programming examples later in this course we'll structure the code as a single program and execute it on a multi-core desktop computer which is an MIMD architecture. - Now if each of our processors is executing a different recipe, that represents the Multiple Program, Multiple Data, or MPMD model. In this scenario, processors can be executing different independent programs at the same time while of course also be operating on different data. Typically in this model one processing node will be selected as the host, or manager, which runs one program that farms out data to the other nodes running a second program. Those other nodes do their work and return their results to the manager. MPMD is not as common as SPMD but it can be useful for some applications that lend themselves to functional decomposition, which we'll cover later on.

## Shared vs. distributed memory

- In addition to a parallel computer's architecture, which can be categorized using Flynn's Taxonomy, another aspect to consider is, uh- - Memory, it's important to understand how the memory's organized and how the computer accesses data. - Right, you could put a billion processors in a computer but if they can't access memory fast enough to get the instructions and data they need then you won't gain anything from having all those processors. Computer memory usually operates at a much slower speed than processors do, and when one processor is reading or writing to memory that often prevents any other processors from accessing that same memory element. There are two main memory architectures that exist for parallel computing, shared memory and distributed memory. In a shared memory system all processors have access to the same memory as part of a global address space. Although each processor operates independently, if one processor changes a memory location all of the other processors will see that change. So if I change something in our shared memory space- - Hey that potato is two potatoes. - Every other processor sees that change, too. Now the term shared memory doesn't necessarily mean all of this data exists on the same physical device. It could be spread across a cluster of systems. The key is that both of our processors see everything that happens in the shared memory space. Shared memory is often classified into one of two categories: uniform memory access and non-uniform memory access, which are based on how the processors are connected to memory and how quickly they can access it. In a uniform memory access or UMA system, all of the processors have equal access to the memory, meaning they can access it equally fast. There are several types of UMA architectures, but the most common is a symmetric multiprocessing system or SMP. An SMP system has two or more identical processors which are connected to a single shared memory, often through a system bus. In the case of modern multi-core processors, which you find in everything from desktop computers to cell phones, each of the processing cores are treated as as a separate processor. For this course we'll be focused on parallel programming within the SMP architecture. In the example code we show you we'll be running on a multi-core desktop computer. Now in most modern processors each core has its own cache, which is a small, very fast piece of memory that only it can see. And it uses it to store data that it's frequently working with. However, caches introduce the challenge that if one processor copies a value from the shared main memory, and then makes a change to it in its local cache, then that change needs to be updated back in the shared memory before another processor reads the old value which is no longer current. This issue called cache coherency is handled by the hardware in multi-core processors so we will not go into detail on it for this course, but it's something you should be aware of if you find yourself working with larger, more complex parallel computing systems. The other type of shared memory is a non-uniform memory access or NUMA system, which is often made by physically connecting multiple SMP systems together. The access is non-uniform because some processors will have quicker access to certain parts of memory than others. It takes longer to access things over the bus. But overall, every processor can still see everything in memory. These shared memory architectures have the advantage of being easier for programming, in regards to memory, because it's easy to share data between different parts of a parallel program. The downside is that they don't always scale well. Adding more processors to a shared memory system will increase traffic on the shared memory bus. And if you factor in maintaining cache coherency it becomes a lot of communication that needs to happen between all the parts. In addition to that, shared memory puts responsibility on the programmer to synchronize memory accesses to ensure correct behavior. But we'll look into that later. - Okay, that's enough about shared memory. In a distributed memory system, each processor has its own local memory with its own address space. So the concept of a global address space doesn't exist. All of the processors are connected through some sort of network, which can be as simple as ethernet. Each processor operates independently, and if it makes changes to its local memory that change is not automatically reflected in the memory of other processors. If I make a change to the data in my memory, Baron's processor is oblivious to that change, it's up to the programmer to explicitly define how and when data is communicated between the nodes in a distributed system, and that's often a disadvantage. - Communication is always tough. - The advantage of a distributed memory architecture is that it's scalable. When you add more processors to the system you get more memory, too. This structure makes it cost-effective to use commodity, off-the-shelf computers, and networking equipment to build large distributed memory systems. Most supercomputers use some form of distributed memory architecture, or a hybrid of distributed and shared memory. - But for this course we'll stick with simple shared memory in an SMP architecture.

## Thread vs. process
- When a computer runs an application that instance of the program executing is referred to as a process. A process consists of the program's code, its data, and information about its state. Each process is independent and has its own separate address space and memory. A computer can have hundreds of active processes at once, and an operating system's job is to manage all of them. - Now, within every process there are one or more smaller sub-elements called threads. These are kind of like tiny processes. Each of those threads is an independent path of execution through the program. A different sequence of instructions. And it can only exist as part of a process. Threads are the basic units that the operating system manages, and it allocates time on the processor to actually execute them. - To conceptualize the relationship between a process and its threads, think of Olivia and I cooking together in the kitchen as being two threads executing as part of the same process. - We both work independently doing our own tasks that contribute to the overall execution of our program. For example, if we are part of a process to make a salad, my thread might handle retrieving vegetables from the pantry and fridge. - And I'll handle chopping them up. If the program requires other tasks, we might create additional threads to handle those those too. Hey Steve! - Hey! - Can you make some salad dressing? - I sure can! - Now our salad-making process has three active threads. And when one of those threads finishes executing its instructions-- - All done. - It'll exit and leave the remaining threads to continue what they're doing. - Threads that belong to the same process share the process' address space which gives them access to the same resources and memory including the program's executable code and data. You can think of the kitchen that our two threads are working in like the shared address space for our process. We both have direct access to the same cookbooks containing our instructions or code, and the ingredients that we're cooking with represents the data and variables we're manipulating. The ability for both of us to use these resources is certainly convenient. And it enables us to easily work together. But it does create the potential to cause problems if we don't coordinate our actions, as we'll see later in this course. - Sharing resources between separate processes is not as easy as sharing between threads in the same process. Because every process exists in its own address space, its own separate kitchen. In this process, our two threads are making a salad. But that other process next door is running a different program, those threads are baking a cake. - Our variables and data are isolated to this address space, this kitchen, so the threads in the other process can't directly access our salad data. - Good! We don't want you healthy salad-makers messing with our cake either! - There are ways to communicate and share data between processes, but it requires a bit more work than communicating between threads. - You have to use system-provided Inter-Process Communication mechanisms like sockets and pipes, allocating special inter-process shared memory space, or using remote procedure calls. Which is beyond the scope of what we'll be discussing in this course. - Now it's possible to write parallel programs that use multiple processes working together towards a common goal, or using multiple threads within a single process. - So which is better? Using multiple threads or multiple processes? - Well, like most things in programming, it depends. It depends on what you're doing and the environment it's running in. Because the implementation of threads and processes differs between operating systems and programming languages. If your application is going to be distributed across multiple computers, you most likely need separate processes for that. But, as a rule of thumb, if you can structure your program to take advantage of multiple threads, stick with using threads rather than multiple processes. Threads are considered light-weight compared to processes, which are more resource-intensive. A thread requires less overhead to create and terminate than a process, and it's usually faster for an operating system to switch between executing threads from the same process than to switch between different processes.

## Concurrent vs. parallel execution
- Just because a program is structured to have multiple threads or processes does not mean they'll necessarily execute in parallel. A concept that's closely related to parallel execution, but often gets confused with it, is concurrency. Concurrency refers to the ability of an algorithm or program to be broken into different parts that can be executed out of order, or partially out of order, without affecting the end result. Concurrency is about how a program is structured and the composition of independently executing processes. Consider this recipe to make a salad, which includes several steps that involve slicing and chopping vegetables. We can decompose those steps into a collection of concurrent tasks, because the relative order in which we do them doesn't matter, they're order independent. To keep things simple, let's just focus on two of those tasks for now. I'll chop onions. - And I'll slice cucumbers. This knife represents our computer's processor. We only have one knife, so, this is a single-core processor, and only one of us will be able to execute our vegetable-chopping routine at any given time. - We'll have to take turns, you go first. - Thanks. My thread will use the processor to execute and slice some cucumbers. Then, after a bit, we'll swap places. - Now my thread gets some time to execute and slice onions. - I want to slice now. - So, we'll swap places again, and we'll keep on doing this until we're both done. In this scenario, we're running concurrently, because our two independent processes overlap in time. However, since we only have a single processor, only one of us will actually be executing at any instant in time. If we swap places and take turns more frequently, it might create the illusion that we're executing simultaneously on our single processor, but this is not true parallel execution. - To actually execute in parallel we need parallel hardware. In our kitchen, that means another knife and cutting board, a second processor, but in regards to computers, parallel hardware can come in a variety of forms. Most modern processors used in things like desktop computers and cell phones have multiple processing cores. Graphics processing units, or GPUs, contain hundreds, or even thousands, of specialized cores working in parallel to make amazing graphics that you see on the screen. And computer clusters distribute their processing across multiple systems. Since we've structured ourselves as concurrent operations, I can begin slicing cucumbers with this processor. - While I cut onions with the other one. Now we're actually executing in parallel because we're both executing at the same time, and as a result, we're able to finish the job faster. Concurrency is about the structure of a program being able to deal with multiple things at once, whereas parallelism is about simultaneous execution, actually doing multiple things at once. Those things could be related, like chopping vegetables, but they don't have to be. Concurrency enables a program to execute in parallel, given the necessary hardware, but a concurrent program is not inherently parallel. - And programs may not always benefit from parallel execution. For example, the software drivers that handle I/O devices, like a mouse, keyboard, and hard drive, need to execute concurrently. They're managed by the operating system as independent things to get executed as needed. In a multi-core system, the execution of those drivers might get split amongst the available processors. However, since I/O operations occur rather infrequently, relative to the speed at which computer operates, we don't really gain anything from parallel execution. Those sparse independent tasks could run just fine on a single processor, and we wouldn't feel a difference. Concurrent programming is useful for I/O-dependent tasks, like graphical user interfaces. When the user clicks a button to execute an operation that might take awhile, to avoid locking up the user interface until it's completed, we can run the operation in a separate concurrent thread. This leaves the thread that's running the UI free to accept new inputs. - That sort of I/O-dependent task is a good use case for concurrency. Parallel processing really becomes useful for computationally-intensive tasks, such as calculating the result of multiplying two matrices together. When large math operations can be divided into independent subparts, executing those parts in parallel on separate processors can really speed things up.

## Global interpreter lock: Python demo

- Using threads to handle concurrent tasks in Python is fairly straightforward. However, the Python interpreter will not allow those concurrent threads to execute simultaneously and parallel, due to a mechanism called the global interpreter lock, or GIL. It's something that's unique to Python and important to address up front. The GIL is a mechanism in Python that prevents multiple Python threads from executing at the same time. That means if your program is written to have 10 concurrent threads, only one of them can execute at a time while the other nine wait their turn. This may seem like an odd limitation, but remember that under the hood, your Python program is actually being executed by a program called an interpreter. The interpreter compiles your Python program into an intermediate bytecode which is then executed with a virtual machine along with any necessary modules from the library. The default and by far most widely used interpreter is CPython, and as its name suggests, its underlying code is written in a mix of C and Python. The global interpreter lock was implemented as a simple way to provide thread-safe memory management in CPython by only letting one Python thread execute at a time. There have been several proposals to eliminate the GIL from CPython because of its negative impact on parallel performance, but in most cases, the advantages of having the GIL outweigh its disadvantages. There are other implementations of Python that do not have the GIL including Jython, which is Java based, IronPython, which is .NET based, and PyPy-STM. But CPython is the default and most popular interpreter so the GIL lives on as a contentious element of Python. Now just because CPython has the GIL does not mean there's no value in writing multi-threaded Python programs. Many applications are I/O-bound, meaning they're mostly waiting on external actions, like network operations or user input. For those types of I/O-intensive tasks, the GIL does not create a significant bottleneck and you can gain a lot by using Python's threading module to implement multiple concurrent threads. On the other hand, if your application is CPU bound and spends most of its time performing CPU-intensive computations, then the GIL can negatively impact multi-threaded performance, but there are ways to get around it. Because Python is an interpreted language, it's inherently slow, so processor-heavy algorithms are often written using faster compiled languages like C++. And then included in function libraries that a top-level Python program can call into. Those operations can then execute outside of the GIL's restrictions using parallel threads. If you want to keep everything written in Python, the workaround is to use Python's multiprocessing package to implement your program with multiple processes instead of multiple threads. Each Python process will be its own instance of the Python interpreter with its own GIL, so the separate processes can execute in parallel. The downside here is that communication between processes is a bit more complicated than between threads, and creating multiple processes uses more system resources than creating multiple threads. For most of the Python examples in this course, we'll stick with using the threading module to demonstrate the programming concepts with concurrent threads. As you'll see, even with the GIL restricting execution, multi-threaded Python programs can still run into a variety of problems that you'll need to protect against. The GIL does not exist to keep you safe. When we reach examples later in this course that involve more CPU-intensive tasks, we'll switch over to using the multiprocessing package so you can see how to use that, as well.

## Multiple threads: Python demo

- We'll start with a demonstration, using Python's threading module to create several concurrent threads and investigate their impact on this computer's CPU usage. But before diving into code, let's first take a look at the number of processors that are available on this computer, which I'll be using for demonstrations throughout this course. To do that, I'll press control, shift, escape to open the task manager, and then select the performance tab. Down at the bottom, I can see that this computer has 12 cores and 24 logical processors. Those numbers mean this computer has 12 separate, complete physical processing cores, and each of those cores supports something called hyper-threading, which enables them to each run two independent applications, at the same time. So the computer treats those 12 physical cores, as 24 logical processors. Now, the hyper-threading in those 12 cores, does not mean I'll get double the performance out of them. Hyper-threading takes advantage of unused parts of the processor, so if one thread is paused, or not using a specific resource, then the other thread may be able to use it. Under certain work loads, that can create performance improvements, but it's highly application dependent. The blue, moving graph shows the total percentage of CPU utilization for all of those processors. I don't have much running right now, so the usage stays low, down near one per cent. If I want to see the CPU usage for each of those processors individually, I can get more information by clicking the open resource monitor link at the bottom of the task manager. And selecting the CPU tab. The charts on the right show the total CPU usage on top, and if I scroll down, I can see how much each of the individual processors on this computer are being utilized. The table on the left lists all of the current processes running on this computer with information including each processes unique process ID number, its PID, its current status, the number of threads, and it's average CPU usage. Now, to show you a few Python threads running in a process, I've created a short example program, which you can find in exercise files, chapter 2, oh 2 oh 4, end, multiple threads, dot py. This program defines a simple function on line 8, called CPU waster, which has a while loop that will spin forever. It doesn't do any useful work, but the thread running that function will stay alive forever and continuously use CPU cycles. Lines 13 through 16 print out information about the program, including its process ID number, the total number of threads in the process, and then the for loop on line 15 prints information about each of those threads. After that, it creates and starts 12 time waster threads, using a for loop on line 19. I chose to start 12 threads here, because that's half as many processors as there are in this system. Don't worry about how the code on line 20 works for now, we'll get to that in a later video. Finally, after starting those threads, the program prints out the same process information again, on lines 23 through 26. Now, I'll switch over to a command prompt, that's already navigated to the folder with that script, and I'll type Python, multiple threads dot py, to run it. I can see that the operating system assigned this process the ID number 11316, and when the process is initially started, it only has one main thread of execution. Then, after starting 12 CPU waster threads, the process ID is the same, but now the program has 13 total threads. When Python created each of those additional threads, it gave them a name of thread dash one, two, three, and so on. As well as a unique thread identifier number. Now, if I switch over to the task manager, I can see that the overall CPU usage has increased compared to before I ran the program. But, overall, it's pretty low, considering I have 13 threads running. And that's because Python's global interpreter lock is only allowing one of those threads to actually execute, at any given moment. So at most, this program can only utilize one CPU worth of resources. I'll switch over to the resource monitor and I can see that the work load is being distributed across those processors. It's not being limited to only running on a single one of the available processors. Now, looking over at the processes tab, I see a process called Python.exe, which has the same process ID number, that my program displayed earlier. And, in the thread columns, it has 13 threads. In the average CPU column, I see this processing is using 4.15 per cent of the CPU, which corresponds to utilizing one of the 24 logical processors in the computer. This program will continue running forever, so I'll manually terminate it by pressing control and the break key.

## Multiple processes: Python demo

- To leverage multiple processors, and to achieve true parallel execution in python, rather than structuring our program to use multiple threads, we'll need to use multiple processes. Fortunately, Python's multiprocessing package makes that pretty straight forward, because it provides an API for spawning additional processes that looks very similar to the threading module. To demonstrate just how similar they are, I'll modify the code from the previous example, which created several threads running the CPU waster function to use several processes instead. First, I'll need to import the multi`processing module. And, since that's a long word, I'll rename it to mp for short. Next, on line 21, I'll replace the threading modules thread class with the multiprocessing packages process class, and finally, when using the multiprocessing package to spawn additional processes, it's important to enclose the main body of the program, in an if statement. To check if the special name variable is equal to main, and I'll explain why this is important later. That's all it takes to convert this program from using multiple threads to multiple processes. I've pulled up a command prompt, alongside the task manager so I can view the CPU usage as that program runs. I'll run it by typing in python multiple processes.py and I can see that the CPU usage quickly rises to around 50% because now those 12 additional processes I created are all executing in parallel. The output from the program, which was printed by the main process, shows that it started with a process ID of 10312 and one thread. Then, after I started 12 CPU Wasters, that same process still only has one thread because the CPU Wasters are now running as separate processes. If I bring up the resource monitor, sort by process name, and then scroll down, I can see there are 13 processes named python.exe. The one on the bottom has the same process ID as the main program, and I can see that it's not using any CPU resources because after kicking off the 12 CPU Wasters processes and printing it's message, the main program is basically done. The other 12 python processes are all executing the CPU Waster function in parallel, so each is utilizing about 4% of the CPU resources. I'll switch back to the combined CPU usage overview on the task manager and then I'll end this program by pressing control break. As those processes are terminated, the CPU usage quickly falls back down to near zero. Now, let's switch back to code and talk about why the main program needed to be put within the if statement on line 12. Let me just add an extra space here. When this script creates and starts a new process on line 22, it tells it to execute the CPU Waster function. However, that new python process doesn't jump straight into the CPU Waster function, because it doesn't know what that is. It needs to run through the entire script first to find the CPU waster function and become aware of any other dependencies. To show that happening, I'll add a print statement outside of the CPU Waster function, but before the if statement to check the name. I'll have my program say hi my name is, and then print out the special name variable. So back in the console, I'll press the up key to bring up the command to run this program again, I see a hello message at the top of the output. That's from my initial main process as it works its way through the script. Then down at the bottom, I have 12 other hello messages from mp main. Python gives those processes that I spawn a different name than the main module so that I can tell them apart. Looking back at the code, if I did not include the if statement on line 14, then all of those processes I spawned would execute the section of code within it, including line 23 which spawns more processes. A process that uncontrollably tries to spawn more and more processes is a problem and python will actually throw an error if you do not include the if name as main line, which prevents that from happening.

## Execution scheduling

- Threads don't just execute whenever they want to. A computer might have hundreds of processes with thousands of threads that all want their turn to run on just a handful of processors. So how do they decide who goes first? - That's the operating system's job. The OS includes a scheduler that controls when different threads and processes get their turn to execute on the CPU. The scheduler makes it possible for multiple programs to run concurrently on a single processor. When a process is created and ready to run, it gets loaded into memory and placed in the ready queue. Think of these as cooks in the kitchen that are ready to work. The scheduler is like the head chef that tells the other cooks when they get to use the cutting board. It cycles through the ready processes so they get a chance to execute on the processor. If there are multiple processors, then the OS will schedule processes to run on each of them to make the most use of the additional resources. A process will run until it finishes, and then a scheduler will assign another process to execute on that processor. Or, a process might get blocked and have to wait for an I/O event, in which case it'll go into a separate I/O waiting queue so another process can run. Or, the scheduler might determine that a process has spent its fair share of time on the processor, and swap it out for another process from the ready queue. When that occurs, it's called a context switch. The operating system has to save the state or context of the process that was running so it can be resumed later, and it has to load the context of the new process that's about to run. If I'm a process that's executing on this processor to chop cucumbers, when a scheduler decides it's time for a context switch, I'll need to pause what I'm doing and store the state of that task. - And as the new process that just got scheduled, I'll load my state information and then begin executing. (chopping) Now, context switches are not instantaneous. It takes time to save and restore the registers and memory state, so the scheduler needs a strategy for how frequently it switches between processes. There's a wide variety of algorithms that different operating system schedulers implement. Some of these algorithms are preemptive, which means they may pause or preempt a running, low-priority task when a higher priority task enters the ready state. In non-preemptive algorithms, once a process enters the running state, it'll be allowed to run for its allotted time. Which algorithm a scheduler chooses to implement will depend on its goals. Some schedulers might try to maximize throughput, or the amount of work they complete in a given time, whereas others might aim to minimize latency, to improve the system's responsiveness. Different operating systems have different purposes, and a desktop OS like Windows will have a different set of goals and use a different type of scheduler than a real-time OS for embedded systems. Now, while it's important to understand the concept of scheduling, and that it's taking place, you usually don't need to worry about the nitty-gritty details of how the scheduler works because it's often handled under the hood by the operating system. In fact, you might not have any control over when the parts of your program actually execute. - And that's an important thing to keep in mind. Avoid running programs expecting that multiple threads or processes will execute in a certain order, or for an equal amount of time, because the operating system may choose to schedule them differently from run to run.

## Thread lifecycle

- When a new process or program begins running, it will start with just one thread, which is called the main thread because it's the main one that runs when the program begins. That main thread can then start or spawn additional threads to help out, referred to as its child threads, which are part of the same process but execute independently to do other tasks. Those threads can spawn their own children if needed and as each of those threads finish executing they'll notify their parent and terminate with the main thread usually being the last to finish execution. Over the life cycle of a thread, from creation through execution and finally termination, threads will usually be in one of four states. If I'm the main thread in this kitchen and I spawn or create another thread to help me, that child thread will begin in the new state. - Hello! - This thread isn't actually running yet so it doesn't take any CPU resources. - I don't even know what I'm supposed to be doing. - Part of creating a new thread is assigning it a function, the code it's going to execute. Olivia, I need you to slice these sausages. We're making soup. - I can do that. I'm ready to start. - Right, you can start now. Some programming languages require you to explicitly start a thread after creating it. - [Olivia] Now that I've started, I'm in the runnable state, which means the operation system can schedule me to execute. Through contact switches, I'll get swapped out with other threads to run on one of the available processors. - Olivia is running independently now, so my thread is free to continue executing my own tasks when it's my turn to get scheduled on the processor. - Aw man, this sausage is frozen. I need to wait for it to thaw before I can continue. When a thread needs to wait for an event to occur, like an external input or a timer, it goes into a blocked state while it waits. The good thing is that while I'm blocked I'm not using any CPU resources. The operating system will return me to the runnable state when the sausage is thawed. - And that frees up the processor for other threads to use. Now my thread may eventually reach a point where I need to wait until one of my children threads has finished for me to continue on. Maybe I've finished preparing everything else. I've completed all of my tasks and I need Olivia to finish slicing the sausage. I can wait for her thread to complete its execution by calling the join method. When I call join, my thread will enter a blocked state waiting until Olivia's done. - Ah, the sausage is finally thawed. Now I'll go back to the runnable state and continue executing. (knife scraping) Now I've finished executing so I'll notify my parent thread that I'm done. Hey Baron, I'm done. And then I'll enter the final terminated state. (Olivia sighing) A thread enters the terminated state when it either completes its execution or it's abnormally aborted. - Since Olivia notified me that she's done, I'll return to the runnable state so I can continue preparing soup. Now, different programming languages may use different names for their states and have a few additional ones, but in general, new, runnable, blocked, and terminated are the four phases of the life cycle of a thread.

## Thread lifecycle: Python demo

- To demonstrate the life cycle of a Python thread, from creation to termination, we've created this example program, which recreates the interactions between Olivia and me, where I spawned her as a second thread to help slice sausages to make soup. Now there are two ways to create a thread and specify its activity in Python. In the previous Python examples, we put the code for our thread to execute into a function. And then passed that function to the thread constructor method as a callable object using the target parameter. The other way to create a thread in Python is to define a custom subclass that inherits from the thread class and overrides its run method. That second approach is what we've done with the class named ChefOlivia on line seven. It inherits from threading.Thread and overrides two of its methods, init and run. These are the only two methods you should override from the thread class. Within the init method, we simply use the super function to execute the parent thread class' init method on line nine. Python will raise an error if you don't do that. The run method below it contains the code to execute when the thread is started. On line 12, ChefOlivia will print a message that she's started and is waiting for the sausage to thaw. Then she waits for three seconds and then finally prints a message that she's done cutting sausage. After printing her last message on line 14, the ChefOlivia thread will terminate because its run method is finished. Down below that, the program's main thread represents me, Barron, in the kitchen. It starts by printing a message on line 18 that Barron has started and requests Olivia's help and then creates a new ChefOlivia object. At this point the ChefOlivia has not been started, so Barron tells her to start by calling her start method on line 22. And then after that, Barron continues cooking soup, which is represented by sleeping for half a second on line 25. Since Barron only sleeps for half a second after staring Olivia's thread, but Olivia sleeps for three whole seconds, Barron will be done well before Olivia. But he needs to wait until she's finished to continue on. This is where the join method comes into play, causing Barron to wait until after Olivia has completed everything she needs to do before he can continue on. Barron calls the join method on line 28. Notice that we're calling the join method on the Olivia thread object from within the main Barron thread. That'll block Barron's execution at that point until Olivia terminates. After Olivia's thread terminates, the main Barron thread will be able to continue on to print its final message on line 30 that they're both done. We'll run that program in the console by typing Python thread_lifecycle.py and pressing enter. I get these series of messages as output. Messages that begin with the word Barron were printed by the main thread. And messages that begin with Olivia came from the second ChefOlivia thread. After Barron told Olivia to start, Olivia printed her first message and then began to sleep for three seconds. Barron continued independently cooking soup for half a second and then waited for Olivia to join. Olivia finally finished which allowed Barron to continue, print its last message and terminate the program. Moving back to code, we can actually watch the status of Olivia's thread change throughout this sequence, by adding print statements with the somewhat morbid sounding is alive method, which returns a Boolean indicating whether or not that thread is alive. I'll add that immediately after creating the ChefOlivia thread object. Print Olivia alive? And then olivia.is_alive. And then open and closed parentheses. Notice that I've included an extra space on the front to make it easier to read among the other output text. Now I'll copy and paste this line, again after I start the Olivia thread. Then after the main thread is done with its half second sleep, Olivia should still be sleeping, so I'll add a third print statement here. And finally after Olivia joins, I'll print whether or not she's alive one final time. Back in the console, I'll press up to run this program again. And I get those alive messages. I see that Olivia's thread is initially not alive after first being created. Then after the Barron thread calls her start method, she is alive. Even though Olivia's thread is sleeping, at the time of the third is alive message, Python considers her thread to still be alive. But after Olivia terminates and joins the main method, the last call to the is alive method returns false.

## Daemon thread

- We often create threads to provide some sort of service or perform a periodic task in support of the main program. A common example of that is garbage collection. A garbage collector is a form of automatic memory management that runs in the background and attempts to reclaim garbage or memory that's no longer being used by the program. Many languages include garbage collection as a standard part of their runtime environment, but for this demonstration I'll spawn my own new thread to handle garbage collection. - Man, what a mess. - Olivia is a separate child thread that will execute independently of my main thread. So, I can continue doing what I'm doing here, getting my soup ingredients ready. - While I try to reclaim some memory, or counter space, by clearing up Barron's garbage. - This setup, with Olivia running as a separate thread to provide that garbage collection service, will work fine until I'm ready to finish executing. Bam! Now my soup's spiced and ready, my main thread is done executing, and I'm ready to exit the program. But I can't. - Because I'm still running. Since Barron spawned me as a normal child thread, he won't be able to exit until I have terminated. - And since Olivia's thread is designed to collect garbage in a continuous loop, she'll never exit. I'll be stuck here waiting forever and this process will never terminate. Threads that are performing background tasks, like garbage collection, can be detached from the main program by making them what's called a demon thread. A demon thread, which you may also hear pronounced as daemon, is a thread that will not prevent the program from exiting if it's still running. By default, new threads are usually spawned as non-demon or normal threads and you have to explicitly turn a thread into a demon or background thread. Olivia, I forgot to tell you this earlier but, you're a demon thread. - Oh man, you detached me. - When my main thread is finished executing and there aren't any non-demon threads left running, this process can terminate. (thudding) And Olivia's demon thread will terminate with it. - Since I was terminated abruptly with the process, I didn't have a chance to gracefully shut down and stop what I was doing. That's fine in the case of a garbage collection routine because all of the memory this process was using will get cleared as part of terminating it. But if I was doing some sort of IO operation, like writing to a file, then terminating in the middle of that operation could end up corrupting data. If you detach a thread to make it a background task, make sure it won't have any negative side effects if it prematurely exits.

## Daemon thread: Python demo

- In this program, to demonstrate a daemon thread, I defined a function called kitchen cleaner on line seven, which represents a periodic background task like garbage collection. The kitchen cleaner uses an infinite while loop to continuously print a message that Olivia cleaned the kitchen once every second. Down in the program's main section, I create and start a new kitchen cleaner thread named Olivia on lines 13 and 14. Then the main thread prints a series of messages that Barron is cooking, which are split up by sleep statements and then finally a message that Barron is done on line 22. Switching over to the console, I'll type python daemon_thread.py. And press enter to run this program. And I see those messages from Barron and Olivia displayed. But after the main thread reaches the end, and prints its final Barron is done message, the program doesn't exit because the kitchen cleaner thread is still going strong. And it will continue to run forever. I'll stop it by pressing control+break. Back in the code, to prevent that from happening, I'll set Olivia to be a daemon thread before she gets started by typing olivia.daemon equals true. To set the thread object's daemon property to true. I'll save that change. And switch back to the console. And when I run that program again, now when the main thread is done executing, Olivia's thread is also terminated so the process can exit. A few things to note here. When a new thread is created, it'll inherit the daemon status from its parent. The main thread is normal non-daemon thread, so by default, any threads that it creates will be non-daemon threads. You must set the daemon property to configure a thread to be daemon or non-daemon before starting it. Otherwise Python will raise runtime error. Finally, daemon threads do not gracefully exit like normal threads. When all of the non-daemon threads in a program are done executing, any remaining daemon threads will be abandoned as Python exits.

## Data race
- One of the main challenges of writing concurrent programs is identifying the possible dependencies between threads to make sure they don't interfere with each other and cause problems. Data races are a common problem that can occur when two or more threads are concurrently accessing the same location in memory and at least one of those threads is writing to that location to modify its value. Fortunately you can protect your program against data races by using synchronization techniques which we'll show you later but to eventually use those techniques you'll first need to know how to recognize the data race. Olivia and I are two concurrent threads working together to figure out what we need to buy from the grocery store. I'll take inventory of the pantry and when I see that we're running low on something, I'll add more of that item to our shared shopping list. - And while Ben does that, I'll look through my recipe book and I'll add ingredients to our shopping list for the meals I wanted us to cook this week. - Even though we're two separate threads doing different tasks, we run the risk of a data race because we're both accessing and modifying the same shared resource, our shopping list. Now to the pantry! - Oh, this garlic mash potato recipe looks delicious I'll need five potatoes for it. I see that our shopping list already has three potatoes on it. Three plus five is eight so I'll erase three and write down eight. As you can see even a simple operation like adding two numbers is actually a multiple step process. First I had to read the existing value of that item from the shopping list then I modified the value by adding what I needed to it and finally I wrote the result back to the shopping list. - It looks like we're running low on garlic in the pantry. I think we should restock it with two more cloves. I see that there's currently one clove of garlic on the list one plus two is, err ... err - My garlic mash potato recipe calls for five cloves of garlic. I see there's currently one clove of garlic on the list. One plus five is six so I'll update the list to have six cloves on it. - Three! One plus two is three. We need three cloves of garlic.

## Data race: Python demo

- To demonstrate a data race and code, we've created this simple Python program that uses two threads to concurrently increment a shared variable. The variable on line six is a counter for the amount of garlic we should buy, and gets initialized to zero. Below that, the shopper function defined on line eight represents a shopper adding garlic to the shopping list. It uses the for loop on line 10 to increment that global count variable 10 times. Down in the program's main section, starting on line 14, I create two shopper threads called barron and olivia, start them both and then use the join method to wait until they're both done. Finally, I print out the value of the garlic count variable on line 20, indicating how much garlic we should buy. Switching over to the console I'll run this program with the command python data_race.py. And the output tells me we should buy 20 garlic. That makes sense because I have two threads that are each adding 10 garlic to the counter. And if I press the up arrow to recall that command and run this program several more times, there's a reasonable chance that I'll continue to get that correct answer of 20. It's not guaranteed, but with each shopper only incrementing the count variable 10 times, there are not a whole lot of opportunities for the data race to cause a problem. So to help make trouble, I'll switch back to the code and I'm going to modify the for loop so that each of the shopper increments that count variable 10 million times. Now the expected output should be 20 million garlic. But when I save that change and run the program again I get a value that's way lower than that, only around 11 million. And if I run the program one more time, I get a different, incorrect value. Even though the simple garlic count plus equal one operation is only a single line of code in the background, the computer is actually performing a three step read, modify, write process. My two concurrent threads end up stepping on each other's toes, and unintentionally overriding each other's changes, which produces an incorrect result. When you realize that one of your programs has a database, grab a cup of coffee because they can take a while to hunt down and fix. There are tools that exists to help detect data races, but they are specific to different languages and environments and beyond the scope of this course. In my opinion, the best defense against data races is a good offense, preventing them from occurring in the first place. Since a data race only occurs when at least one of the concurrent threads is modifying the value of a memory location, pay close attention to anywhere you use an assignment operation, or an operator like the plus equal incrementor that changes a variable's value. If there's a potential for two or more threads to access that variable and make changes to it, then you'll almost certainly need to use some sort of mechanism to protect it.

## Mutual exclusion

- Anytime multiple threads are concurrently reading and writing a shared resource, it creates the potential for incorrect behavior, like a data erase, but we can defend against that by identifying and protecting critical sections of code. A critical section or critical region is part of a program that accesses a shared resource, such as a data structured memory or an external device, and it may not operate correctly if multiple threads concurrently access it. The critical section needs to be protected so that it only allows one thread or process to execute in it at a time. - Barren and I experienced a data erase as we added garlic to our shared shopping list, because incrementing a value is actually a three step process. Read the current value, modify it, and then write back the result. Those three steps are a critical section and they need to execute as an uninterrupted action, so we don't accidentally override each other. - I have an idea, give me your pencil. - Hey, I was using that. - Now there's only one pencil for us to share, and the rule is that only the person holding the pencil can access the shopping list, either to read or write it. That way one of us won't accidentally read a wrong value, because the other one is only halfway done updating it. In this arrangement, the pencil is serving as a mechanism called a mutex, short for mutual exclusion, which you'll also hear referred to as a lock. Only one thread or process can have possession of the lock at a time so it can be used to prevent multiple threads from simultaneously accessing a shared resource, forcing them to take turns. If either of us wants to access the shopping list, we first need to pick up the pencil to acquire the lock on it. We do whatever we need to with the shared notepad, and then when we're done, release the lock by putting down the pencil. The operation to acquire the lock is an atomic operation, which means it's always executed as a single, indivisible action. To the rest of the system, an atomic operation appears to happen instantaneously, even if under the hood it really takes multiple steps. The key here is that the atomic operation is un-interruptible. - If I grab the pencil-- - Acquiring the mutex is an atomic action that no other thread can interfere with halfway through. Either you have the mutex, or you don't. Now that you do have a lock on our pencil, you can safely execute in the critical section. - I see we already have ten carrots on the list, I'll add five more to that. - Oh, and we're going to need some more onions too. - Well, I currently possess the mutex, so you will have to wait until I'm done. Threads that try to acquire a lock that's currently possessed by another thread, can pause and wait 'til it's available. There, I'm done with the notepad for now, so I'll release the lock. - And I'll acquire the lock, so I can add onions to the list. - Don't forget to release the mutex when you're done. - Okay. - Since threads can get blocked and stuck waiting for a thread in the critical section to finish executing, it's important to keep the section of code protected with the mutex as short as possible. If I take the pencil, execute a critical section by adding more lettuce, and then hold onto the pencil while I contemplate what else to buy... - I'm stuck waiting for Olivia to return the pencil so I can use it, and getting kind of annoyed. - Only thinking about what I want to buy doesn't actually require the shared notepad. So the operation doesn't require mutual exclusion. I should of returned the pencil as soon as I was done updating the list, that way Baron could use it to execute the critical section while I'm busy doing other things.

## Mutual exclusion: Python demo

- To demonstrate how to manually enforce mutual exclusion with locks in Python, we'll modify the example program from earlier with two shoppers that have a data race as they can currently increment the amount of garlic to buy. Locks are included as part of Python's threading module which I've already imported on line four. So, I'll create a new lock object using the constructor method from that module. Threading dot lock. And I've assigned it the unimaginative name pencil, because Olivia and I used a pencil in our example to serve as a lock. Now, to keep the two shopper threads from modifying the garlic count variable at the same time, I'll call the pencil's acquire method before entering the for loop on line eleven and then I'll call its release method immediately after the for loop has finished. Pencil dot release. I'll save those changes, switch over to the console and run the program. The first shopper thread to begin executing will take the pencil, increment the garlic count 10 million times and then unlock the pencil so the other thread can do the same. That prevents the data race and I get the expected output of 20 million. Now, that solution worked fine here because incrementing the garlic count is a fairly quick operation and that's all the shopper threads needed to do, 10 million times each. But what if there was a longer I/O operation involved? Perhaps each of these shoppers needed some time to stop and think and ponder the meaning of life every time they add an item to the list. Back in the code, I'll simulate that by adding a print statement before I increment the garlic count variable that says the thread is thinking. I'll say, print threading dot current underscore thread dot get name and that'll give me the name of the current thread and then, is thinking. After that, I'll add a sleep statement to make the thread sleep for half a second. And that also requires me to import the time module at the top of my program. Finally, since I put that sleep method in there, I'll also reduce the number of loop iterations on line 13 from 10 million down to just five, so this program doesn't take forever to run. I'll save that change, go back to the console and run the program again. Now I see a new thinking message appear every half a second. Five from one thread and then five from the other. So this program takes about five seconds to finish executing. Since each thread does its thinking while holding onto the pencil, the other thread is stuck waiting outside of the critical section during that time. The critical section here is way bigger than it needs to be. I only really need to protect line 16 which increments the garlic count. Sleeping and thinking doesn't affect the shared data, so the shopper does not need to be holding the pencil while they do that. So to improve this program, I'll move the pencil's acquire and release statements so that the shopper locks it immediately before incrementing the garlic count by copying, or cutting and pasting that and then they release it immediately after incrementing the garlic count. I'll save and run that change. Now the program will run twice as fast because the threads are not holding onto the pencil while they're busy thinking. I see the thinking messages from both threads appear in pairs and it only takes about two and a half seconds to execute. We've minimized the critical section to only protect the part of this program that truly requires mutual exclusion.

# Reentrant lock

- Olivia and I have been using this pencil as a mutex. Only one person at a time can own, or have a lock on it, and only that person can access our shared resource, this notepad. - If I attempt to lock the mutex while another thread has it, my thread will be blocked, and I need to wait until he unlocks it so it becomes available. - And if I attempt to lock the mutex, it doesn't appear to be available, so my thread will just have to wait, too. (drumming) - It's behind your ear. You already locked it. - Oh, well, my thread can't unlock the mutex while I'm blocked waiting on it, and I'll be waiting on the mutex forever because I'll never be able to unlock it. I'm stuck and so are you. If a thread tries to lock a mutex that it's already locked, it'll enter into a waiting list for that mutex, which results in something called a deadlock, because no other thread can unlock that mutex. - There may be times when a program needs to lock a mutex multiple times before unlocking it. In that case, you should use a reentrant mutex to prevent this type of problem. A reentrant mutex is a particular type of mutex that can be locked multiple times by the same process or thread. Internally, the reentrant mutex keeps track of how many times it's been locked by the owning thread, and it has to be unlocked an equal number of times before another thread can lock it. If this pencil is a reentrant mutex, when I pick it up, I lock it. - Now Olivia's thread has a hold on the mutex so she's the only one that can lock or unlock it. - Since the pencil is reentrant, I can lock it again. Now, the pencil has been locked twice by me, which means I'll have to unlock it twice to fully release my hold on it. If your program needs to lock a mutex multiple times, using a reentrant mutex may seem like an easy way to avoid a deadlock, but if you don't unlock the reentrant mutex the same number of times, you can still end up stuck. - I'm waiting. Thanks. Many programmers like using reentrant locks because it can make things easier. You don't need to worry as much about what's already been locked, and they make it easier to retrofit locks into existing code. As an example, say I have a function to increment a shared counter, and it uses a mutex to protect that operation. If later I create another function that uses the same mutex to protect some other section of code, and that section of code uses the increment counter function, since those functions are nested, when I execute my function, it'll end up locking the mutex twice before unlocking it. If I was using a regular non-reentrant lock, that would produce a deadlock, but with a reentrant mutex this works just fine. Now, like many things in the world of programmers, there are some very strong opinions about whether reentrant locks are good or evil. Some opponents of using reentrant locks will argue that the example I just showed you should be refactored to avoid having nested locks by using a third function that increments the counter and only gets called from within a protected section. I'm not going to advocate either way on this debate. There are pros and cons to both sides. - One use case where reentrant locks are really needed is when writing a recursive function. That is a function that calls itself. If the function makes a recursive call to itself from within a locked section, it will lock the mutex multiple times as it repeats itself, and then unlock the mutex an equal number of times as it returns and unwinds. Since a reentrant mutex can be used recursively like this, you'll often hear it referred to as a recursive mutex or a recursive lock. Different languages use different terms, but these all basically mean the same thing.

# Rlock: Python demo

[**Example**](https://github.com/manwar/python-guide/tree/master/examples/concurrent-and-parallel-programming/part-1/CH04/04_02)

- To demonstrate using a reentrant lock in Python, we've modified the previous example we used to demonstrate a data race and mutual exclusion with two shoppers that are concurrently incrementing the amount of items to buy. In this version, we're counting the amount of garlic and potatoes to buy with the variables that are initialized on lines six and seven. There are two helper functions on lines 10 and 16 called add_garlic and add_potato, which increment the corresponding garlic_count or potato_count variables. And each of those functions acquire and release the same lock object, named pencil, to enforce mutual exclusion around those operations and prevent a data race. The shopper function simply uses a for loop on line 23 to execute those add_garlic and add_potato functions 10,000 times each. Down in the program's main section, we create and start two shopper threads, and then after they finish running, I print out the amount of garlic and potatoes to buy. I'll switch over to the console and run this program by typing python reentrance_lock.py. When it finishes, it indicates that we need 20,000 garlic and 20,000 potatoes, so, quite a feast. Now, let's say every time we add a potato, we also want to add an additional garlic, because potatoes and garlic go really well together. To do that, we might add a call to the add_garlic function inside of the add_potato function, immediately after incrementing the potato_count. By inserting the add_garlic function inside of the add_potato function, we've effectively nested two calls to the pencil's acquire method. When the add_potato function gets called, it will acquire the pencil, then, when the add_potato function calls add_garlic, the add_garlic function will acquire the pencil again. So, the pencil ends up being locked twice in a row by those nested functions, before then being released twice as the functions complete. When I run the program with those nested calls, it gets stuck as soon as the pencil is acquired twice in a row. It doesn't have any output, so, I can't really tell looking at it, but if I wait for this program to finish, I'll be waiting here forever. So, I'll press Control and the Break key to manually end it. Fortunately, the fix for this only requires adding one letter. On line eight, I'll change the lock constructor method to Rlock, which is Python's implementation of a reentrant lock that can be acquired multiple time before being released. I'll save that change. And now when I run the program, I get the output that we should buy 40,000 garlic and 20,000 potatoes. That makes sense because I increment the garlic count in both the add_garlic function and the add_potato functions. One interesting difference between the regular lock and Rlock in Python is that the regular lock can be released by different threads than the one that acquired it, but the reentrant lock must be released by the same thread that acquired it. And of course, it must be released by that thread as many times as it was acquired before it will be available for another thread to take. This is just the difference between how lock and Rlock are implemented in Python. Now, you might be tempted to devise a complex scheme using regular locks in which some threads acquire the lock and other threads release it. And if you find yourself thinking about that, stop. Trying to split lock operations across multiple threads is generally a bad idea, and a surefire way to create problems. If a thread tries to release a lock that has not already been acquired, Python will raise an error and can crash your program.

# Try lock
- When multiple threads each have multiple tasks to perform, making those threads block and wait every time they attempt to acquire a lock that's already taken may not be necessary or efficient. Olivia and I are two threads doing several different tasks. My thread will be taking an inventory of the fridge to see what things we're running low on, and then add those to the shopping list on our shared notepad. I'll go back and forth between those two tasks. - And my thread is searching through the newspaper for grocery coupons and then adding those items to the shared shopping list. Ooh, there are some good deals this week. Now that I've found some items that I want, I'll take the pencil, which is our mutex, to lock access to the shared notepad so I can add them. - I saw we're low on milk, so now I'll go to acquire the pencil, and I see Olivia has it. If I attempt to lock a mutex in a regular blocking fashion, my thread would enter a waiting state at this point, doing nothing until Olivia unlocks it. If I don't have anything else to do, so I can't continue with other things until after I've accessed the shared notepad, that's okay, it's what has to happen. But in this scenario, I do have other useful things to do that don't require the notepad. I can keep searching the fridge for other things we need. So rather than using the standard locking method to acquire the mutex, I'll use what's called Try Lock, or Try Enter, which is a non-blocking version of the lock or acquire method. It returns immediately and one of two things will happen. If the mutex you're trying to lock is available, it will get locked and the method will return true. Otherwise, if the mutex is already possessed by another thread, the Try Lock method will immediately return false. That return value of true or false lets the thread know whether or not it was successful in acquiring the lock. So if I try to lock the pencil that Olivia currently has, I know immediately that my attempt has failed, so I can go back to searching the fridge. - There, I'm done writing for now. So I'll unlock the pencil and go back to searching for coupons. Since Barron wasn't blocked waiting for this mutex, it's just sitting unlocked, available for anyone to take. Now, Barron likes to explain Try Lock with pencils and notepads. I think of it more like being at a house party with a bunch of your friends, your fellow threads. There's one restroom at the house that everyone at the party will need to use at some point. But only one person can use it at a time. When you try to use it and try opening the door, you realize it's locked because someone's already inside. You could stand there and wait until they come out, or you could go back to the party, keeping having fun, and try again later.

# Non-blocking acquire: Python demo

[**Example**](https://github.com/manwar/python-guide/tree/master/examples/concurrent-and-parallel-programming/part-1/CH04/04_04)

- In Python, the common try mock operation can be implemented by configuring the acquire method to be non-blocking. To demonstrate that in use, we're creating this example which simulates two shoppers searching for items they need, and then adding them to a shared notepad. The global variable on line seven represents the number of items on their notepad. Within the shopper function, each thread has a local variable on line 13 for the number of items to add to the notepad. How many items they've found in a coupon book, or perhaps missing from the fridge. The while loop on line 14 will keep the shoppers searching for items and adding them to the notepad until there are at least 20 items. If the shopper has items to add to the notepad, they'll execute the if clause on line 15. In which they acquire the pencil, add all of their items to the list, and print a message with how many items they added. That then resets their items to add count back to zero. The thread sleeps for 300 milliseconds to simulate time spent writing. And then finally unlocks the pencil on line 21. If the shopper did not have anything to add, then they would have executed the else clause on line 22, where they spend 100 milliseconds to search for and find an item they need, which then increments their own items to add counter. Down in the main section, I create two shoppers named Barron and Olivia. Then record the time from when I start their threads until they both finish to see how long it takes for them to find at least 20 items. Now, I'll run the program in the console by typing python non-blocking underscore acquire. And I see that it took them a little over six and a half seconds to find all those items. Notice that the two threads are never adding more than one item at a time. Since the default acquire method blocks execution, these two threads end up taking turns. Swapping places between the top and bottom sections of the if else statement. Now, within the acquire method on line 16, I'll configure the optional blocking parameter to be false. That will cause the acquire method to always immediately return a Boolean value to indicate whether or not the lock was successfully acquired. So, I'm going to move and incorporate that call into the logic of the if statement on line 15 by cutting it, adding an and statement, and then pasting that acquire spot here, and delete the line below it. Now, the order of these statements on line 15 on each side of the and operator matters. Because Python evaluates the statement from left to right. It will first check to see if there are items to add. And only if the left side of the and is true, will it evaluate the right side, and execute the non-blocking acquire method. If the lock is available, then calling acquire will lock it, and return true. And the program will execute the code between line 16 to 20 to add items to the shared notepad. If the lock was not available, then the non-blocking acquire method will immediately return false, and that thread will execute the else clause from lines 22 to 24 to look for other things to buy. When I run the program now, it executes much faster. Finishing in a little over two seconds, which is less than half the amount of time as before. Notice that now, when one of the threads gets its turn in the critical section, it's often adding more than one item to the notepad. With the non-blocking acquire method in place, as one thread takes its time writing to the notepad, the other thread is able to jump past that section of code to search for other things to buy multiple times. It's been freed up to accomplish other things.

# Read-write lock

- We use a lock or mutex to protect a critical section of code to defend against data erases, which can occur when multiple threads are concurrently accessing the same location in memory and at least one of those threads is writing to that location. That second part is key, because if we have a bunch of threads and none of them are writing, they all just want to read from the same location, that's fine. It's okay to let multiple threads read the same shared value as long as no one else can change it. They'll all safely see the same thing. Danger only exists when you add a thread that's writing to the mix. When we use a basic lock or mutex to protect the shared resource, we limit access so that only one of the threads can use it at a time, regardless of whether that thread is reading or writing or both. - That works, but it's not necessarily the most efficient way to do things, especially when there are lots of threads that only need to read. This is where reader-writer locks can be useful. A reader-writer lock, or shared mutex, can be locked in one of two ways. It can be locked in a shared read mode that allows multiple threads that only need to read simultaneously to lock it. Or it can be locked in an exclusive write mode that limits access to only one thread at a time, allowing that thread to safely write to the shared resource. A read-write lock is useful for protecting a shared resource like our calendar, because Barron and I frequently need to read the calendar throughout the day. - I can never remember what day it is. - But we rarely need to modify it. - Only once a day. - This marker represents our shared mutex. When my thread wants to read the calendar, I'll lock the mutex in the read only mode by placing my finger on it. - I also want to read the calendar. I can also place my finger on the marker. Now we both have a shared lock on it, so we can both concurrently read it. - When I'm done checking the date, I'll release my lock on the mutex. Now I think it's time to increment the calendar's date. In other words, I want to modify it. And to do that, I'll need to lock the shared mutex in the exclusive write mode by picking it up. - But I'm still holding on to the lock to read. - A thread trying to acquire the lock in write mode can't do so as long as it's still being held by any other threads in the read mode. So I'll have to wait. - Done. - Now the shared mutex is completely free, so I'll pick it up to place a write lock on it and update the calendar. - Ah, I already forgot what day it is. And I can't read the calendar now, because Olivia has an exclusive hold on the lock to write. Since only one thread can have the write lock at a time, all other threads wanting to read or write will have to wait until the lock becomes available again. - Now recognizing when to use a read-write lock is just as important as knowing how to use it. In certain scenarios, read-write locks can improve a program's performance versus using a standard mutex. But they are more complicated to implement, and they typically use more resources under the hood to keep track of the number of readers. - And there can be language-dependent differences in how they're implemented that affect performance. Do they give preference to readers or writers that are trying to acquire the lock? Deciding which type of mutex to use is a complicated decision, but as a general rule of thumb, it makes sense to use a shared reader-writer lock when you have a lot more threads that will be reading from the shared data than the number of threads that will be writing to it, such as certain types of database applications. If the majority of your threads are writing, then there's not much, if any, advantage to using a read-write lock.

# Read-write lock: Python demo

[**Example**](https://github.com/manwar/python-guide/tree/master/examples/concurrent-and-parallel-programming/part-1/CH04/04_06)

- Reader-writer locks are a common feature in many programming languages that support concurrency, however they're not included by default in Python. When I need something that isn't standard in Python, my first stop is always the Python package index at pypi.org and sure enough, someone has already implemented a reader-writer lock package for python. I'll install it for this demonstration using the Python package manager with the console command pip install reader writer lock. And it's that easy. In this example program, we create 10 threads to concurrently read what day it is from a shared calendar while two other threads update it. The calendar here is just a list of strings to represent the seven days of the week on line six, as well as an integer to indicate which day today is on line seven. I've created a single regular lock object named marker on line eight which all of the threads will use to enforce mutual exclusion when interacting with the today variable. There are two functions on lines 10 and 18 for the threads that will be reading the calendar value and those that will be writing to update it. They both use a while loop to continuously run until they see that it's the end of the week. In the calendar reader's while loop, the thread acquires the marker on line 14, prints out a message about what day it sees it is and then releases the marker. In the calendar writer's while loop, the thread acquires the marker on line 22, changes the calendar to the next day, prints a message about doing so and then releases the marker lock. Finally, down in the program's main section, I use a pair of for loops to create and start ten reader threads and two writer threads. So there are five times as many readers as there are writers. Notice that I'm using the args keyword in the thread constructor on lines 30 and 33 to pass in the loop iteration counter, i, as an argument to the calendar reader and calendar writer functions. I use that to give each of the calendar readers and writers a unique ID number. The args keyword takes the tuple of arguments to use when invoking the target function. I'll run this program in the console by typing Python, read write underscore lock dot pi and the threads get schedules to execute and each thread usually ends up getting to run through the while loop several times before another thread gets its turn. If I scroll up through the output, I see a lot of messages from different readers looking at what day it is and occasionally the writer threads get their chance to get scheduled. Since I'm using a single lock to protect both critical sections, only one thread can ever be reading or writing at a time. Now, to upgrade this program from using a basic lock to a reader-writer lock, I'll need to import the reader-writer lock package I installed earlier with the command from readerwriterlock import rwlock. Next, I'll change the lock constructor on line nine to use the RW lock package instead of the threading module and I'll create and RWLockFair object instead of a regular lock. Fair means this version of the RW lock will give fair and equal priority to readers and writers. There are also other implementations of the RW lock that can give priority to either the readers or to writers. The RW lock object has two methods. Gen r lock generates one lock for reading which can be held by multiple threads at once and gen w lock which generates another lock object for writers which can only be held by a single thread at once. Back in the code, I'll create and object called read marker using the marker's gen r lock method and down in the calendar writer function I'll create a write marker using the marker's gen w lock method. Now I just need to update the acquire and release statements in the calendar reader and writer to use their corresponding read marker and write marker. So I'll put read markers in the calendar reader and use the write marker in the calendar writer. Finally I'll add one last feature to the calendar reader's print statement to display the number of threads that are currently holding the read marker. I'll say read count and the reader-writer lock doesn't make that information easily accessible but I can pull the amount of threads that are currently holding the counter by going to read underscore marker dot c underscore rw underscore lock dot v underscore read underscore count. Bit of a long statement there. I'll save that change, switch back over to the console and run that program again. Now the output stream I get is much shorter that the first time I ran it. I can see that several of the readers are able to check the date and that multiple readers are able to be in the critical section at the same time. Up to 10 readers at several points. Finally, the writer got its chance to get in there and update the calendar. The readers got scheduled again to view that and after both writers made their updates, the program was finally able to exit.

## Deadlock

- Olivia and I decided to take a snack break to demonstrate some of the problems that can occur when using locks. A classic example that's used to illustrate synchronization issues when multiple threads are competing for multiple locks is the Dining Philosopher's problem. In this scenario Olivia and I are two philosophers or threads doing what philosophers do best; Thinking and eating. We both need to access a shared resource this plate of sushi and each time one of us takes a piece of sushi we're modifying it's value the number of pieces that are left. The act of taking sushi from the plate is a critical section so to protect it we've devised a mutual exclusion process using these two chopsticks as mutexes. When I want to take a bite of sushi I'll first pick up the chopstick closest to me to acquire a lock on it then I pick up the farther chopstick. Now I have possession of both locks I'm in the critical section so I'll take a piece of sushi and then put down the far chopstick to release my lock on it and then the close chopstick. And finally, since I'm a philosopher, I'll go back to philosophizing. - Ah! Eureka! That was an interesting thought. Well, I'm feeling hungry now so I'll acquire the chopstick closest to me and then the one further away. I'll take a piece of sushi and then release the far chopstick and then the one closer to me. As dining philosophers we'll both continue to alternate between eating and thinking but since we're operating as concurrent threads neither one of us knows when the other one wants to eat or think And that can lead to problems. If I get hungry again and pick up the chopstick closest to me. - And I also get hungry and pick up my close chopstick. I see we've come to an impasse. We've both acquired one of the two locks that we need so we're both stuck waiting on the other thread to release the other lock to make progress. This is one example of a situation called deadlock. Each member of a group is waiting for some other member to take action and as a result neither member is able to make progress. Avoiding deadlock is a common challenge in concurrent programs that use mutual exclusion mechanisms to protect critical sections of code. We want our program to be free from deadlock to guarantee liveness which is a set of properties that require concurrent programs to make progress. Some processes or threads may have to take turns in a critical section but a well written program with liveness guarantees that all processes will eventually make progress. But, we're clearly not making any progress here and I'm getting hangry. - Eureka! I have another idea. - Oh. Do tell, do tell. - Well, our deadlock occurred because we both reached for the chopstick closest to us first. I grabbed this chopstick first and you grabbed this chopstick first. That set us up for a deadlock. But, if we prioritize these locks so that we both try to acquire this chopstick first then we won't have this problem because we'll both be competing for the same first lock. - Well, let's give that a try. Ah! Brilliant. If you acquire the first chopstick and then I try to acquire it I'll just be stuck here waiting until you finish taking your sushi and release both locks. Then, I can grab that chopstick and this one and take my turn in the critical section. - In this example, we had two separate locks protecting a single shared resource. That may not be the most realistic scenario but it demonstrates the point. Now, imagine something like a banking application with a set of bank accounts where each one has it's own mutex. To ensure that only one thread will be withdrawing from or depositing funds to that account at a time. To transfer funds between two accounts a thread needs to acquire the locks for both the sender and the receiver since it would be modifying the value of both accounts. If there are multiple threads concurrently making transfers between the accounts then there's a real chance that the could end up competing for the same locks and run into this sort of deadlock scenario.

## Deadlock: Python demo

[**Example**](https://github.com/manwar/python-guide/tree/master/examples/concurrent-and-parallel-programming/part-1/CH05/05_02)

- To demonstrate a deadlock with a dining philosophers problem and how to resolve it will expand our scenario from having just two philosophers, Olivia and me, to having three philosophers. Olivia, me and our buddy Steve, each competing for two of the three chopsticks placed around the table, labeled here as A, B and C. In our example Python program, we instantiate those three lock objects on lines six through eight and name them chopstick A, B, and C. We also create a variable named sushi count to represent the amount of sushi left between the philosophers. Below that, the philosopher function on line 11 is used to represent our philosopher's who think and eat sushi. The function has three input parameters. The philosopher's name and two locks, named first chopstick and second chopstick to indicate the order in which the philosopher will acquire them. The while loop on line 13 will make the philosopher continue taking sushi as long as there's some left on the plate. Within the loop, the philosopher will pick up and acquire a lock on their first chopstick followed by their second chopstick. Then if there's still sushi left on the plate, they'll take a piece by decrementing the sushi count variable on line 18 and print a message showing how much is left. Finally, the philosopher releases both chopsticks to put them down for someone else to take. Down in the programs main section, we create and start three philosopher threads named Baron, Olivia and Steve. Each of those philosophers is past two of the three chopstick locks instantiated at the top of the program. Notice that each philosopher has a different first and second chopstick. Baron will acquire A first and then B. Olivia acquires chopstick B then C. And Steve acquires C first followed by chopstick A. Now when I run this program by typing python deadlock dot py, it seems to run fine. I get a message that Baron took a piece of sushi five times and then that seems pretty normal. Due to scheduling, only one philosopher is getting to eat here but all of the sushi is eventually eaten and the program finishes, which is valid behavior. And this highlights the tricky nature of deadlocks and why they're hard to detect and debug. Just like a race condition, you might get lucky and never experience a problem with your program. Even if the potential for a deadlock exists. To give this program more opportunities to deadlock I'm going to increase the amount of sushi from five to 500, we're three really hungry philosophers. Now if I run the program again, it locks up with 445 pieces of sushi remaining. Our philosophers are in a deadlock. If I press Control, Shift Escape to open the Windows task manager, and go to the performance tab, I see that the CPU is not overly busy, it's only at about one percent. Since the threads are stuck waiting on each other, the deadlock program doesn't use up CPU cycles. Now my program will be stuck in this state forever so I'll need to manually terminate it by pressing Control Break. Running this program again, will result in a deadlock after a different amount of sushi. The amount of progress it makes before the deadlock will vary depending on how the threads get scheduled to execute. If we're really lucky, it is possible that the program could make it to the end of the 500 piece sushi plate. But luck is not a good strategy for programming so let's implement Olivia's solution of prioritizing the locks. We'll say that chopstick A has the highest priority, B is second and C is third. And each philosopher should always acquire their highest priority chopstick first. I can see on line 25 that Baron acquires A before B. Olivia acquires B before C but Steve is causing the problem here because he acquires chopstick C before A. So I'll swap the order of those. Acquire A first and then C. I'll save that change. Now when I run the program after making that change, it runs to the end. and we have three very well fed philosophers. Now we designed this example to be as simple as possible by only including a single shared resource, the sushi plate. In practice, this program only really needed one lock to protect it. We intentionally created the potential for deadlock because I nested two locks inside of each other to demonstrate the concept. As your program grows in complexity to include more critical sections, locks and parallel threads with intertwined dependencies it becomes increasingly more difficult to identify and prevent potential deadlocks. The simplest technique to prevent deadlocks is the one we used in this video. Which is to ensure that locks are always taken in the same order by every thread. However, lock ordering may not always be feasible. For example, a thread may not know all of the locks it will need to acquire ahead of taking any of them. Another technique for preventing deadlocks is to put a timeout on lock attempts. If a thread is not able to successfully acquire all of the locks it needs within a certain amount of time, it will back up, free all of the locks that it did take and then wait for a random amount of time before trying again to give other threads a chance to take the locks they need.

## Abandoned lock

- Now that we've figured out how to avoid a deadlock between our two philosophers using chopsticks we can return to our routine of eating and philosophizing. I'm ready for another piece of sushi so I'll pick up the first chopstick, then the second one. And I think I left the refrigerator open. - Well that was rude of him. We've entered another form of a deadlock through thread death. If one thread or process acquires a lock and then terminates because of some unexpected reason it may not automatically release the lock before it disappears. That leaves others tasks like me stuck waiting for a lock that will never be released and getting hungry. - Sorry about that. Let's look at some code.

## Abandoned lock: Python demo

[**Example**](https://github.com/manwar/python-guide/tree/master/examples/concurrent-and-parallel-programming/part-1/CH05/05_04)

- To demonstrate what happens if a lock gets abandoned in Python, we'll be modifying the previous dining philosophers example that we used to demonstrate a dead lock with three philosophers using chopsticks to take sushi from a shared plate. We've already fixed the dead lock in this version of the code, so if I run this program, the philosophers successfully take turns eating sushi until all 500 pieces are gone. The critical section for this program exists between the acquire methods on lines 14 and 15 and the release methods on lines 21 and 22. Now, if one of the philosopher's threads acquires the locks, and then something goes wrong in that critical section to cause an unexpected error, that could kill its thread before it gets a chance to release the lock. To simulate that happening, I'll add another if statement to check if there are exactly 10 pieces of sushi left, and if so, I'll do my favorite technique for intentionally crashing a program, and that's to divide by zero. Now, you should never divide by zero, but I'm doing it here to trigger an exception that will cause Python to crash the currently executing thread. I'll save that change. When I run this program it gets all the way down to 10 pieces remaining. And then the thread that happens to be executing at that time, in this case that's the Barron thread, it hits that divide by zero and crashes. The other threads are stuck waiting on the locks that will never get released by Barron, so the program is stuck here forever. This scenario is not the same as the dead lock we looked at in a previous video, because the threads here are not waiting on each other to release a lock, but it's a related scenario and the impact is the same, the program isn't making any progress. So, to prevent this type of situation from occurring, we should put the critical section within a try block. If we have any exception handling code, we can optionally include an except clause after the try block to catch and deal with that error, but what we really care about here is making sure that the locks always get released before the current thread gets terminated if it crashes. And to do that, I'll add a finally block after that try statement, and put the calls to unlock the chopsticks in it. I'll save that change. And now when I run that program, an exception still occurs when one of the threads takes that tenth piece of sushi, in this case that was Olivia, but thanks to the finally clause, that thread is able to release the lock before it terminates. I can see that after the Olivia thread took the tenth piece of sushi and crashed, the Barron thread took over to finish eating the remaining sushi. It's good practice to always make sure locks will be released, if something goes wrong and unexpectedly crashes a thread. And Python makes that especially easy, because lock objects support working with context managers. Using the with statement on a lock, as shown on the right, is equivalent to using the try and finally blocks shown on the left. Using a context manager is the more pythonic way to program, so let's modify this example to use the with statement instead. I'll delete the entire finally block, and I can also delete the try statement, I won't need that now. I'll replace the acquire method on line 14 to say with first_chopstick:, then nested within that goes another call with second_chopstick:. And I'll move this whole block to within there. Now I'm using context managers, so if I save and run this program, it runs just the same as before, but now the code is more pythonically structured.

## Starvation

- It would be nice if Olivia and I took turns acquiring and releasing the pair of chopsticks so we could take an equal amount of sushi from the shared plate, but that's not guaranteed to happen. The operating system decides when each of our threads gets scheduled to execute and depending on the timing of that, it can lead to problems. If Olivia puts down the chopsticks to release her lock on the critical section, but my thread doesn't get a chance to acquire them before she takes them again then I'll be stuck waiting again until she takes another piece. If that happens occasionally, it's probably not a big deal, but if it happens regularly. - Too slow. - Then my thread's going to starve. Starvation occurs when a thread is unable to gain access to a necessary resource and is therefore unable to make progress. If another greedy thread is frequently holding a lock on the shared resource, then the starved thread won't get a chance to execute. - In a simple scenario like ours, with two equal threads competing for execution time, starvation probably isn't a concern. Both of our threads should get plenty of chances to take sushi. However, if our two threads are given different priorities then that may not be the case. Barron knows I get grumpy when I don't eat. - And I certainly don't want that, so I'll give Olivia's thread a higher priority. - Thanks! - How different thread priorities get treated will depend on the operating system, but generally higher priority threads will be scheduled to execute more often and that can leave low priority threads, like me, feeling hungry. Another thing that can lead to starvation is having too many concurrent threads. - Oh, I forgot to mention that I invited some friends over. (scoffing) - Well with this many competing threads, I may never get any sushi.

## Starvation: Python demo

[**Example**](https://github.com/manwar/python-guide/tree/master/examples/concurrent-and-parallel-programming/part-1/CH05/05_06)

- To demonstrate thread starvation in Python we'll modify the dining philosophers example by adding a local variable within the philosopher function to keep track of how many pieces of sushi each philosopher thread gets to eat. I'm going to name that sushi_eaten and I'll initialize it to zero. We'll increment the sushi eaten variable every time the philosopher takes a piece of sushi. sushi_eaten += 1, and at the end of the philosopher function after the while loop had finished we'll print out the number of pieces that it took. print(name), 'took', sushi_eaten, 'pieces' There are currently 5000 pieces of sushi up for grabs on line 9, so I'll switch over to a command prompt and run this program. (keys clacking) When it finishes, we see that each of the three philosophers got a different amount of sushi and it's not particularly fair. Olivia took way more sushi than Barron or Steve. She has over 3000 of the pieces. But it's not because she's greedy. In this case, it's because of the order in which the three philosophers are taking chopsticks. Barron and Steve are both competing for chopstick_a as their first chopstick but Olivia is the only philosopher going for chopstick_b first. Since she has less competition to get her first chopstick she ends up getting to take sushi more frequently. So, to make things fair for this example, I'll make all of the philosophers acquire the same two chopsticks, a first and then b. I'll save and run that program again. And this time Steve was especially greedy but there's still plenty to go around so all of the philosophers do get a chance to eat some. Now, let's see what happens if we drastically increase the number of philosophers at the dinner party using a for loop to create 50 instances of Barron, Olivia, and Steve. (keyboard clacking) That means we'll have 150 philosophers competing for 5000 pieces of sushi. I'll go back to the console and run that change and after running this program if I look at the final counts for each thread I can see that some philosophers, like Steve here, got a lot of sushi compared to others and many of these philosophers starve completely taking zero All of these threads were created with the same default priority but because there were so many some threads never got a chance to execute and this represents a real problem in programs containing a large number of threads. For example, if instead of feeding sushi to a bunch of philosophers this program was a web server that created new threads to handle a huge number of incoming requests some of those requests may never get processed. And that would lead to some very impatient and angry users on the other end. There are techniques that can be used to improve or even guarantee fairness among threads but that type of workload management is very situation dependent and beyond the scope of this course

## Livelock
- Our greed and competition for sushi has led us to a life of deadlocks and starvation. There's only one piece of sushi left and I can see you're still hungry. You should have it. - Thank you my dear, but I can see you're still hungry too and I would feel like a lousy husband if I allowed my wife to go hungry. You should have the last bite. - Oh, but I can't bear to see you hungry. You shall have the final bite. - Well, this is annoying. We've entered into another tricky situation known as a livelock. A livelock looks similar to a deadlock in the sense that two threads are blocking each other from making progress, but the difference is that the threads in a livelock are actively trying to resolve the problem. A livelock can occur when two or more threads are designed to respond to the actions of each other. Both threads are busy doing something, but the combination of their efforts prevent them from actually making progress and accomplishing anything useful. The program will never reach the end. The ironic thing about livelocks is that they're often caused by algorithms that are intended to detect and recover from deadlock. If one or more process or thread takes action to resolve the deadlock, then those threads can end up being overly polite and stuck in a livelock. To avoid that, ensure that only one process takes action chosen by priority or some other mechanism, like random selection. - Rock, paper, scissors for it? - One, two, three shoot! - I win! - Now Olivia gets the last piece of sushi and that resolves our livelock.

## Livelock: Python demo

[**Example**](https://github.com/manwar/python-guide/tree/master/examples/concurrent-and-parallel-programming/part-1/CH05/05_08)

- To demonstrate a livelock in Python we'll modify the original dining philosopher's example that we used to demonstrate a deadlock. Now, we have not implemented the priority strategy for lock ordering to prevent a deadlock. So if I run this program with Python livelock dot py, sure enough, after several iterations it hits a deadlock. The philosophers have acquired their first chopstick lock and they're stuck waiting for their second one to become available. One possible way to fix this problem would be to have the philosophers release the lock on their first chopstick if the second chopstick is not available when they try to take it. That will give another philosopher a chance to take that first chopstick and hopefully prevent a deadlock. To implement that, I'll modify the acquire method for the second chopstick on line 15 to be non-blocking by setting the blocking value to false. I'll also add an if statement to execute if that lock was not taken. And inside of that if statement, I'll print a message that says the philosopher released their chopstick. And then I'll release their first chopstick. Otherwise, I'll create a corresponding else clause. And if that happens we'll execute the try finally block to take a piece of sushi and then release both chopsticks. I'll save that change and then run the program. And I just see a lot of output messages that the philosophers are releasing their first chopstick. Occasionally, the scheduling might work out so that one of the philosophers is able to acquire both of the locks they need to take a bite of sushi but the majority of the time here, these philosophers are picking up their first chopstick, seeing that the second one is not available, so they politely put the first chopstick back so another philosopher can use it. If I press control-shift-escape to bring up the Windows task manager, over on the performance tab I can see the CPU usage is sitting at around 6%. And that corresponds to using one of the 24 processors available on this system. All of the threads are actively working to pick up and put down chopsticks. They're just not making any useful progress. Livelocks can be even harder to locate and debug than deadlocks but when your multi-threaded program seems to be stuck in a lock looking at the CPU utilization may give you some insight into whether it's a live or a deadlock. I'll stop this program by pressing control-break. Now, to resolve this livelock, I'm going to introduce some randomness into the locking process by importing the time and random modules. Then after a philosopher decides to put back their first chopstick, I'll make them sleep or philosophize for a random amount of time. With time dot sleep, random, and I'll divide by 10. So a tenth of a second. Now, when I run the program one final time if I scroll through it, I'll see way fewer instances that the philosophers returned their chopsticks. So they're able to keep taking pieces of sushi. The program makes progress and is able to finish without running into a deadlock or a livelock.

## Synchronization

## Condition variable

- Barron and I just made a slow cooker full of delicious hot soup and I'm ready to dig in. - But we should take turns to make sure we each get our fair share of soup. In this scenario, we're two hungry threads, competing for access to a shared resource, the soup. And the slow cooker lid will act as a mutex to protect it. Only the thread that holds the lid can check to see how much soup is left, determine if it's their turn to take the next serving, and modify the amount of soup that's left by taking some. - Mmm, that was some really good soup, I think I'll have another serving. Aww, I see you haven't taken your share yet. What about now? How about now, no. - Olivia's thread is wasting a lot of energy repeatedly acquiring the mutex to check for a certain condition to see if it's her turn to take more soup. And she'll continue doing that until my thread eventually gets scheduled so I can acquire the lid, see that it's my turn, and take my serving. - What I was doing is called busy waiting, or spinning, repeatedly acquiring and releasing the lock to check for a certain condition to continue. It isn't very efficient, especially if it goes on for a long time. - This is one of the limitations of using just a mutex. Sure it restricts multiple threads from taking soup at the same time, a way to signal each other to synchronize our actions. To do that, we can use another mechanism called a condition variable, which serves as a queue or container for threads that are waiting for a certain condition to occur. Think of it as a place for threads to wait and be notified. The condition variable is associated with a mutex, and they work together to implement a higher level construct called a monitor. Monitors protect a critical section of code with mutual exclusion, and they provide the ability for threads to wait or block until a certain condition has become true along with a mechanism to signal those waiting threads when their condition has been met. Conceptually, you can think of a monitor like a room that contains the procedures and shared data that you want to protect. Only one thread can be in that room at a time to use those procedures and access the data. The mutex is a lock on the door, other threads that try to enter the protected section while it's occupied, will wait outside in a condition variable which is like a waiting room. They might all be waiting for the same condition to occur before they enter the monitor room, or there might be multiple condition variables, or multiple waiting rooms, waiting for different conditions to occur to acquire that same mutex. When the thread inside the monitor finishes its business in the critical section, it will signal one of the conditions that it's their turn to execute, then it releases its lock on the door to exit the critical section. One of the threads waiting for that condition that was signaled, will wake up and take its turn in the monitor, locking the door behind it so it can execute the critical section. Now the condition variable has three main operations: Wait, signal, and broadcast. Before using a condition variable, I first need to acquire the mutex associated with it, check for my condition, I see that it's not my turn to take more soup, so I'll use the condition variables wait operation, which releases my lock on the mutex, and then puts my thread to sleep or a pause state, and places it into a queue waiting for another thread to signal that somebody else takes soup. - Since Barron releases his lock on the lid before going to sleep, now I can acquire it, see that it's my turn to take some soup, so I'll do that. Then I'll use the signal operation to wake up a single thread from the waiting queue, so it can acquire the lock. Depending on the language you're using, you'll also see this operation called notify or wake. Barron wake up, it's your turn to take some soup. Finally, I'll release my lock on the mutex. - Ah, my turn. The third condition variable operation, broadcast, is similar to the signal operation, except that it wakes up all of threads in the waiting queue. You might also see it called things like notify all or wake all. Now in this little scenario we only had two threads signaling each other on a single condition, that somebody took soup, which then changes who's turn it is to take the next serving. A more common use case that requires multiple condition variables is implementing a shared queue or buffer. If multiple threads will be putting items in a queue and taking them out, then it needs a mutex to ensure that only one thread can add or remove items from it at a time. And for that we can use two condition variables. If a thread tries to add an item to the queue, which is already full, then it can wait on a condition variable to indicate when the buffer is not full. And if a thread tries to take an item but the queue's empty then it can wait on another condition variable for BufferNotEmpty. These condition variables enable threads to signal each other when the state of the queue changes. to signal each other when the state of the queue changes.

## Condition variable: Python demo

- This Python program, to demonstrate using a condition variable, has a function named hungry_person defined on line nine, which has an input parameter for a person_id number. The hungry_person function will run as a thread that alternates with other hungry_person threads to take servings of soup until it's all gone. The variable on line seven represents the number of soup servings left, and the slowcooker_lid on line six is the lock to protect the soup_serving's variable so only one hungry_person can change it at a time. Down within the while loop on line 11, the hungry_person uses a context manager to lock the slowcooker_lid, then the if statement on line 13 compares their id_number with the number of soup_servings remaining. Modulo two because this example creates two hungry_person threads in its main section. If it's the current thread's turn and there's still soup left, then hungry_person will decrement the soup_servings on line 14, and print a message that they took soup. Otherwise, the hungry_person will put the lid back, on line 17, and check again for their turn later. I'll run this program by typing python condition_variable.py. And I see that these two threads spend a lot of cycles checking to see if it was their turn and then putting the pot lid back. If I scroll it through the output, I'll see that over the course of all that, they do take turns eating, it's just not very efficiently. So let's use condition variables to help them communicate. I'll use the threading module to create a condition object to signal after one of the threads has taken soup. I'll call that soup_taken and then use threading.condition, and I'll associate that condition variable with the slowcooker_lid using the lock argument. If you do not specify a lock to use with a condition variable, then Python will create a new r lock object to use as its underlying lock. The typical usage pattern for a condition variable involves acquiring the lock, then using a while loop to check a condition. If the condition is not true, then we'll wait on the condition variable for another loop iteration. Otherwise, when the condition is true, I'll continue past the loop to execute the critical section of code and then finally release the lock. In Python, it's common to use the with statement to automatically handle acquiring and releasing the lock with a context manager. Now, I want to emphasize here that the condition variable is not the condition itself, or an event. The condition that we're checking for is the logic of the while loop. Is it this thread's turn to take soup? The condition variable is just a place for the threads to wait until they're signaled by another thread to check that condition again. Back in the code, I'll change the if statement on line 14 into a while loop. And I'll modify the condition it checks for to see if it's not this hungry_person's turn to take soup. If it's not their turn, then I'll have them wait on the condition variable to get signaled after another thread takes the soup. So I'll type soup_taken.wait. I'll also move this print statement, that says a thread checked the condition, saw it wasn't their turn, and then put the lid back into this while loop. That way we can see when a thread calls the wait method and then releases its hold on the slowcooker_lid for another thread to acquire. Actually, I will move that in front of that. So we'll print that message before the thread waits. Now, if the thread checks the condition, and it is their turn to take soup, then execution will continue past the while loop. I need to add another check to make sure there's still soup left. So I'll say if soup_servings is greater than zero, if so, the thread will take soup, and print a message, and then finally it will need to signal another thread to wake up. The basic pattern for signaling a condition variable involves first making sure you have a lock on the mutex to ensure you have exclusive access to whatever comprises the condition, and that's handled here by a context manager. Then do something that changes the state for that condition, notify at least one of the other waiting threads to wake up, and finally, unlock the mutex so another thread can proceed to use it. Again, the unlocking of the mutex is handled here by the context manager. In this program, I'm going to delete the else statement here. I'll add the line soup_taken.notify, after the hungry_person takes a serving of soup. I'll save those changes. Now I'll run the program, and I see that the two hungry_people take turns. But now they don't waste a whole lot of energy repeatedly checking who's turn it is and putting the lid back. Now, let's see what happens if I expand this dinner party to include more hungry_people by modifying the for loop down in the program's main section to create five hungry_people threads. I'll also modify the condition statement on line 14 to rotate the serving among those five people. Now I'll save and run this program again, and it gets stuck. I'll press Control + Break to manually terminate it. The problem here is that I use the notify method on line 20, which will only wake up one of the waiting threads. Of those four other threads, if it doesn't wake up the one whose turn it is, then the program will get stuck. So the fix here is to change that method to notify all, to wake up all of the waiting threads. I'll save that change and now when I run the program everything works great. All of the threads eat and the last one puts the lid back when there are no more servings left. If you only need to signal one of the waiting threads, and you don't care which one it is, then the basic notify method will work fine. But in this example, since I want a specific thread, out of those five hungry_people to wake up and see that it's their turn. Relying on the signal notify method to wake up the right thread will not always work.

## Producer-consumer

- A common design pattern in concurrent programming is the producer-consumer architecture where one or more threads, or processes, act as a producer, which adds elements to some shared data structure. And one or more other threads act as a consumer which removes items from that structure and does something with them. To demonstrate that, I'll be the producer serving up bowls of soup. - I guess that makes me the consumer who eats the soup. I like this demonstration. - After I fill a bowl, I'll add it to this line of bowls that represents a queue. Queues operate on a principle called a First-In-First-Out or FIFO, which means items are removed in the same order that they're put into the queue. The first item that was added will be the first item to be removed. - So when I'm ready to consume another bowl of soup I'll grab one from this end of the line because it's been in the queue the longest. These bowls of soup represent elements of data for the consumer thread to process, or perhaps packaged tasks for the consumer to execute. Now when multiple threads are operating in the type of producer-consumer situation it poses several challenges for synchronization. First off, the queue is a shared resource, so we'll need something to enforce mutual exclusion and make sure that only one thread can use it at a time to add or remove items. We also need to make sure that the producer will not try to add data to the queue when it's full, and that the consumer won't try to remove data from an empty buffer. Some programming languages may include implementations of a queue that's considered thread-safe and handles all of these challenges under the hood so you don't have to, but if your language does not include that support, then you can use the combination of a mutex and condition variables to implement your own thread-safe synchronized queue. Hey Olivia, you can slow down production, I can't eat soup this fast. - No can do, I'm a soup serving machine. - You may run into scenarios where the producer cannot be paused if the queue fills up. The producer might be an external source of streaming data that you can't slow down. So it's important to consider the rate at which items are produced and consumed from the queue. If the consumer can't keep up with production then we face a buffer overflow and we'll lose data. This table is only so big, our queue can only hold a limited number of bowls of soup before they start falling on the floor. Some programming languages offer implementations of unbounded queues which are implemented using linked lists to have an advertised unlimited capacity. But keep in mind even those will be limited by the amount of physical memory in the computer. - The rate at which the producer is adding items may not always be consistent. For example, in network applications data might arrive in bursts of network brackets But if those bursts occur rather infrequently then consumer has time to catch up between bursts. You should consider the average rate at which items are produced and consumed. You want the average rate of production to be less than the average rate of consumption. - Well it looks like you're pouring at a pretty steady pace, and I definitely can't keep up alone. I'm going to need some help. Hey Steve, do you want some soup. - You bet my friend, I'm a soup eating machine. - Excellent, with two consumer threads eating in parallel maybe we'll be able keep up with Olivia's rate of production. Now there are only two tasks going on here, Olivia is serving soup while Steve and I eat it. But if more steps were required to process this data, perhaps we also need to season the soup, then we could expand our simple producer-consumer setup into a pipeline of tasks. A pipeline consists of a chain of processing elements arranged so that the output of each element is the input to the next one. It's basically a series of producer-consumer pairs connected together with some sort of buffer like a queue between each consecutive element. - As a pipeline, I pass my full bowls of soup to a queue. - I take bowls from that queue, add spice, then I pass them along to another queue, which Steve takes from to eat. If all three of our threads can execute in parallel, then as a pipeline we're processing up to three bowls at any given moment. Now the issue of processing rates is still a concern, each element needs to be able to consume and process data faster than the elements upstream can produce it. faster than the elements upstream can produce it.

## Producer-consumer threads: Python demo

- This Python example program has two primary functions. A soup producer on line 10, and a soup consumer on line 17. I use those functions to create two threads down in the program's main section. As their names suggest, the soup producer thread is generating bowls of soup which I represent with strings for the consumer to eat. The producer and consumer use a queue to pass objects between them. When we create that queue named serving line on line eight we set the max size argument to five. Which means the queue is limited to hold up to five items at a time. We created the serving line queue using Python's queue module which is designed for exchanging objects between multiple threads. The queue class already implements all of the necessary locks and synchronization mechanisms for multiple producers and consumer threads to use it concurrently. So, conveniently we don't have to worry about that. Now, looking at the soup producer function, the for loop on line 11 is used to serve up 20 bowls of delicious, hot soup. For every iteration it uses the put_nowait method on line 12 to attempt to add a string object representing a bowl of soup to the serving line queue. If there is space available in the queue, then the add method will insert the item. Otherwise, it will raise an exception. After that, the producer prints a message that includes the number of remaining spaces in the queue, and finally on line 15, it sleeps for 200 milliseconds to simulate the time it takes to serve up a bowl of soup. Down in the soup consumer function, the while loop on line 18 will continuously take soup from the serving line. It calls the queues get method on line 19 to get the string representing a bowl of soup, prints a message that it ate that bowl, and then sleeps for 300 milliseconds to simulate the time it takes to consume the soup. Those are all the pieces. So now I'll switch over to a console and run this program by typing python, producer, consumer, threads.py. I see messages from the producer that it's serving bowls of soup, and from the consumer that it's eating them. However, after a while the soup producer throws an exception that the queue is full and crashes. The problem here is that the producer is adding soup faster than the consumer can eat it. It only takes 200 milliseconds to produce a bowl, but 300 milliseconds to consume it. Scrolling back up through the output, I see that the remaining capacity in the serving line gradually decreased down to zero before the producer threw the error. To resolve this problem, I'll use a for loop to add a second consumer thread to this program to help eat the soup. But still only one producer. Although, it takes each soup consumer 300 milliseconds to eat a bowl, now that I have two of them working together, on average my program will be able to process a bowl of soup every 150 milliseconds. Which is faster than the soup producer serves it up. So when I run this program now, the consumers are able to keep up with the producer and everything works fine. Except that this program never finishes after the producer is done serving 20 bowls. Because the consumer threads are still running, waiting for more soup to come through the queue for them to eat. So I need to implement some sort of signal to let the consumers know that the producer is done. And one way to do that is to pass a final message through the queue. I'll terminate this program by pressing control break, switch back over to the code. After the soup producer's for loop completes, I'll add a final string to the queue with a special message to the consumer. Serving_line.put_nowait, and I'll say no soup for you. And since there are two consumers, I'll need to add it twice, one for each. So I'll just copy and paste that. Now, down in the soup consumer, after retrieving a string from the queue, I'll check to see if it says no soup for you. And if so, we'll break out of the loop. Now when I run this program, all of the soup gets eaten and at the end, the program finishes appropriately. I used strings to represent bowls of soup in this example, but queues can be used to pass any type of data or other packaged tasks to the consumer threads to be processed.

## Producer-consumer processes: Python demo

- We were able to resolve the buffer overflow problem in the previous producer consumer example by adding an additional consumer thread. That worked because we simulated the time it took for a producer to add items to the queue and for the consumer to process those items by using the sleep statements on lines 15 and 25. When a thread is sleeping, it does not use CPU resources. So that scenario represents a situation where the consumers are performing an I/O bound task. Like downloading files over a network. But what would happen if the consumers task was CPU intensive instead. Perhaps the producer was streaming data that the consumer needed to process in your real time. To simulate that scenario, I'll define a new helper function called CPU work and I'll pass it in argument representing the amount of CPU work to simulate. Within the function, I'll simply create a variable named X. And initialize it to zero. And then I'll use a for loop to increment X by the number of work units times one million. And within that loop, I'll simply do X plus equals one. This simulates a CPU intensive operation that will keep the processor busy for a period of time. On my own computer, calling this function with one unit or work will keep the CPU busy for around 75 milliseconds. But that duration will vary from computer to computer, depending on the processor speed. Now down in the consumer function, I'll replace the 300 millisecond sleep statement on line 30 with a call to CPU work for four units of work. That should take roughly the same amount of time or 300 milliseconds. I'll leave the producer function alone for now so it will continue putting new elements into the queue at a steady rate of once every 200 milliseconds. We can see down in the main section on lines 33 through 35, that the program will spawn two consumer threads and one producer thread. So I'll save those changes and then run that program. Python producer consumer processes dot py. As you can see it fills up the queue and the producer error's out. So why is that happening now? The problem is not a shortage of consumer threads. In fact, I can modify that for loop on line 33 to create five consumer threads, which is more than double what we had before. And if I run that program again, the queue still fills up and we have the buffer overflow error. The problem here is with Python. In particular the global interpreter lock. The global interpreter lock or GIL only allows one Python thread to execute at a time. So at any given moment, only one of the consumer threads will actually be executing the CPU work task. That's why creating more threads doesn't solve our problem. To get around this limitation in Python, we can restructure the program to use multiple processes instead. To do that, I'll replace the import threading statement on line five to import the multi-processing package as mp. Then I'll replace the regular queue on line nine to use the queue class from the multi-processing module which is specifically designed to exchange data between processes. And it doesn't need the max size argument name there. Since we'll be spawning the producer and consumer as separate processes, in order for them to see the same serving line queue object, we'll need to pass it as an input argument to their functions. Serving line. And serving line. Finally, down in the main section, I'll replace the lines to create the consumer and producer threads with their counterparts to create processes instead. So that's mp dot process. And I'll need to pass in an argument for that serving line with the args keyword. Copy that. And do the same for the producer. I'll also reset my for loop to only create two consumers as we did earlier on. And one last minor tweak, on line 19, the max size property for the multi-processing queue has an underscore so I'll add that here. Now, when I save and run this program, it works. By using multi processes, the two consumers can run in parallel on separate processors to complete their CPU work. They're no longer being limited by the global interpreter lock. That was necessary in this scenario due to the CPU intensive work load but if the consumer tasks were I/O bound, then simply using threads will usually work just fine.

## Semaphore

- A semaphore is another synchronization mechanism that can be used to control access to shared resources, sort of like a mutex. But unlike a mutex, a semaphore can allow multiple threads to access the resource at the same time. And it includes a counter to track how many times it's been acquired or released. As long as the semaphore's count value is positive any thread can acquire the semaphore, which then decrements that counter value. If the counter reaches zero, then threads trying to acquire the semaphore will be blocked and placed in a queue to wait until it's available. When a thread is done using the resource it releases the semaphore, which increments that counter value. And if there are any other threads waiting to acquire the semaphore, they'll be signaled to wake up and do so. - Hmm, looks like my phone's about to die. - Lucky us, there's a charger right here. - Nice, this charger has two ports so it can be used by up to two devices at the same time. You can think of the number of open ports as a semaphore. Right now, this semaphore has a value of two because there are two free ports. And when I acquire one of these ports by plugging in my phone it decrements the semaphore's value to one. - I'll acquire the other port. And that decreases the semaphore's value to zero which means it's unavailable for anyone else to use. - Lucky me, there's a charger right here. - Oh man, sorry, Steve. The semaphore is locked right now because there aren't any available ports. You'll have to wait until one of us is done charging and releases the semaphore. - No worries, I'll wait. - You know, I don't need a charge that bad. I'll release my port, which increments the semaphore's value and I'll notify Steve that it's available. Hey, Steve, wake up. - Cool, now the semaphore is positive. I can acquire it and charge my phone. - This type of semaphore that we're using here is called a counting semaphore because it can have a value of zero, one, two, three, and so on to represent the number of resources we have. In our scenario, we're counting available charger ports but in software, a counting semaphore might be used to manage access among multiple threads to a limited pool of connections for something like a server or a database, or a counting semaphore could be used to track how many items are in a queue. Now, it's also common to restrict the possible values of a semaphore to only being either zero or one, with zero representing a locked state and one representing unlocked. This is called a binary semaphore and at first glance it looks a lot like a mutex. In fact, it can be used just like a mutex with a thread acquiring and releasing the semaphore to lock and unlock it. However, there is a key difference. A mutex can only be unlocked by the same thread that originally locked it. A semaphore on the other hand can be acquired and released by different threads. Any thread can increment the semaphore's value by releasing it or attempt to decrement the value by acquiring it. That may sound like a recipe for trouble and it certainly can be if you're not careful but the ability for different threads to increment and decrement a semaphore's value and for threads to wait and be signaled by the semaphore is what enables it to be used as a signaling mechanism to synchronize the action between threads. For example, a pair of semaphores can be used in a similar way to condition variables to synchronize producer and consumer threads, adding and removing elements from a shared finite cue or buffer. One semaphore tracks the number of items in the buffer, shown here as fill count, and the other one tracks the number of free spaces, which I'll call empty count. To add an element to the buffer, the producer will first acquire the empty count, which decrements its value, then it pushes the item into the buffer, and finally it releases the fill count semaphore to increment its value. Now, on the other side of the buffer, when the consumer wants to take an item, it first acquires fill count to decrement its value, then it removes an item from the buffer, and finally increments the empty count by releasing it. Notice that the producer and consumer each acquire a different semaphore as the first operation in their respective sequences. If the consumer tries to take an item when the buffer is empty, then when it tries to acquire that fill count semaphore, it'll block and wait until a producer adds an item and releases fill count, which will then signal the consumer to continue. Likewise, if the producer tries to add an item to the full buffer, then it goes to acquire the empty count semaphore, it'll block and wait until a consumer removes an item and releases the empty count.

## Semaphore: Python demo

- For this Python example, we'll use a counting semaphore to control access to a cell phone charger and keep track of the number of available ports. The semaphore named charger on line eight is initialized to a value of four representing the number of available charging ports. Below that the cell phone function attempts to acquire the chargers semaphore on line 12. If the semaphore is not available because its value is zero then the thread will wait there until a charging port opens up and the semaphore is released. Once a cell phone thread has acquired the semaphore, it prints a message that it's charging and then sleeps for a random amount of time from one to two seconds. After that, the cell phone prints a message that it's all done charging and then releases the semaphore to increment its value so another thread can acquire it. Down in the main section for this program, we use a simple for loop to create and start 10 cell phone threads. I'll switch over to a console and run this program by typing python semaphore.py. I see four of the phones connect immediately at the beginning, then as each phone finishes charging and releases the semaphore, another phone acquires it to begin charging. At most, there will be four phones connected to the charger at any given time. Python semaphores work with the context manager in the same way that locks do. So instead of explicitly acquiring and releasing the semaphore, we can replace line 12 to use a width statement instead. So I'll say width charger, indent those, and then delete the release statement. Now instead of using this charger as a counting semaphore, if I change its initialization value from four to just one, now it'll act as a binary semaphore. When I save and run this program again, now only one thread at a time will be able to acquire the semaphore. The way I'm using the binary semaphore now with the same thread that acquires it also releasing it is basically the same as a mute text. In fact, we could replace the semaphore in this program we a reentrant R lock and it would function the same way.

## Barriers

## Race condition
- Data races and race conditions are two different potential problems in concurrent programs that people often confuse with each other probably because they have similar sounding names with the word race in them. Data races can occur when two or more threads concurrently access the same memory location. If at least on of those threads is writing to, or changing that memory value, that can cause the threads to overwrite each other or read wrong values. - That's a pretty straightforward definition, which makes it possible to create automated tools to identify potential data races in code. And to prevent those data races, you need to ensure mutual exclusion for the shared resource. A race condition, on the other hand, is a flaw in the timing or ordering of a program's execution that causes incorrect behavior. In practice, many race conditions are caused by data races, and many data races lead to race conditions. But those two problems are not dependent on each other. - It's possible to have data races without a race condition and race conditions without a data race. Olivia and I invited Steve and gang over to play video games next weekend, so we need to figure out how many bags of chips we need to buy to keep them all fed. Our shopping list is shared resource, and this pencil serves as a mutex to protect it. Only the person or thread with the pencil can view or modify the shopping list. - I'll go first. I see that our shopping list already has one bag of chips. With the Steve and gang coming over, I think we need three more. So, one plus three, that means we need four bags. - Well, I always overestimate the amount of chips we need for party, so I'm going to double that. I see we have four, two times four is eight. Great, we need eight. Now, let's rewind that and see how else those operations could've played out if our two threads got scheduled differently. (whirring) - I'll go first first. - Hold on, I'll go first this time. I see one bag of chips, but I like to over estimate so I'll double that. One times two is two. - Thanks. Now I'll add three bags to that. Two plus three is five. Hm, five bags is less than the eight we calculated last time. (grunts) - Don't tell me we're not going to have enough chips for the party! - It's okay, we'll fix this. - Phew. - Even though we're using this pencil as a mutex to protect against a data race, the potential for a race condition still exists because the order in which our threads execute is not deterministic. When deciding how many bags to buy, if my thread runs first to add three bags before Baron doubles it, that gives us eight. But if Baron instead runs first to double to original value before I add three bags, then we end up with five. - The race condition we created here is fairly straightforward, but in practice, race conditions can be really hard to discover. And that's because a program might run correctly for millions of times while you're building and testing it, so you think everything's fine. You release the finished program, and then one time, things happen to execute in a different order and that causes an incorrect result. Unfortunately, there's not a single catchall way to detect race conditions. Sometimes putting sleep statements at different places throughout your code can help to uncover potential race conditions by changing the timing and, therefore, order in which threads get executed. That said, race conditions are often a type of heisenbug, which is a software bug that seems to disappear, or alter its behavior, when you try to study it. Running debuggers and doing things to affect the timing of your code in search of a race condition may actually prevent the race condition from occurring.

## Race condition: Python demo

- This example demonstrates a race condition in Python with several threads either adding to or multiplying the value of the bags of chips variable on line six which represents the number of chips we should buy for our party. The barron_shopper function on line 14 begins by calling the cpu_work function to simulate doing a but of CPU intensive work. This provides some time for the randomness of the execution scheduler to influence when the shopper threads will get scheduled relative to each other to execute the rest of their code. After completing its CPU work, the barron_shopper acquires the lock named pencil using a context manager on line 17, and then doubles the bags of chips on line 18, and prints a message. The olivia_shopper function below that does basically the same thing except she adds three bags of chips on line 25 instead of doubling them. Down in the main section, we use a series of for loops to create five barron_shopper threads and five olivia_shoppers. Start them all on line 34, wait for them to join on line 36, and then finally, print a message with the total number of chips to buy. Notice that both the barron and olivia_shoppers lock the pencil using a width statement on line 17 and 24 to limit access when modifying the shared bag of chips variable. Since only one thread can read or write that variable at a time, this program is protected against having a data race, but it's still vulnerable to a race condition. To show that, I'll run this program by typing python race_condition.py, and after all 10 threads finish it prints a message that we need to buy 194 bags of chips. Now if I press the up arrow and run it again, this time we need 149 bags of chips. The relative order in which those Olivia and barron threads were scheduled to add and multiply the bags of chips was different which gave us a different result. Running it again, yet another answer. Again, the problem here is not a data race, because we've guaranteed mutual exclusion by having the shoppers lock the pencil before modifying the bags of chips. However, there is a race condition here because the order in which these threads get scheduled to execute changes the final result.

## Barrier

- To prevent our race condition from occurring, we need a way to synchronize our actions so we execute our respective multiplication and addition operations in the correct order. And we can do that with something called a barrier. A barrier is a stopping point for a group of threads that prevents them from proceeding until all or enough threads have reached the barrier. I like to think of threads waiting on a barrier like players on a sports team coming together for a huddle. Before they join the huddle, the players might be doing other things, putting on their equipment or getting a drink of water. As they finish those individual activities, they join their teammates at the huddle. Players in the huddle wait there until all of their fellow teammates arrive. Then they all yell, "Break." And then they scatter about to continue playing their game. - We can use a similar strategy here to solve our race condition, huddling together to synchronize when we each execute our operations to add and multiply items on the shopping list. I should complete my operation of adding three bags of chips to the list before we huddle together. Then, afterwards, Barron can double the amount. - Sounds good. - I'm scheduled to execute first this time, so I'll require the pencil. I'll add my three bags of chips to the list. And release my lock on the pencil. And then meet you at the huddle. Don't leave me hanging. - I don't have anything to do before the huddle, so- - [Both] Break! - Now we're past the barrier, so I'll double the number of chips. That gives us eight, which is the right amount. By using a barrier, the order in which our threads actually get scheduled to execute doesn't matter because the barrier synchronizes us. Olivia always adds three bags before the barrier, and I multiply by two after it. If we were to run that program again and I happen to get scheduled first, well, I don't have anything to do before the barrier, so I'll wait for Olivia. - When I eventually get scheduled to execute, I'll complete my operations, then joint the barrier. - [Both] Break! - Now I'm free to continue doing whatever else I need to do. I'm going to see if we have any salsa. - And I can double the chips on our shopping list. Although the order in which our threads got scheduled was different, the end result is the same. We need eight bags of chips for the party.

## Barrier: Python demo

- To show you how to implement a barrier in Python, we'll build on the previous example that demonstrated a race condition by creating 10 shopper threads names Olivia and Barron that either add or multiply the number of chips to buy. Without a barrier in place, this program has a race condition that produces a different final result each time we run it. So we'll use a python barrier object to make sure all five of the Olivia shopper threads execute their add operation before the five Barron shoppers multiply the bags of chips. The barrier object is included in Pythons threading module and will create a new barrier object at the top of the program, called Fist Bump, by typing threading dot barrier. The constructor takes an argument for the number of threads to wait on the barrier before it releases and since this program instantiates 10 shopper threads, five of them are Barron and five are Olivia and we want all of them to arrive at the barrier before the program continues, we'll give it an input of 10. Now that we've created the barrier, it's time to figure out where to use it. I'm starting all of my shopper threads together at roughly the same time. And I can't control when each one will get scheduled to execute. But all I really care about here is making sure that all of the Olivia threads execute their addition operation before the Barron's execute their multiplication. So I'll use my barrier to separate those operations. The Olivia threads will execute their addition operation before waiting at the barrier. Whereas the Barron threads will go straight to the barrier and wait there. And then once all 10 threads have arrived at the barrier and are waiting on it, the barrier will release. And then the Barron threads will execute their multiplication operations. To implement that for the Olivia shopper, I'll add my code to wait on the barrier after adding three bags of chips release the pencil lock by calling Fist Bump dot wait after the with block on line 24. Up in the Barron shopper function, I'll put the fist bump dot wait method before the with statement to acquire the pencil and double the chips. Now when I run this program, all of the Olivia shoppers execute their addition operation first and then all of the Barron shoppers do their multiplication afterwards. And that gives me a final total of 512 bags of chips. And if I run the program again, I'll always get the same answer.

## Asynchronous Task

## Computational graph

- The key to parallel programming is determining which steps within a program can be executed in parallel and then, figuring out how to coordinate them, and one tool to help model how steps in a program relate to each other is a computational graph. Consider the steps to make a very simple salad. I'll need to chop some lettuce, chop some tomatoes, mix those chopped ingredients together, and then finally add salad dressing. Each of those steps represents a task which is a unit of execution or a unit of work. I can draw those tasks as nodes in a graph and use arrows to indicate progression from one task to the next. A task cannot begin executing until all of the tasks with arrows feeding into it have completed. When those four tasks are arranged sequentially like this, they represent a single path of execution which could be implemented as a single thread or process. If I do those steps in that order, I'll make a somewhat boring salad, but that's not the only ordering that could work. The tasks of chopping lettuce and chopping tomatoes can occur asynchronously, meaning the order in which they happen relative to each other doesn't really matter. I could chop the lettuce first or the tomatoes first, or ideally, I'll chop them both in parallel to save some time. So, I'll add another node to the front of my diagram labeled Spawn which has two output arrows leading in to the two asynchronous chopping tasks. Both tasks can begin running at any time after the Spawn operation. Now, there is a dependency between those two chopping tasks and the third task because I can't mix the chopped ingredients together until both of the chopping tasks are complete. My program will need some form of synchronization, so I'll add another node to represent that operation. This Sync node will not be able to execute until both of the chopping tasks that feed into it are complete. I've used the terms Spawn and Sync here, but you'll also see the terms Fork and Join used in this context. The fact that there are not any direct edges or connections between the chopped lettuce and chopped tomato tasks indicates the possibility for parallel execution. So if this program was implemented using two threads as shown here with a second thread spawning or forking from the main thread, the two chopping tasks can run at the same time. This type of diagram is called a directed acyclic graph or DAG. Directed because each edge is directed from one node or vertex to another, and acyclic meaning it doesn't have any loops that cycle back on itself. There are several variations and ways to draw these types of computational graphs, but their general-purpose is to provide an abstract representation of the program. They help to visualize the relationship and dependencies between tasks. They can also be used to get a sense of how parallel a program can be. Every node represents a task or operation and for each one I'll indicate the amount of time it takes to execute. For this example, I'm just using made-up numbers in units of seconds. If I add together the execution times for all of the nodes, that gives me a metric called work which represents the time it would take to execute all of these tasks on a single processor. For this example, that comes out to 60 seconds. Next, I'll identify the longest possible path through this graph, following the directed edges from node to node, which is referred to as the critical path. It represents the longest series of sequential operations through the program. If I add together the times for all of those nodes along the critical path, I get another metric called span which indicates the shortest possible execution time if this program was parallelized as much as possible. In this case, that's 45 seconds. The ratio of work to span indicates the ideal parallelism of this program. How much faster can the parallel version of this program possibly run using multiple processors than the sequential version running on just one processor? The parallelism ratio of 1.33 means that at very best, the parallel version of this program will be 33% faster than the sequential version. We may not be able to reduce the total amount of work a program needs to do, so minimizing the critical path is important in designing parallel algorithms, because span determines the shortest possible execution time.

## Thread pool

- After identifying the tasks in a program that can run asynchronously, one way to run those tasks in parallel is to create independent threads or processes for each of them. Preparing a basic salad requires us to chop lettuce and chop tomatoes. So Olivia and I will act as independent threads to execute those tasks in parallel on our two processors, the knives. - A salad with just lettuce and tomatoes is so boring. We need something more. What about cucumbers? - Sure, chopping cucumbers is another task that can run asynchronously, so we'll spawn another thread to handle that. Hey, Barron. - Hey, Barron. - We need onions too. What's up, Olivia? - What's up? - Another task, another thread. - And mushrooms. - Again, another thread. - And carrots, and celery, and peppers, and an eggplant. - Whoa, whoa, whoa. It's getting crowded in here. We've got a lot of threads in the kitchen, but only two processors to execute on. That means a lot of threads will be standing around waiting their turn. Although threads are considered to be lightweight, every time we spawn a new thread, it does require some amount of overhead in terms of processor time and memory. Or, in this case, kitchen space. In some scenarios, rather than creating a new thread for every single task, it can be more efficient to use a thread pool, which creates and maintains a small collection of worker threads. As the program submits tasks to the thread pool, the thread pool reuses those existing worker threads to execute the tasks. Submitting tasks to a thread pool is like adding them to a to-do list for the worker threads. Now, Olivia and I are two workers in a pool, and we have a queue of tasks, our vegetables, waiting for us to chop. After one of us finishes executing our current task, we'll take another one from the queue. - I'm done chopping the tomatoes, so I'll move onto the cucumbers. - Reusing threads with a thread pool addresses the overhead involved with creating new threads, and that becomes a real advantage when the time it takes to execute the task is less than the time required to create a new thread. - It doesn't take long to chop one of the vegetables, but it does take a long time to call up our friends to take on each of these individual tasks. - Since our threads already exist, when a new task arrives, we eliminate the delay of thread creation, which can make our program more responsive.

## Thread pool: Python demo

- To create a thread pool in Python, we'll be using the ThreadPoolExecutor Class which is part of the concurrent.futures module. It provides a high-level interface for asynchronously running tasks rather than working with individual threads directly. Under the hood, the ThreadPoolExecutor manages a pool of threads so that you don't have to manually create new threads for each task. You simply submit callable objects to the ThreadPoolExecutor, and it assigns them to existing threads in its thread pool to run asynchronously. To demonstrate that, we've created this program which defines a function named vegetable_chopper on line six that simply prints a message, which includes the name of the current thread and a vegetable ID number, which gets passed as an input argument to the function. Down in the main section, we use a for loop on line 11 to manually create and start 100 vegetable_chopper threads. I'll run this program by typing python thread_pool.py and it creates 100 separate threads to chop 100 vegetables. And I see all of their unique names in the output, ranging from Thread-1 to Thread-100. Now let's accomplish that same task of chopping 100 vegetables but do so with a thread pool instead. First, we'll need to import the ThreadPoolExecutor class from the concurrent.futures module with from concurrent.futures import ThreadPoolExecutor. Then down in the main section, we'll create a new ThreadPoolExecutor named pool and I'll use the max _workers parameter to limit it to only having five threads. By default, if you do not specify a value for max_workers, it gets set to none, which will make the ThreadPoolExecutor create up to five times as many threads as there are processors in the system. This is based on the assumption that thread pools are commonly used to overlap IO bound tasks rather than CPU intensive tasks, so the number of threads should exceed the number of processors. Now within the for loop, rather than creating a new thread object, I'll submit the vegetable_chopper function to the thread pool. I'm submitting it as a callable object, and I'll pass the vegetable variable for the vegetable ID argument. Finally, after the loop finishes, I'll call pool.shutdown and that will close down the thread pool. Calling shutdown will free up any resource that the pool is using after all of its remaining submitted tasks have finished executing. You can't submit any more tasks after calling shutdown. By default, the wait parameter is set to True, which prevents the shutdown method from returning until all of the pending tasks have finished and the resources have been freed. I'll run the program with those changes, and the output still prints 100 messages, because the program chops 100 vegetables. I submitted 100 tasks to the thread pool. But now the thread names are different. I can see that all of these threads belong to pool number zero, but the thread numbers are between zero and four. We're reusing the same five threads in the pool to execute all 100 tasks.

## Process pool: Python demo

- Python's ThreadPoolExecutor is great when you have a bunch of IO-dependent tasks to perform. However, if the tasks are primarily CPU intensive, then there isn't much benefit to running them all in concurrent threads due to the global interpreter lock. The workaround in Python is to use multiple processes. So, in addition to the ThreadPoolExecutor, the Python concurrent.futures module also has a ProcessPoolExecutor. To modify our previous example to use the ProcessPoolExecutor instead, I'll change line five from ThreadPoolExecutor to ProcessPoolExecutor. And then, I'll make a similar change on line 12. And that's it. Now, the program will use a pool with multiple processes instead of multiple threads to execute the vegetable chopper tasks I submit to it. Now, one more improvement I can make to this program, both the ThreadPoolExecutor and the ProcessPoolExecutor are designed to be used with a context manager. So, I'll modify line 12 and turn it into a with statement. So, with the ProcessPoolExecutor as pool, and then, I'll move the for loop inside of that. The context manager automatically shuts down the pool when it's done, so I can delete the explicit shutdown method on line 15. I'll save and run that program. And all of the outputs say that the MainThread chopped a vegetable. So, what's going on here? Well, we created a pool with five separate processes in it, and each of those processes has its own MainThread, which will execute the vegetable chopper task that's assigned to it. So, when the vegetable chopper function prints out the name of the current thread, that will always be one of the five main threads. So, rather than display the current thread's name, I'll modify the code to display the current process ID by importing the os module, and then, replacing the call to the current thread.get name method, with os.getpid, for process ID. I'll save that change. Now, when I run the program, we see the process ID numbers along the left side, and the same five PIDs show up over and over, because those processes are being reused.

## Future

- Launching asynchronous tasks is a great way to accomplish multiple things at once. Olivia, can you go check how many vegetables are in the pantry? - Sure, I can do that. - While Olivia is busy asynchronously counting veggies, my thread is free to continue doing other work. But now she's gone, and I need a way to get that result back from her when she's done. This is where a mechanism called a Future can be used. A Future acts as a placeholder for a result that's initially unknown, but will be available at some point in the future. It provides a mechanism to access the result of an asynchronous operation. I like to think of a Future like an IOU note for the result. Hey, Olivia. - Hey, you! - Hey, I need you to check how many vegetables are in the pantry and give me back an answer. - Sure, I promise to do that. And here's an IOU note that I'll get that answer. - Thank you. Now I've got a handle to see that future result. And I'll hold onto it as I continue doing other work in the kitchen. Eventually, I may reach a point in my work that I need the result from Olivia, perhaps to make a decision or complete some type of computation. The Future is read-only, and I see that the result isn't ready yet. So all I can do is wait until Olivia finishes. When she finally does, she'll write the result value to that Future, which is called resolving or fulfilling it. She's fulfilling her promise to get me an answer. And I see we have zero veggies left in the pantry. - We chopped them all up making salads. - Well, at least now I have an answer. And now I need to make a trip to the store.

## Future: Python demo

- When you submit a task to an executor in Python, the submit method returns an instance of the future class, which encapsulates the asynchronous execution of the callable task. The future class has several methods that can be used to check the status of the task's execution. Cancel it if needed. And most importantly, it has a method named result which is used to retrieve the return value after the call has completed. To demonstrate that in action, I'll recreate my interaction with Olivia from the previous video, starting with this basic shell of a program. First, I'll import the thread pool executor class from the concurrent.futures module, and I'll also import the time module. Then I'll create a new function named how many vegetables, and this will serve as our callable task. Within that function, I'll print a message that Olivia is counting vegetables. Then I'll have the function sleep for three seconds. And finally, the function will return a value for the number of vegetables in the pantry. For simplicity, let's say that's always 42. Now, down in the main section, I'll print a message at the beginning of the program that says Barron asks Olivia how many veggies are in the pantry. Then I'll create a new thread pool executor to use with the context manager. And I need parentheses here. And within that, I'll use the executor's submit method to pass it the how many vegetables function as a callable object to execute. And I'll capture the future object that it returns as a variable named future. I'll print a quick message to say that Barron can do other things while he waits for the result. And then I'll use one final print statement to show the result value from the future using the future.result method. Oliva responded with future.result. Calling the future object's result method will give me the return value from that function. However, if the future has not completed execution yet, then invoking the result method will block and wait until it's ready. I'll save all those changes. I'll switch over to a console and run that program. And it immediately prints out the first two messages from the main method, and the message from the callable that Olivia is counting veggies. However, that last print statement was blocked, waiting for the future to complete. After the three second wait in the how many veggies function, the main method finally gets the result value from the future, and prints the message that Olivia responded with 42.

## Divide and conquer

- One class of algorithms that are well-suited for parallel execution across multiple processors are divide and conquer algorithms. They work by first dividing a large problem into a number of smaller subproblems of roughly equal size. Next, the conquer phase recursively solves each of those subproblems, and finally, the solution to the subproblems are combined together to produce the overall solution for the original problem. The common structure for dividing conquered code usually consists of an if/else statement. If the algorithm has reached what's called a base case, meaning the problem has been subdivided into a small enough piece to solve directly, then simply solve it. Otherwise, following the else case, divide the current problem into two smaller pieces referred to as the left and right problems. Solve both of those problems recursively using the same divide and conquer strategy. Then, combine the left and right solutions. Consider the task of summing together a large number of elements. I have an array of shopping receipts here and I need to add them all together to figure out how much we spent buying all those vegetables. If I were doing that alone as a sequential process, I would simply iterate through the receipts in the array and accumulate their values. Five plus 13 is 18 plus seven is 25 plus three is 28. - That's going to take forever. Let's divide and conquer this task. You add this half of receipts and I will add this half. We've subdivided this task into two subtasks that each of our threads can execute. - Do you want to subdivide it into even smaller parts? - Sure. We can continue to recursively divide the receipts into smaller and smaller subgroups until we eventually reach our base case. For some algorithms, the base case may require you to continue recursively subdividing until you reach individual elements. But for our purpose, we can define our base case to stop when we reach a threshold amount of receipts. At that point, we'll add each of those subgroups of receipts together, then reverse or unwind the recursion to combine the results from all of the subproblems to get our final answer. - My half of the receipts add up to 41. - And mine add up to 62, so we spent a total of $103 on groceries. Divide and conquer algorithms lend themselves to being made parallel because each of the subproblems are independent, so they can be executed in parallel on different processors. - Now, just because a divide and conquer algorithm can be made parallel doesn't mean it's always advantageous to do so. Depending on the size of the problem set and the complexity of the operations involved, the cost and overhead involved in making the algorithm parallel may outweigh the potential benefits.

## Divide and conquer: Python demo

- To demonstrate a parallel divide and conquer algorithm in Python, we'll implement a function that recursively sums together all of the integers between two values. It should return the same result as we would get by calling Python's built in sum function on that same range of numbers. We'll start with a single threaded implementation of the recursive sum routine. Which takes two input parameters on line four, a low value, and a high value representing the range of numbers to sum over. The if statement on line five looks at the difference between the low and high values to determine if the problem has been sufficiently subdivided. And if so, we've reached the base case, so it returns the sum of numbers in that range. Otherwise, the else statement beginning on line seven determines the middle index between low and high, then recursively calls the recursive sum function on the low to middle index. Which is referred to here as the left half. And from the middle to high index, the right half. Then, it returns the sum of those left and right halves. Down in the main section, this program simply calls the recursive sum function on line 14 for a range of one to a million, and then prints the result. Switching over to the console, I'll run this program by typing Python divide and conquer dot py. And it gives me this really big number as the output. Which is the sum of all the numbers from one to a million. Now, let's modify this algorithm to take advantage of parallel execution. Summing together numbers is a cpu intensive operation. So, to get around Python's global interpreter lock, I'll need to implement this using multiple processes. From the concurrent futures module... I'll import the process pool executor. And a function called as underscore completed. And I'll explain what that does later. Next, I'm going to add an optional named argument to the function definition to pass in the process pool. And by default, that will be none. At the beginning of the function, I'll check to see if that pool argument is none. And if so, that indicates that we're in the initial call to disfunction. So, if pool is empty, then I'll create a process pool executor. And within that, I'll call the recursive sum function from low to high. And this is where I'll pass in that newly created process pool executor to the pool parameter. Now, I'll also capture the output from this recursive sum call with a variable named futures. And we'll come back to use that part a little bit later. When the recursive sum function calls itself recursively, the pool input will be populated. So. I'll add an else clause after that. And I'll indent the rest of the code to be within that else block. In the base case on line 12, rather than directly returning the sum function over the low to high range, I'll submit that sum function to the process pool. Pool dot submit. And I'll pass in the low to high range as its argument. This call to the submit function will return a future object. And I'm going to wrap this in square brackets so that it actually becomes a list containing that future object. We'll see why in just a moment. Below that, I'll update the recursive calls to disfunction for the left and right halves, to also pass along the process pool. Since the base case on line 12 is returning a list that contains a future, the return statement on line 17 will concatenate the list of futures from the left and right halves. As the recursion unwinds, and those left and right lists get concatenated together, we ultimately end up with a list containing futures for all of the subdivided tasks on line nine. From there, we'll just need to retrieve their individual results and sum them together. And we can do that with just a single line of Python code using comprehensions. Return. Sum. And I'll do f dot result. For f in as underscore completed. Futures. The as underscore completed function returns an iterator that yields futures from the list of futures as they complete. So, the for loop will cycle through those completed futures and retrieve their result, and then it passes those to the sum function to be added up. Now that this program is ready to run, I'll save my changes. Switch over to the console. And when I run it, I get the same output value as before. Except now we're using multiple processes to perform those calculations in parallel.

## Evaluating Parallel Performance

## Speedup, latency, and throughput

- There are several reasons for using multiple processors to execute a program in parallel. One reason, might be to increase the size of the problem you can tackle in a certain amount of time. For example, we're going to a party and I promise to bring 10 cupcakes. Working by myself, I can decorate 10 cupcakes in one hour, they're very fancy cupcakes, but if Baron joins as a second processor, doing the same type of work in parallel, together we can decorate 20 cupcakes in one hour. This type of parallelization is called Weak Scaling, we're keeping the size of the problem we're bringing in more processors to accomplish more work in the same amount of time. - Another reason for parallelization and bringing in more processors is to accomplish a given task faster. If Olivia promised to bring 10 cupcakes to the party, then working alone, it would take her one hour to decorate all of them but if we split the workload, so she'll do half and I'll do half then working together in parallel we can decorate those 10 cupcakes in only about 30 minutes. This is called Strong Scaling and it involves breaking down and spreading a problem across multiple processors to execute the program faster. In those two examples, we're using parallel processors to do more work in a set amount of time or do a set amount of work in less time. In either case, we're increasing the program's overall throughput, that is, the number of tasks it can be complete in a given amount of time. Throughput is related to another important metric, called latency, which is the amount of time it takes to execute a task from beginning to end. Latency is measured in units of time, so if it takes six minutes to decorate one cupcake, that's a latency of six minutes. Throughput is expressed in actions per unit of time, so the throughput of one processor, that is Olivia working alone, is 10 cupcakes per hour, two processors working in parallel will have the same latency of six minutes to decorate each cupcake but their combined throughput increases and with three processors the throughput goes even higher to 30 cupcakes per hour. - A metric that's commonly used to measure the effectiveness of a parallel program is speedup, which is related to the program's efficiency. Speedup is calculated as a ratio of the time it takes to execute the program in the optimal sequential manner with just one worker or a single processor, over the time it takes to execute in a parallel manner with a certain number of parallel processors, so if one worker takes an hour or 60 minutes, to make 10 cupcakes but two workers can do the same job in only 30 minutes, that corresponds to a speedup of two. If adding a third worker drops the time to 20 minutes, that's a speedup of three. - Now, our simplified cupcake example really represents a best case scenario. Because a task like decorating cupcakes can be completely parallelized among multiple workers, but in practice, that's rarely the case. It's more common to have programs where some parts can be parallelized , but other parts can't. Let's say at the end of our cupcake decorating program, we need to pack the finished cupcakes into this container. If only one of our threads can interact with the shared container at a time, we'll have to take turns using it, so that part of our program, we'll have to execute sequentially, and that creates a limit on the amount of speedup and that creates a limit on the amount of speedup we can possibly achieve. we can possibly achieve.

# Amdahl's law

- There is a well-known equation for estimating the speedup that a parallel program can achieve called Amdahl's law, which is named after the computer scientist that formulated it. It's a way to estimate how much bang for your buck you'll actually get by parallelizing a program. In this equation for Amdahl's law, p represents the portion of a program that can be made parallel and s is the speedup for that parallelized portion of the program running on multiple processors. So for this example, if 95% of our cupcake decorating program can be executed in parallel and doing so with two processors produces a speedup of two for that part of the program, then the theoretical speedup for the entire program is about 1.9, which is a little bit less than two. If we add a third processor to increase the speedup for the parallelized portion to three, then the overall speedup will be around 2.7. Using four processors gives us a speedup of 3.5, and so on. Now, let's say we spend a lot of money and buy a computer with 1000 processing cores. Instinctively, I would expect that to give me a speedup of somewhere at least close to 1000, that'd be great. But according to Amdahl's law, the overall speedup with 1000 processors will only be around 19.6. If I go wild and bump it up to a million processors, the overall speedup only increases to slightly less than 20. - 95% of our program is parallelizable, but the 5% that's not, the part of the program that has to execute sequentially, that's creating an upper limit on the speedup we can achieve. A million processors might be able to execute the parallel portion of the program in a blink of an eye, but that sequential 5% will still take the same amount of time as it would with just one processor. This chart shows the estimated speedup that can be achieved when 95% of a program can be parallelized. As the number of processors is increased from left to right, the speedup rises until it eventually maxes out at 20. If only 90% of a program can be parallelized, then at best, we'll get a speedup of 10. A 75% parallelizable program has a maximum speedup of four. And if only 50% of a program can be executed in parallel, then even with an infinite number of processors, the best we can achieve is a measly speedup of two. If that's all we can get, then we might decide that it's not worth the effort to write the program so it will run in parallel. Amdahl's law illustrates why using multiple processors for parallel computing is only really useful for programs that are highly parallelizable. - When first learning about parallel programming, it's natural to be excited and want to parallelize everything. Computers have lots of cores nowadays, so why not make everything as parallel as possible? That's a common trap to fall into. Just because you can write programs to be parallel, doesn't mean you always should, because the costs and overhead associated with parallelization can sometimes outweigh the benefits. Amdahl's law is one handy tool to estimate the benefits of parallelizing a program to determine whether or not it makes sense to do so.

## Measure speedup

- Amdahl's law is great for estimating the speedup that you might achieve by parallelizing a program, but once you've created that parallel program, it can be useful to actually measure its speedup empirically. Speedup is calculated as the ratio of two values, the time it takes for the program to execute sequentially divided by the time it takes to execute in its parallel implementation. That means we'll actually need to take two separate measurements to calculate speedup. First we'll see how long the algorithm takes to execute with a single processor. Now, this doesn't mean just take the parallel version of the program and run it with only one processor, because the parallel program will include additional overhead that isn't necessary to run sequentially. We want to measure the best possible implementation of that algorithm written with sequential execution in mind. - I've got my stopwatch ready. Let's see how fast you can add up these receipts by yourself. - Well, the fastest way to do that work working alone is to simply iterate through the stack of receipts and accumulate their totals. - Ready, set, go. - Done! $103. - 25 seconds. - Great. That's our sequential baseline. Now let's try that again working together using a divide and conquer approach that's structured for parallel execution. - Okay, ready, set, go. - Done! $103. - 17 seconds this time around. - Cool, we can calculate speedup now. 25 seconds divided by 17 seconds gives us a speedup of 1.47. Working together in parallel made us almost 1 1/2 times faster. - Not too shabby considering there are only two of us. But if this had been a result with many more processors to help with the work, then a speedup of only 1.47 doesn't sound that impressive. - True, speedup is a great metric, but it doesn't paint the whole picture. Another metric to consider is efficiency, which indicates how well system resources, like additional processors, are utilized. We can get a rough calculation for efficiency by dividing the speedup by the number of processors. So, with just two processors, Olivia and me, we achieved a speedup of 1.47, which means we were 73.5% efficient. And I think that's pretty good. Now, let's say we increase the number of processors in our system to eight, but doing so only produces a speedup of 2.2. Now we're only running at 27.5% efficiency. Our program did not scale well to utilize those additional processors. Measuring speedup and efficiency gives us a sense of how well our parallel program is actually performing. As long as you achieve a speedup that's greater than one, you know you've achieved at least something by making the program parallel. - But if your speedup is less than one, you're better off just running the sequential algorithm. - Now, a few recommended things to consider when benchmarking a program's performance. It's important to limit the number of programs running at the same time, so they don't compete with the program Also since execution scheduling and other background tasks, like garbage collection, can change how long a program takes, from run to run, I like to measure the execution time for multiple independent runs of the program and then average those times together. use just-in-time compilation to compile parts of the program at runtime. when it first starts up, so you want to let the environment warm up before you begin taking measurements. Some programming languages may have compiler options or runtime settings that can address those concerns, but as a very simple warmup, I always like to run the algorithm I'm going to measuring I always like to run the algorithm I'm going to measuring once before I actually run and measure it once before I actually run and measure it to make sure things, like the cache, are in a somewhat consistent state from run to run. are in a somewhat consistent state from run to run.

## Measure speedup: Python demo

- To demonstrate how I measure the speedup of a parallel program in Python I'll be using the recursive sum algorithm that we created in an earlier video which uses a parallel divide-and-conquer approach to sum together all of the numbers within a range of values. The parallelized implementation of that algorithm is contained within the par_sum function on line 13. But since we've already covered how that works, I'll use code folding to hide it for now. The sequential summing algorithm to use for comparison is in the function on line nine, which simply uses the built-in sum function over the specified range. I'll hide that too for now. The main section of this program contains a simple framework that I like to use to evaluate performance when developing parallel algorithms. On line 28 we have a variable to indicate the number of evaluation runs to measure each implementation and then we'll average those times together. Increasing that number to average together more runs will decrease the impact of run-to-run variability. However, that also makes the program take longer to run. For each run the algorithm will sum the numbers from one to the SUM_VALUE on line 29 which is currently set to 100 million. Lines 31 through 38 contain the code to measure the sequential execution time. Before I begin timing anything I run the sequential sum function once on line 32 to capture its output. And doing this also serves as a rudimentary way to warm up the system. On line 33 I create a variable to accumulate all of the sequential run times, and then I use a for loop to execute the sequential algorithm for the number of eval runs. At the beginning of each run I capture the current system time on line 35 with the variable start. Then I execute the sequential algorithm, and when it's done I calculate the elapsed time and add it to the accumulator. Finally after all of those evaluation runs I divide the accumulated sequential time by the number of eval runs on line 38 to get the average. The next block of code on lines 40 through 47 accomplishes the same thing but for the parallel algorithm instead. After those parallel evaluation runs complete, the final block of code displays the results. The if statement on line 49 checks to make sure that the sequential and parallel algorithms both produce the same result and if they did not something went wrong so it throws an exception. Otherwise the last four print statements display the average sequential and parallel execution times along with the corresponding speedup and efficiency based on the number of processors in the system. Now when I run this program by typing python measure_speedup.py I see a message that it's beginning the sequential test, then after a bit, the parallel test, and then the results pop up. The sequential algorithm took an average of a little over 7,000 milliseconds whereas the parallel algorithm only took an average of about 760 milliseconds. That corresponds to a speedup of 9.23 and an efficiency of 38.45% which is calculated based on the 24 logical processors in this computer. With those results in hand I can try to make adjustments and tweak my parallel algorithm, and then run the benchmark again to see of those changes increase or decrease its performance. For example, I could try changing the base case threshold for my parallel divide-and-conquer algorithm on line 19 from 100,000 to just 10,000. If I save and run the program again, when it finishes I can see that making that change actually decreased my parallel performance to the point that it's even slower than the sequential algorithm. This framework provides an easy way to evaluate the relative performance of a parallel program. So think of it as a tool you can use to investigate your parallel algorithms to hopefully make them better.

## Designing Parallel Programs

## Partitioning

- We've looked at a lot of mechanisms for implementing concurrent and parallel programs and considered the concepts and challenges associated with them. Now it's time for the big question. How do you actually design a parallel program? Over the next few videos, we'll look at a common four-step methodology for taking a problem and developing a parallel solution for it. This methodology can be used to design complex programs that run on large-scale parallel systems. And not all parts of it are applicable to writing simple desktop applications like we've done in this course, but the concepts are still good to understand. The four stages can be summarized as partitioning, communication, agglomeration, and mapping. That first stage, partitioning, is about breaking down the problem into discreet chunks of work that can be distributed to multiple tasks. At this beginning stage, we're not concerned with practical issues like the number of processors in our computer. We'll consider that later. For now, our goal is to simply decompose the problem at hand into as many small tasks as possible, and there are two basic ways to approach partitioning, domain decomposition and functional decomposition. Domain, or data decomposition, focuses on dividing the data associated with the problem into lots of small and, if possible, equally-sized partitions. The secondary focus is then to consider the computations to be performed and associating them with that partition data. For example, if Oliva and I need to decorate this tray of cupcakes, that's a two-dimensional array of data elements we need to process. So, we can use domain decomposition to split that work into two similar tasks. We could break the array into two blocks, I'll handle decorating this half, and you take the other block. - Or, we could break up elements cyclically - Sure! That's just another way to break up this data and different ways of decomposing data can have different advantages and disadvantages depending on the problem and hardware involved. Once we've partitioned the data, we can turn our focus towards the processing that needs to be applied to each section. - The other form of decomposition, functional decomposition, provides a different, complimentary way to break down the problem. Rather than focusing on the data being manipulated, functional decomposition begins by considering all of the computational work that a program needs to do. And then divides that into separate tasks that perform different portions of the overall work. The data requirements for those tasks are a secondary consideration. For example, to functionally decompose making cupcakes, we would first break out all of the individual tasks that go into making them. Then, from there, we continue on to consider the data involved with each of those tasks. Keep in mind that domain and functional decomposition are complimentary ways to approach a problem. And it's natural to use a combination of the two. Programmers typically start with domain decomposition because it forms a foundation for a lot of parallel algorithms. But sometimes taking a functional approach instead can provide different ways of thinking about these problems. It's worth taking the time to explore alternative perspectives. It can reveal problems or opportunities for better optimization that would be missed by considering data alone. by considering data alone.

## Communication

- After decomposing the problem into separate tasks, the next step in our design process is to establish communication, which involves figuring out how to coordinate execution and share data between the tasks. - Hang on a sec. Do we always need communication? - Well, my dear, communication is the foundation of a good relationship. - Yah, yah, but I was talking about data. Some problems can be decomposed in ways that do not require tasks to share data between them. Consider the job of frosting these cupcakes. If I'm tasked to add frosting to this one, and you're tasked to add frosting to that one, even though we're operating on adjacent elements in this array, there's no need for us to communicate with each other. They're completely independent tasks. This is embarrassingly easy to make parallel. - Sure, we could spend our quality family time together in the kitchen not talking to each other, but what if there is a need to share data between tasks. Let's say we want to decorate the cupcakes to have a rainbow pattern across them. That would require each task to know information about the color of its neighboring cupcakes. I need to know what color you're making your cupcakes, so I can color my cupcakes accordingly. Although our separate tasks can execute concurrently, we're no longer completely independent from each other. In this type of situation, we might establish a network of direct point-to-point communication links between neighboring tasks. For each link, one task is acting as the sender, or producer of data, and the other task that needs it is the receiver or consumer. That type of local point-to-point communication can work when each task only needs to communicate with a small number of other tasks. - But if your tasks need to communicate with a larger audience, then you should consider other structures for sharing data between multiple tasks. You might have one task that broadcasts the same data out to all members of a group or collective, or it scatters different pieces of the data out to each of the members to process. Afterwards, that task can gather all of the individual results from the members of the group and combine them for a final output. When operations require this type of global communication, it's important to consider how it can grow and scale. Simply establishing point-to-point pairs may not be sufficient. If one task is acting as a centralized manager to coordinate operations with a group of distributed workers, as the number of workers grow, the communication workload of a central manager grows too, and may turn it into a bottleneck. This is where strategies like divide and conquer can be useful. in a way that reduces the burden on any one task. These are just a few high-level structures to serve as a starting point as you begin to plan the communications for a parallel program. - A few other factors to consider include whether the communications will be synchronous or asynchronous. Synchronous communications are sometimes called blocking communications, because all tasks involved have to wait until the entire communication process is completed to continue doing other work. That can potentially result in tasks spending a lot of time waiting on communications instead of doing useful work. Asynchronous communications, on the other hand, are often referred to as nonblocking communications, because after a task sends an asynchronous message, it can begin doing other work immediately, regardless of when the receiving task actually gets that message. You should also consider the amount of processing overhead a communication strategy involves, because the computer cycles spent sending and receiving data are cycles not being spent processing it. Latency is another factor to consider, the time it takes for a message to travel from point A to B, expressed in units of time, like microseconds. And bandwidth, which is the amount data that can be communicated per unit of time, expressed in some unit of bytes per second. Now if you're just writing basic multi-threaded or multi-processed programs to run on a desktop computer, some of these factors like latency and bandwidth, probably aren't major concerns, because everything is running on the same physical system. But as you develop larger programs that distribute their processing across multiple physical systems, those inter-system communication factors can have a significant impact on the overall performance. can have a significant impact on the overall performance.

## Agglomeration

- In the first two stages of our parallel design process, we partitioned a problem into a set of separate tasks and established communication to provide those tasks with the data they needed. We looked at different ways to decompose the problem and focused on defining as many small tasks as possible. That approach helped us consider a wide range of opportunities for parallel execution. However, the solution it created is not very efficient, especially if there are way more tasks than there are processors on the target computer. - Now it's time to turn our thinking from abstract to something concrete and modify that design to execute more efficiently on a specific computer. In the third agglomeration stage, we'll revisit the decisions we made during the partitioning and communication stages to consider changes to make our program more efficient, combining some of those tasks and possibly replicating data or computations. As a parallel program executes, periods of time spent performing usable computations are usually separated by periods of communication and synchronization events. The concept of granularity gives us a qualitative measure of the time spent performing computation over the time spent on communication. Parallelism can be classified into two categories based on the amount of work performed by each task. With fine-grained parallelism, a program is broken down into a large number of small tasks. The benefit is that lots of small tasks can be more evenly distributed among processors to maximize their usage, a concept called load balancing. The downside is that having lots of tasks increases the overhead for communication and synchronization, so it has a lower computation-to-communication ratio. On the other end of the spectrum, coarse-grained parallelism splits the program into a small number of large tasks. The advantage is that it has a much lower communication overhead, so more time can be spent on computation. However, the larger chunks of work may produce a load imbalance, where certain tasks process the bulk of data, while others remain idle. Those are two extremes and the most efficient solution will be dependent on the algorithm and the hardware on which it runs. For most general purpose computers, that's usually in the middle with some form of medium-grained parallelism. - When we partitioned our cupcakes earlier, we took a fine-grained approach. The array has twelve elements that need to be frosted, so we decomposed that into twelve separate tasks, one for each cupcake. As we evaluated communication, we determined that each cupcake task will need to share data with the four other cupcakes surrounding it to coordinate their colors to form a rainbow pattern, which would require 34 communication events. In addition to that being a lot of communication, twelve tasks is way more than the number of processors in our kitchen. There's only two of us. - Then let's agglomerate and combine some of those tasks. Since we only have two processors, Olivia and me, we'll restructure the program into two tasks that are each responsible for frosting six of the cupcakes. That reduces the amount of communications between those tasks, from 34 down to just 2, because everything else is handled locally within the task. However, now each time they communicate, they'll have to share more information to convey the status of the three cupcakes along that edge. Now we have two tasks and two processors. Perfect. - Hey guys, I heard you need some help frosting cupcakes. - (frosting splat) - Oh, Steve! We just restructured our program into two tasks, and now with Steve, we have three processors available, and that's too many cooks. - Too many cooks? - Too many cooks. One of us would be sitting idle, while the other two cooks are busy processing. It's easy to make short-sighted decisions like this that can limit a program's scalability. Choosing to restructure our program into just two tasks prevented us from taking advantage of Steve's additional processing power. A well-designed parallel program should adapt to changes in the number of processors, so keep flexibility in mind. Try not to incorporate unnecessary, hard-coded limits on the number of tasks in the program. If possible, use compiled time or run time parameters to control the granularity.

## Mapping

- The fourth and final stage of our parallel design process is mapping. And this is where we specify where each of the tasks we established will actually execute. Now this mapping stage does not apply if you're only using a single process or system because there's only one place to execute the program or if you're using a system with automated task scheduling. So if I'm just writing programs to run on a desktop computer, like the examples we've shown you throughout this course, mapping isn't even a consideration. The operating system handles scheduling threads to execute on specific processor cores, so that's out of our hands. Mapping really becomes a factor if you're using a distributed system or specialized hardware with lots of parallel processors for large-scale problems, like in scientific computing applications. The usual goal of a mapping algorithm is to minimize the total execution time of the program, and there are two main strategies to achieve that goal. You can place tasks that are capable of executing concurrently on different processors to increase the overall concurrency, or you can focus on placing tasks that communicate with each other frequently on the same processor to increase locality by keeping them close together. In some situations, it might be possible to leverage both of those approaches, but more often, they'll conflict with each other, which means the design will have to make trade-offs. There's a variety of different load-balancing algorithms that use domain decomposition and agglomeration techniques to map task execution to processors. If the number of tasks or the amount of computation and communication per task changes as the program executes, that makes the problem more complex, and it may require dynamic load-balancing techniques that periodically determine a new mapping strategy. Designing a good mapping algorithm is highly dependent on both the program structure and the hardware it's running on, and that gets beyond the scope of this course. So to summarize the four-step parallel design process, we start by taking a problem and partitioning or decomposing it into a collection of tasks. Then we evaluate the communication necessary to synchronize and share data between those tasks. After that, we agglomerate or combine those tasks into groups to increase the program's efficiency with certain hardware in mind. And then finally, those tasks get mapped to specific processors to actually execute.

## Welcome to the challenges

- All right, folks. You've seen how to design parallel programs. Now, it's time for a few challenge problems to practice using the concepts we've covered in this course. - Over the next few videos, we'll present you with several algorithms that could benefit from being restructured for concurrency, and to execute in parallel. As a starting point, we'll provide you with the sequential version of each algorithm, and it's your job to implement your own concurrent solution. - Keep in mind, there's not a single correct answer to these challenges. So, take some time to think them through. And when you're done with each challenge, be sure to watch our solution videos to see how we approach the problems.

## Challenge: Matrix multiply in Python

- Your goal for this challenge is to design and build a parallel program that calculates the product of two matrices, which is a common mathematical operation that can benefit a lot from parallel computation. Each matrix will be represented in Python as a two-dimensional list of lists containing numeric values. The first dimension of the list structure indexes rows of the matrix and is usually represented with the variable letter i. The second dimension indexes columns and is represented with the letter j. So for example, if the first index value is zero, that refers to the top row of the matrix. And if the second value is two, that identifies the element at index two, along that top row. It's common to represent the individual elements of a matrix using notation with two subscripts like this, with the first subscript indicating the row, and the second indicating the column. When it comes to multiplication, two matrices can be multiplied together if the number of columns in the first matrix, A, is equal to the number of rows in the second matrix, B. So for example, Matrix A shown here is a four by two matrix, and B is a two by three matrix. Since those inner dimensions are the same, they can be multiplied. The product of A times B, which I'll call Matrix C on the right, will have dimensions based on the number of rows in A and the number of columns in B. Each element in Matrix C is the product of the corresponding row in A and column in B. So for example, element C two comma one is the product of row two for Matrix A and column one from B. Written as an equation, C two one equals a two zero times b zero one plus a two one times b one one, and so on. If I replace the subscripts in that equation with the variables i and j, it's a little easier to see how that first index i corresponds to the row in A, and the second index j corresponds to the column in B. That's the end of the short math lesson on how matrix multiplication works. To give you a starting point for this challenge, we've already implemented a sequential version of matrix multiplication in the example program. The sequential matrix multiply function on line 10 takes in two two-dimensional matrices to multiply named A and B. The first few lines of the function establish a few useful variables for the number of rows and columns in A and B, which come in useful when writing calculations. Also, the if statement on line 16 checks to make sure that A and B have valid dimensions to be multiplied together and will raise an error if they don't. Below that, line 19 initializes a two-dimensional list of lists, named C, to hold the result matrix. Then it uses a set of nested for loops to iterate through each of the rows in A and columns in B. The third for loop on line 22 sums together the products of elements from the row in A and column in B and stores the accumulated sum in the corresponding location of the result Matrix C. Finally, after all of the values in Matrix C have been populated, it's returned as the output on line 24. Below the sequential matrix multiplier is another function on line 27 named parallel matrix multiply. I've copied the lines of code to set up the useful variables for the rows and number of columns, but after that, the function is empty and that's where you come in. Your job for this challenge is to implement your own parallelized version of matrix multiplication that will hopefully execute faster than the sequential version. Down in the program's main method, we've implemented our simple framework for measuring a parallel program speedup, which we covered in an earlier video. After you implement your parallel matrix multiply function, you'll be able to run this program to see how well it performs. A few quick tips. Don't feel like you have to keep your code within this one function. Feel free to create any additional helper functions that you need. Also, matrix multiplication is a CPU intensive task, so you'll almost certainly need to use the Python multiprocessing package. For our solution, we ended up using a shared memory array to share data between multiple processes. Now that goes beyond what we've covered in this course, so if you want to use a shared memory array for your own solution, you can read more about them on the Python documentation page for the multiprocessing package under the section on sharedctype objects. This is definitely a tough challenge. Good luck.

## Solution: Matrix multiply in Python

[[**Example 1**](https://github.com/manwar/python-guide/blob/master/examples/concurrent-and-parallel-programming/part-1/CH06/06_02/begin/matrix_multiply.py)] - 
[[**Example 2**](https://github.com/manwar/python-guide/blob/master/examples/concurrent-and-parallel-programming/part-1/CH06/06_03/begin/matrix_multiply.py)]

- To design our parallel solution for the matrix multiplication challenge, we began with domain decomposition to consider the ways we could partition the problem. One very convenient aspect of matrix multiplication is that every element of the result matrix C can be calculated independently. For example, calculating element C two, one only requires knowledge of row two from Matrix A and column one from Matrix B. Likewise, calculating C zero, two only requires row zero from A and column two from B. The elements of Matrix C don't need to know anything about the other elements in C. Calculating the individual elements of C for a four by three result matrix can be partitioned into 12 independent tasks. This type of problem is sometimes called embarrassingly parallel because it breaks apart so easily and doesn't require communication between each of the tasks. That can turn into a lot of tasks, especially for a large result matrix. So we decided to agglomerate those tasks based on row and will modify the number of rows in each group at run time, based on the number of processors that are available in the computer. If the system only has two processors, then we'll combine the computation into two tasks. If it has four processors, then that's four tasks, and so on. Since these computations are CPU-intensive, we'll be using Python's multi-processing package to create tasks as separate processes rather than separate threads to get around the limitations of the Global Interpreter Lock. Our matrix multiplication solution consists of two functions, which I have hidden here using code folding. The par_matrix_multiply function on line 27 and a helper function below it on line 63 named _par_worker, which is responsible for calculating the results for a subset of rows in the total solution matrix. I'll expand the par_matrix_multiply and looking at the primary multiplier function, on line 37, it performs a quick check to determine if the output matrix will be less than 25,000 elements. We did some testing on our own computer and found that if the output was smaller than that, then the sequential algorithm would usually be faster, so it calls that on line 38. Below that, we get the number of available processors on line 41 and then divide the rows of the output matrix into roughly equal-sized chunks. Line 43 initialized a shared memory array to hold the elements of the result matrix C, so that each of the helper par worker processes can put their portion of the result directly into it. Python's multiprocessing package provides two options to allocate an array of shared memory that can be accessed by multiple processes. The standard multiprocessing array uses a lock to synchronize access among the processes to prevent a data erase. The multiprocessing.RawArray on the other hand, does not include synchronization. If multiple processes would be reading and writing the same array elements, then you should stick with the array that has built-in synchronization. However, we determined that the matrix multiplication is embarrassingly parallel and that the calculation for each output element could be treated as a completely independent task. So, we decided to use a raw array because it allows the program to run a lot faster because it doesn't have to deal with synchronization. The output C will be a two-dimensional matrix with dimensions of the number of rows in A by the number of columns in B. However, the shared memory array is only one-dimensional. We named it C_1D and treated its contents as a flattened version of the 2D result matrix. Each of the parallel worker processes were assigned a portion of the one-dimensional C array to fill in with results. And then at the end, we converted the flattened one-dimensional C array into a two-dimensional list of lists as the final output. Lines 44 through 53 contain the code that creates and runs each of the worker processes that calculate a subset of the solution and put the results in the shared one-dimensional array. After that, lines 56 through 60 reshape the one-dimensional array into the two-dimensional lists of lists for the final output. If you look down at the parallel worker function beginning at line 63, the set of three nested for loops is very similar to the sequential matrix multiplication algorithm. We just changed the indices on line 64 to process a subset of the rows in A and modify the indexing math on line 67 to insert elements into a one-dimensional array instead of the two dimensional list of lists. So, that's an overview of how our solution works. To demonstrate this program, I'll switch over to a console and I'll press Control + Shift + Escape to open the Task Manager and then go to the Performance tab. If you're using a Mac computer, you can check out the same properties in the Activity Monitor. Now I'll run this program by typing python matrix_multiply.py. And it's set to multiply two matrices together with 1,000 by 1,000 elements each. So this is going to take a while to run. When it begins executing the sequential algorithm, I see the CPU usages stays at around 5% because I'm only using one of the 24 processors in this computer. For the purposes of this demo, we'll go ahead and speed this along. When the parallel algorithm beings running, the CPU usage rises up to 100% because now we're utilizing all of the available processors. The parallel version will also take a few minutes to run so it's speed that up as well. When the program finishes, I see that the parallel algorithm produces a speed-up of seven compared to the sequential version, which corresponds with 29% efficiency with my 24 processors. I'd say that's pretty good. Now I'll only see these type of good results if the input matrix is large enough. If I reduce the matrices from being 1,000 by 1,000 to just 200 by 200, save that change and run the test again, it completes much faster, only taking a few seconds. But the speed-up, by experience, ends up being less than one because the overhead involved in the parallel version makes it less efficient than the sequential algorithm when the problem size is that small. Our solution is just one way to tackle this problem so if you came up with a different approach, that's great. Compare it with our solution to understand the advantages and disadvantages of each.

## Challenge: Merge sort in Python

- For our second challenge problem, your goal will be to design and build a parallel version of the classic merge sort algorithm to sort an array of random integers. Merge sort is a well-known divide-and-conquer algorithm for sorting the elements in an array. During the divide phase, it recursively splits the array into two halves, referred to as the left half and the right half. That continues until the subarrays have been recursively divided down to their smallest, individual unit. Those subarrays are considered to be sorted because they only have one element, and then from there the merge phase repeatedly merges those subarrays to produce new, larger sorted subarrays and it continues until there is only one subarray remaining, which is the final sorted result. To give you a starting point for this challenge, we've already implemented a sequential version of the merge sort algorithm in this program. The sequential merge sort function on line nine takes in the array of integers to be sorted along with optional arguments, which will be the indices of the subset of array to sort when this function is called recursively. Technically, this code uses a Python list to hold the data, instead of an actual array and we did that intentionally to avoid requiring an external dependency with the NumPy library to include actual arrays. But the way the merge sort algorithm works is the same with a list, and we will refer to it as an array throughout this video and in the code. If the merge sort function is called without any args, that indicates the first time it is being called. It calls itself recursively on line 11, passing in zero and the length of the array minus one as the left and right indices to sort the entire array. On subsequent calls, the else clause on line 13 will execute. If the left index is less than the right index, then there are still multiple items in the subarray, so it will need to be divided further. Line 16 calculates the middle index between the left and right points, then the merge sort function recursively calls itself to sort the left half from the left to middle index, and then the right half from the next item after the middle up to right index. Finally, it calls the merge function on line 19 to merge the two sorted subarrays. Merge is where the magic really happens in this program. The merge function takes in left, middle, and right indices, which indicate the left and right subsections of the array to merge. When the merge function gets called, both of those two subsections will already have been individually sorted. And then the merge function combines them into a single sorted section. Feel free to examine the merge function's code, but to use it you really don't need to know how it works. The one key thing to understand is that merges sorted values into the original array. That means this merge sort algorithm is sorting the array in place. The Python list object that gets passed as the input array to merge sort function will be modified. Now, down below the merge function on line 51 is an empty function named par merge sort. Your job for this challenge is to implement your own parallelized version of the merge sort function. Feel free to create any helper functions or additional classes that you need, and to reuse sections of the code from the sequential version. In particular, we recommend reusing our merge function to tackle this challenge. Down in the program's main section, we've implemented our simple framework for measuring a parallel program speed-up, so you can evaluate how well your parallel program performs compared to the sequential version. Good luck.

## Solution: Merge sort in Python

- For our solution to the merge sort challenge, we used a recursive divide and conquer approach with multiple processes conquering the task of sorting subsections of the array. For the divide phase, rather than recursively subdividing the array until it reaches single elements, we configured our base case to subdivide the array based on the number of processors in the computer. For example, if the computer only had four processors, then it would only go through two layers of subdivision to produce four sub-arrays in need of sorting. We then spawn additional processes that use the sequential merge sort algorithm to sort each of those sub-arrays and then the main process merges the results back together. By limiting the depth of recursion in our base case, we're able to use a few processes, only as many as we have processors in the system to sort large sections of the array. When the par merge sort function is called the first time, without any additional args, the if statement on line 53 will execute. Rather than making copies and passing large portions of the array to each of the helper processes, we initialized a shared raw array that all of the sub-processes will be able to interact with and we initialize it with the values from the original input array. Since each sub-process will be manipulating and sorting a separate section of the overall shared array, they will not be reading and writing the same elements. So we can use the raw array class, which does not include synchronization. If we had used the regular multiprocessing array, which does include built in synchronization, than the program would run significantly slower, as the processes compete with each other for the lock. When we call the par merge sort function recursively, on line 55, in addition to passing initial left and right indices, we also pass a third argument to track the depth of the recursion. After the recursive calls have all finished, line 56 inserts the results stored in the shared array back into the original array before returning it. Down in the else clause, the map on line 60 determines if the recursive calls have reached a sufficient depth based on the number of processors in the system. And if so, it executes the sequential merge sort algorithm. Otherwise, the function continues to subdivide the array further. On line 64, it creates and starts a new process to run the parallel merge sort algorithm on the left half of the array. Then on line 66, it uses the current process to recursively sort the right half. After both of those halves have been sorted, it finally calls the merge function on line 68, which is the exact same merge function we used in the sequential version. So, that's how our solution works. On line 73, I have the program configured to generate and sort a one million element array of integers. Before I run this program to test it out, I'll press control + shift + escape to bring up the task manager, and select the performance tab. I'll bring up a console on the other half of the screen, and I'll run this program by typing python merge_sort.py. When it finishes, I can see that when we're sorting a million numbers, the speed up of the parallel version is almost four times as fast as the sequential version. Now, if the input array was a lot smaller. Let's lower it down to just one thousand. And I'll run that program again. It finishes a lot faster, but when sorting that small of an array, the parallel version takes way longer than the sequential version due to the overhead involved in creating those additional processes. Now, let's go in the other direction and create a really big array with ten million points. I'll save and run that version. As the sequential algorithm runs, the CPU usage stays around 5% and this will take a while, so let's fast forward. When the parallel implementation runs, we can see the CPU usage spike to 100%. Notice that the CPU usage stays high for a short period of time and then drops back down to 5% for another long stretch. The period of high CPU usage happens when the separate processes, they're using the sequential merge sort algorithm to sort their separate sub-arrays, but then at the end, when those sub-arrays are re-merged, that occurs in a single main process, so the usage is back down to the regular 5%. After the program finishes, I see that the sequential algorithm took a little over 200 seconds to sort ten million elements, but the parallel algorithm only took around 35 seconds to sort those elements. And that corresponds to a speed up of 5.8. I'd say that's pretty good.

## Challenge: Download images in Python

- For third and final challenge problem your goal will be to design and build a program to concurrently download a collection of image files from the internet and return the total number of bytes. We have 50 images hosted at the URLs shown here which you can use at the target files to download for this challenge. All of the JPEG files on that server are numbered sequentially from one to 50. To give you a starting point for this challenge, we've already implemented a sequential version of an image downloader in this example program. The sequential download images function on line 11 takes in a list of integers representing the image numbers to download. It simply uses a for loop to iterate through that list and passes each number to the download image helper function which downloads the corresponding file and then returns the number of bytes. The function accumulates those return values in the total bytes variable and then returns the final result on line 15. We created the download image helper function on line 18 for you to use in your solution for this challenge so you don't have to worry about figuring out the URL request code. Since we only have 50 images hosted on our target website, the code on line 19 takes whatever number you pass into this function and forces it to be between one and 50. So it corresponds to a valid image URL. After that, the method builds the URL, downloads the file, and then returns the total number of bytes. Below that on line 31 is the empty shell for a parallel download image function. Your job for this challenge is to implement your own parallelized function to download the list of images. We highly recommend reusing our download image helper function\ as you build your solution. Good luck.

## Solution: Download images in Python

- For our solution to the download images challenge, we considered each of the images that needed to be downloaded as separate tasks, which would each be a call to the download image helper function. Since we would need to get the result value, for the number of bytes from each of these tasks, it seemed like a perfect use case for futures, with each task being created as a callable object, to be executed by a thread pool. So, that's what we did. Line 33 of our implementation of the parallel download images function, establishes a new thread pool, with the default settings. So, it will have up to five times as many threads, as there are processing cores in our machine. On the next line, we use a Python list comprehension, to submit all of the image numbers as tasks for the download image function, and store the returned future objects in a list. The for loop on line 35 then retrieves those future objects from the list, as they complete, retrieves the result's value, and then adds that to the accumulator variable named total bytes. After all of the submitted tasks have been completed, and the result values added together, the function returns the final total byte value on line 37. Now, before I run this program to test it out, I'll press Control Shift Escape, to open the task manager, and go to the performance tab, and I'll open a console on the other side of the screen. I'll run this program by typing python download_images.py, as the Sequential Implementation is executing, notice that the CPU usage is still really low, only around 1%, so it isn't even utilizing a full processor. However, the ethernet usage increases, fluctuating around 1-3 megabits per second. Now, let's fast forward. When the Parallel Implementations starts running, the ethernet usage spikes up even higher. But, the CPU usage is barely affected. It's still utilizing less than a single processor. When the program finishes, I see the parallel version had a speed up of over 31, and on my 24-core machine, that corresponds to a processor efficiency of 132%. Which, seems unrealistic. The multi-threaded version was definitely a lot faster, so, let's talk about why. The previous two challenges with Matrix multiplication, and Merge sort, were both CPU-bound problems. And that's why we used multiple processes, that could execute in parallel, to get around Python's global interpreter lock. As their parallelized code ran, the CPU usage spiked up towards 100%, because all of those processes we created, were running full time, on all of the processors, to calculate a solution. This challenge, on the other hand, is an I/O Bound problem. The limiting factor in how fast we can download all of the images, is not the number of processors, it's our internet speed, we could add 100 more processors to this system, and it wouldn't make a difference, because the program barely utilized even one processor. So, rather than structuring this program to use multiple processes, we chose to use lighter-weight threads instead. Creating several concurrent threads improved the program's performance, because they could respond and do work, as their requested data was downloaded, and became available. They would have not benefited much, if any, from being able to execute in parallel. Python's global interpreter lock was not limiting us this time. We saved this challenge for the end, to emphasize the importance of understanding the limiting factors in a problem, so you can address the right issues, to get the best performance out of your programs. 
