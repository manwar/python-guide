Course: https://www.linkedin.com/learning/python-parallel-and-concurrent-programming-part-1

Learn parallel programming basics
- There's an old saying that two heads are better than one. - Well, when it comes to parallel programming I say two threads are better than one. I'm Barron Stone. - And I'm Olivia Stone. In this course, we'll introduce you to the fundamental concepts for concurrent and parallel programming. - These are the basic mechanisms you need to develop programs that can do multiple things at once, to take advantage of multi-core processors, and parallel hardware. - [Olivia] We'll focus on the general concepts and relate them back to everyday activities in the kitchen to make those ideas easier to understand and have some fun. - [Barron] Then, to cement those abstract ideas, we'll demonstrate them in action using the Python programming language. If you're new to concurrent and parallel programming, this is a great place to start. - [Together] Let's get to it.

What you should know
- The purpose of this course is to give beginner and intermediate programmers a basic understanding of how to write concurrent and parallel programs. To get the most out of it you should have some programming experience but you don't need to be an expert. The videos of us working in the kitchen will demonstrate the key concepts in general terms, and then in the Python code examples I'll explain the unique terminology and nuances that make Python different than other programming languages. If you're interested in concurrent and parallel programming in other languages be on the lookout for different versions of this course. You'll find the same general concept videos but we'll show you the example code in another programming language.

Sequential vs. parallel computing
- Let's start by looking at what parallel computing means and why it's useful. Why it's worth the extra effort to write parallel code. A computer program is just a list of instructions that tells a computer what to do. Like the steps in a recipe that tell me what to do when I'm cooking. Like a computer, I simply follow those instructions to execute the program. So to execute the program, or recipe, to make a salad, I'll start by chopping some lettuce and putting it on a plate. (knife chopping) Then I'll slice up a cucumber and add it. (knife slicing) Next, I'll slice and add a few chunks of tomato. (knife slicing) I'll try not to cry while I slice the onion. (knife slicing) And finally, I add the dressing. Done. As a single cook working alone in the kitchen, I'm a single processor, executing this program in a sequential manner. The program is broken down into a sequence of discrete instructions that I execute one after another. And I can only execute one instruction at any given moment. There's no overlap between them. This type of serial or sequential programming is how software has traditionally been written. And it's how new programmers are usually taught to code, because it's easy to understand, but it has its limitations. The time it takes for a sequential program to run is limited by the speed of the processor and how fast it can execute that series of instructions. I'll slice and chop ingredients as fast as I can, but there's a limit to how quickly I can complete all of those tasks by myself. Each step takes some amount of time and in total, it takes me about three minutes to execute this program and make a salad. That's my personal speed record and I can't make a salad any faster than that without help. - That's my cue. Two cooks in the kitchen represent a system with multiple processors. Now we can break down the salad recipe and execute some of those steps in parallel. - While I chop the lettuce, - I'll slice the cucumber. (knives chopping and slicing) - And when I'm done chopping lettuce, I'll slice the tomatoes. - And I'll chop the onion. (knives chopping and slicing) - And finally, I'll add some dressing. - Hold on. Now it's ready. - Finally, the dressing. - Working together, we broke the recipe into independent parts that can be executed simultaneously by different processors. While I was slicing cucumbers and onions, Barron was chopping lettuce and tomatoes. That final step of adding dressing was dependent on all of the previous steps being done. So we had to coordinate with each other for that step. By working together in parallel, it only took us two minutes to make the salad, which is faster than the three minutes it took Barron to do it alone. Adding a second cook in the kitchen doesn't necessarily mean we'll make the salad twice as fast, because adding extra cooks in the kitchen adds complexity. We have to spend extra effort to communicate with each other to coordinate our actions. - And there might be times when one of us has to wait for the other cook to finish a certain step before we continue on. Those coordination challenges are part of what make writing parallel programs harder than simple sequential programs. But that extra work can be worth the extra effort, because when done right, parallel execution increases the overall throughput of a program, enabling us to break down large tasks to accomplish them faster or to accomplish more tasks in a given amount of time. Some computing problems are so large or complex, that it's not practical or even possible to solve them with a single computer. Web search engines that process millions of transactions every second are only possible thanks to parallel computing. - In many industries, the time saved using parallel computing also leads to saving money. The advantages of being able to solve a problem faster often outweighs the cost of investing in parallel computing hardware.

Parallel computing architectures
- Parallel computing requires parallel hardware with multiple processors to execute different parts of a program at the same time. But before you dive into writing software, it helps to understand how different types of parallel computers are structured. One of the most widely used systems for classifying multiprocessor architectures is Flynn's Taxonomy, which distinguishes four classes of computer architecture based on two factors: the number of concurrent instruction or control streams, and the number of data streams. The class names are usually written as four letter acronyms that indicate whether they have single or multiple instruction streams and data streams. For example, SIMD stands for Single Instruction, Multiple Data. The simplest of these four classes is the Single Instruction, Single Data or SISD architecture which is a sequential computer with a single processor unit. If I'm an SISD computer, at any given time I can only execute one series of instructions, such as chopping, and I can only act on one element of data at a time, this carrot. (knife chops) - It's simple, like an old computer. The next class in Flynn's Taxonomy is Single Instruction, Multiple Data, or SIMD, which is a type of parallel computer with multiple processing units. All of its processors execute the same instruction at any given time, but they can each operate on different data element. As an SIMD computer, our two processors are both executing the same chopping instruction. But I'm chopping celery as my data while Barron chops a carrot. - And we'll execute those instructions in sync with each other. (two knives chop) This type of SIMD architecture is well suited for applications that perform the same handful of operations on a massive set of data elements like image processing. And most modern computers use graphic processing units, or GPUs, with SIMD instructions, to do just that. Our third class is the opposite of SIMD. In a Multiple Instruction, Single Data or MISD architecture, each processing unit independently executes its own separate series of instructions, however, all of those processors are operating on the same single stream of data. That's like Olivia executing the chopping instruction while I execute a different peeling instruction, but we're both chopping and peeling the same carrot at the same tine. - Yeah, we're not doing that. - As you can see, MISD doesn't make much practical sense so it's not a commonly used architecture. - But our fourth and final architecture is. In a Multiple Instruction, Multiple Data, or MIMD computer every processing unit can be executing a different series of instructions, and at the same time each of those processors can be operating on a different set of data. Now I can slice celery while Barron peels carrots. (knives peel and slice) - MIMD is the most commonly used architecture in Flynn's Taxonomy, and you'll find it in everything from multi-core PCs to network clusters in supercomputers. Now that broad MIMD category is sometimes further subdivided into two parallel programming models which also have four letter names. Single Program, Multiple Data, or SPMD, and Multiple Program, Multiple Data, MPMD. In the SPMD model, multiple processing units are executing a copy of the same single program simultaneously, however, they can each use different data. That might sound a lot like the SIMD architecture from earlier, but it's different, because although each processor is executing the same program, they do not have to be executing the same instruction at the same time. The processors can run asynchronously and the program usually includes conditional logic that allows different tasks within the program to only execute specific parts of the overall program. If Olivia and I are both following the same recipe or program, I can execute part of it, while Olivia's processor handles a different task. This SPMD model is the most common style of parallel programming, and when we show you programming examples later in this course we'll structure the code as a single program and execute it on a multi-core desktop computer which is an MIMD architecture. - Now if each of our processors is executing a different recipe, that represents the Multiple Program, Multiple Data, or MPMD model. In this scenario, processors can be executing different independent programs at the same time while of course also be operating on different data. Typically in this model one processing node will be selected as the host, or manager, which runs one program that farms out data to the other nodes running a second program. Those other nodes do their work and return their results to the manager. MPMD is not as common as SPMD but it can be useful for some applications that lend themselves to functional decomposition, which we'll cover later on.

Shared vs. distributed memory
- In addition to a parallel computer's architecture, which can be categorized using Flynn's Taxonomy, another aspect to consider is, uh- - Memory, it's important to understand how the memory's organized and how the computer accesses data. - Right, you could put a billion processors in a computer but if they can't access memory fast enough to get the instructions and data they need then you won't gain anything from having all those processors. Computer memory usually operates at a much slower speed than processors do, and when one processor is reading or writing to memory that often prevents any other processors from accessing that same memory element. There are two main memory architectures that exist for parallel computing, shared memory and distributed memory. In a shared memory system all processors have access to the same memory as part of a global address space. Although each processor operates independently, if one processor changes a memory location all of the other processors will see that change. So if I change something in our shared memory space- - Hey that potato is two potatoes. - Every other processor sees that change, too. Now the term shared memory doesn't necessarily mean all of this data exists on the same physical device. It could be spread across a cluster of systems. The key is that both of our processors see everything that happens in the shared memory space. Shared memory is often classified into one of two categories: uniform memory access and non-uniform memory access, which are based on how the processors are connected to memory and how quickly they can access it. In a uniform memory access or UMA system, all of the processors have equal access to the memory, meaning they can access it equally fast. There are several types of UMA architectures, but the most common is a symmetric multiprocessing system or SMP. An SMP system has two or more identical processors which are connected to a single shared memory, often through a system bus. In the case of modern multi-core processors, which you find in everything from desktop computers to cell phones, each of the processing cores are treated as as a separate processor. For this course we'll be focused on parallel programming within the SMP architecture. In the example code we show you we'll be running on a multi-core desktop computer. Now in most modern processors each core has its own cache, which is a small, very fast piece of memory that only it can see. And it uses it to store data that it's frequently working with. However, caches introduce the challenge that if one processor copies a value from the shared main memory, and then makes a change to it in its local cache, then that change needs to be updated back in the shared memory before another processor reads the old value which is no longer current. This issue called cache coherency is handled by the hardware in multi-core processors so we will not go into detail on it for this course, but it's something you should be aware of if you find yourself working with larger, more complex parallel computing systems. The other type of shared memory is a non-uniform memory access or NUMA system, which is often made by physically connecting multiple SMP systems together. The access is non-uniform because some processors will have quicker access to certain parts of memory than others. It takes longer to access things over the bus. But overall, every processor can still see everything in memory. These shared memory architectures have the advantage of being easier for programming, in regards to memory, because it's easy to share data between different parts of a parallel program. The downside is that they don't always scale well. Adding more processors to a shared memory system will increase traffic on the shared memory bus. And if you factor in maintaining cache coherency it becomes a lot of communication that needs to happen between all the parts. In addition to that, shared memory puts responsibility on the programmer to synchronize memory accesses to ensure correct behavior. But we'll look into that later. - Okay, that's enough about shared memory. In a distributed memory system, each processor has its own local memory with its own address space. So the concept of a global address space doesn't exist. All of the processors are connected through some sort of network, which can be as simple as ethernet. Each processor operates independently, and if it makes changes to its local memory that change is not automatically reflected in the memory of other processors. If I make a change to the data in my memory, Baron's processor is oblivious to that change, it's up to the programmer to explicitly define how and when data is communicated between the nodes in a distributed system, and that's often a disadvantage. - Communication is always tough. - The advantage of a distributed memory architecture is that it's scalable. When you add more processors to the system you get more memory, too. This structure makes it cost-effective to use commodity, off-the-shelf computers, and networking equipment to build large distributed memory systems. Most supercomputers use some form of distributed memory architecture, or a hybrid of distributed and shared memory. - But for this course we'll stick with simple shared memory in an SMP architecture.

Thread vs. process
- When a computer runs an application that instance of the program executing is referred to as a process. A process consists of the program's code, its data, and information about its state. Each process is independent and has its own separate address space and memory. A computer can have hundreds of active processes at once, and an operating system's job is to manage all of them. - Now, within every process there are one or more smaller sub-elements called threads. These are kind of like tiny processes. Each of those threads is an independent path of execution through the program. A different sequence of instructions. And it can only exist as part of a process. Threads are the basic units that the operating system manages, and it allocates time on the processor to actually execute them. - To conceptualize the relationship between a process and its threads, think of Olivia and I cooking together in the kitchen as being two threads executing as part of the same process. - We both work independently doing our own tasks that contribute to the overall execution of our program. For example, if we are part of a process to make a salad, my thread might handle retrieving vegetables from the pantry and fridge. - And I'll handle chopping them up. If the program requires other tasks, we might create additional threads to handle those those too. Hey Steve! - Hey! - Can you make some salad dressing? - I sure can! - Now our salad-making process has three active threads. And when one of those threads finishes executing its instructions-- - All done. - It'll exit and leave the remaining threads to continue what they're doing. - Threads that belong to the same process share the process' address space which gives them access to the same resources and memory including the program's executable code and data. You can think of the kitchen that our two threads are working in like the shared address space for our process. We both have direct access to the same cookbooks containing our instructions or code, and the ingredients that we're cooking with represents the data and variables we're manipulating. The ability for both of us to use these resources is certainly convenient. And it enables us to easily work together. But it does create the potential to cause problems if we don't coordinate our actions, as we'll see later in this course. - Sharing resources between separate processes is not as easy as sharing between threads in the same process. Because every process exists in its own address space, its own separate kitchen. In this process, our two threads are making a salad. But that other process next door is running a different program, those threads are baking a cake. - Our variables and data are isolated to this address space, this kitchen, so the threads in the other process can't directly access our salad data. - Good! We don't want you healthy salad-makers messing with our cake either! - There are ways to communicate and share data between processes, but it requires a bit more work than communicating between threads. - You have to use system-provided Inter-Process Communication mechanisms like sockets and pipes, allocating special inter-process shared memory space, or using remote procedure calls. Which is beyond the scope of what we'll be discussing in this course. - Now it's possible to write parallel programs that use multiple processes working together towards a common goal, or using multiple threads within a single process. - So which is better? Using multiple threads or multiple processes? - Well, like most things in programming, it depends. It depends on what you're doing and the environment it's running in. Because the implementation of threads and processes differs between operating systems and programming languages. If your application is going to be distributed across multiple computers, you most likely need separate processes for that. But, as a rule of thumb, if you can structure your program to take advantage of multiple threads, stick with using threads rather than multiple processes. Threads are considered light-weight compared to processes, which are more resource-intensive. A thread requires less overhead to create and terminate than a process, and it's usually faster for an operating system to switch between executing threads from the same process than to switch between different processes.

Concurrent vs. parallel execution
- Just because a program is structured to have multiple threads or processes does not mean they'll necessarily execute in parallel. A concept that's closely related to parallel execution, but often gets confused with it, is concurrency. Concurrency refers to the ability of an algorithm or program to be broken into different parts that can be executed out of order, or partially out of order, without affecting the end result. Concurrency is about how a program is structured and the composition of independently executing processes. Consider this recipe to make a salad, which includes several steps that involve slicing and chopping vegetables. We can decompose those steps into a collection of concurrent tasks, because the relative order in which we do them doesn't matter, they're order independent. To keep things simple, let's just focus on two of those tasks for now. I'll chop onions. - And I'll slice cucumbers. This knife represents our computer's processor. We only have one knife, so, this is a single-core processor, and only one of us will be able to execute our vegetable-chopping routine at any given time. - We'll have to take turns, you go first. - Thanks. My thread will use the processor to execute and slice some cucumbers. Then, after a bit, we'll swap places. - Now my thread gets some time to execute and slice onions. - I want to slice now. - So, we'll swap places again, and we'll keep on doing this until we're both done. In this scenario, we're running concurrently, because our two independent processes overlap in time. However, since we only have a single processor, only one of us will actually be executing at any instant in time. If we swap places and take turns more frequently, it might create the illusion that we're executing simultaneously on our single processor, but this is not true parallel execution. - To actually execute in parallel we need parallel hardware. In our kitchen, that means another knife and cutting board, a second processor, but in regards to computers, parallel hardware can come in a variety of forms. Most modern processors used in things like desktop computers and cell phones have multiple processing cores. Graphics processing units, or GPUs, contain hundreds, or even thousands, of specialized cores working in parallel to make amazing graphics that you see on the screen. And computer clusters distribute their processing across multiple systems. Since we've structured ourselves as concurrent operations, I can begin slicing cucumbers with this processor. - While I cut onions with the other one. Now we're actually executing in parallel because we're both executing at the same time, and as a result, we're able to finish the job faster. Concurrency is about the structure of a program being able to deal with multiple things at once, whereas parallelism is about simultaneous execution, actually doing multiple things at once. Those things could be related, like chopping vegetables, but they don't have to be. Concurrency enables a program to execute in parallel, given the necessary hardware, but a concurrent program is not inherently parallel. - And programs may not always benefit from parallel execution. For example, the software drivers that handle I/O devices, like a mouse, keyboard, and hard drive, need to execute concurrently. They're managed by the operating system as independent things to get executed as needed. In a multi-core system, the execution of those drivers might get split amongst the available processors. However, since I/O operations occur rather infrequently, relative to the speed at which computer operates, we don't really gain anything from parallel execution. Those sparse independent tasks could run just fine on a single processor, and we wouldn't feel a difference. Concurrent programming is useful for I/O-dependent tasks, like graphical user interfaces. When the user clicks a button to execute an operation that might take awhile, to avoid locking up the user interface until it's completed, we can run the operation in a separate concurrent thread. This leaves the thread that's running the UI free to accept new inputs. - That sort of I/O-dependent task is a good use case for concurrency. Parallel processing really becomes useful for computationally-intensive tasks, such as calculating the result of multiplying two matrices together. When large math operations can be divided into independent subparts, executing those parts in parallel on separate processors can really speed things up.

Global interpreter lock: Python demo
- Using threads to handle concurrent tasks in Python is fairly straightforward. However, the Python interpreter will not allow those concurrent threads to execute simultaneously and parallel, due to a mechanism called the global interpreter lock, or GIL. It's something that's unique to Python and important to address up front. The GIL is a mechanism in Python that prevents multiple Python threads from executing at the same time. That means if your program is written to have 10 concurrent threads, only one of them can execute at a time while the other nine wait their turn. This may seem like an odd limitation, but remember that under the hood, your Python program is actually being executed by a program called an interpreter. The interpreter compiles your Python program into an intermediate bytecode which is then executed with a virtual machine along with any necessary modules from the library. The default and by far most widely used interpreter is CPython, and as its name suggests, its underlying code is written in a mix of C and Python. The global interpreter lock was implemented as a simple way to provide thread-safe memory management in CPython by only letting one Python thread execute at a time. There have been several proposals to eliminate the GIL from CPython because of its negative impact on parallel performance, but in most cases, the advantages of having the GIL outweigh its disadvantages. There are other implementations of Python that do not have the GIL including Jython, which is Java based, IronPython, which is .NET based, and PyPy-STM. But CPython is the default and most popular interpreter so the GIL lives on as a contentious element of Python. Now just because CPython has the GIL does not mean there's no value in writing multi-threaded Python programs. Many applications are I/O-bound, meaning they're mostly waiting on external actions, like network operations or user input. For those types of I/O-intensive tasks, the GIL does not create a significant bottleneck and you can gain a lot by using Python's threading module to implement multiple concurrent threads. On the other hand, if your application is CPU bound and spends most of its time performing CPU-intensive computations, then the GIL can negatively impact multi-threaded performance, but there are ways to get around it. Because Python is an interpreted language, it's inherently slow, so processor-heavy algorithms are often written using faster compiled languages like C++. And then included in function libraries that a top-level Python program can call into. Those operations can then execute outside of the GIL's restrictions using parallel threads. If you want to keep everything written in Python, the workaround is to use Python's multiprocessing package to implement your program with multiple processes instead of multiple threads. Each Python process will be its own instance of the Python interpreter with its own GIL, so the separate processes can execute in parallel. The downside here is that communication between processes is a bit more complicated than between threads, and creating multiple processes uses more system resources than creating multiple threads. For most of the Python examples in this course, we'll stick with using the threading module to demonstrate the programming concepts with concurrent threads. As you'll see, even with the GIL restricting execution, multi-threaded Python programs can still run into a variety of problems that you'll need to protect against. The GIL does not exist to keep you safe. When we reach examples later in this course that involve more CPU-intensive tasks, we'll switch over to using the multiprocessing package so you can see how to use that, as well.

Multiple threads: Python demo
- We'll start with a demonstration, using Python's threading module to create several concurrent threads and investigate their impact on this computer's CPU usage. But before diving into code, let's first take a look at the number of processors that are available on this computer, which I'll be using for demonstrations throughout this course. To do that, I'll press control, shift, escape to open the task manager, and then select the performance tab. Down at the bottom, I can see that this computer has 12 cores and 24 logical processors. Those numbers mean this computer has 12 separate, complete physical processing cores, and each of those cores supports something called hyper-threading, which enables them to each run two independent applications, at the same time. So the computer treats those 12 physical cores, as 24 logical processors. Now, the hyper-threading in those 12 cores, does not mean I'll get double the performance out of them. Hyper-threading takes advantage of unused parts of the processor, so if one thread is paused, or not using a specific resource, then the other thread may be able to use it. Under certain work loads, that can create performance improvements, but it's highly application dependent. The blue, moving graph shows the total percentage of CPU utilization for all of those processors. I don't have much running right now, so the usage stays low, down near one per cent. If I want to see the CPU usage for each of those processors individually, I can get more information by clicking the open resource monitor link at the bottom of the task manager. And selecting the CPU tab. The charts on the right show the total CPU usage on top, and if I scroll down, I can see how much each of the individual processors on this computer are being utilized. The table on the left lists all of the current processes running on this computer with information including each processes unique process ID number, its PID, its current status, the number of threads, and it's average CPU usage. Now, to show you a few Python threads running in a process, I've created a short example program, which you can find in exercise files, chapter 2, oh 2 oh 4, end, multiple threads, dot py. This program defines a simple function on line 8, called CPU waster, which has a while loop that will spin forever. It doesn't do any useful work, but the thread running that function will stay alive forever and continuously use CPU cycles. Lines 13 through 16 print out information about the program, including its process ID number, the total number of threads in the process, and then the for loop on line 15 prints information about each of those threads. After that, it creates and starts 12 time waster threads, using a for loop on line 19. I chose to start 12 threads here, because that's half as many processors as there are in this system. Don't worry about how the code on line 20 works for now, we'll get to that in a later video. Finally, after starting those threads, the program prints out the same process information again, on lines 23 through 26. Now, I'll switch over to a command prompt, that's already navigated to the folder with that script, and I'll type Python, multiple threads dot py, to run it. I can see that the operating system assigned this process the ID number 11316, and when the process is initially started, it only has one main thread of execution. Then, after starting 12 CPU waster threads, the process ID is the same, but now the program has 13 total threads. When Python created each of those additional threads, it gave them a name of thread dash one, two, three, and so on. As well as a unique thread identifier number. Now, if I switch over to the task manager, I can see that the overall CPU usage has increased compared to before I ran the program. But, overall, it's pretty low, considering I have 13 threads running. And that's because Python's global interpreter lock is only allowing one of those threads to actually execute, at any given moment. So at most, this program can only utilize one CPU worth of resources. I'll switch over to the resource monitor and I can see that the work load is being distributed across those processors. It's not being limited to only running on a single one of the available processors. Now, looking over at the processes tab, I see a process called Python.exe, which has the same process ID number, that my program displayed earlier. And, in the thread columns, it has 13 threads. In the average CPU column, I see this processing is using 4.15 per cent of the CPU, which corresponds to utilizing one of the 24 logical processors in the computer. This program will continue running forever, so I'll manually terminate it by pressing control and the break key.

Multiple processes: Python demo
- To leverage multiple processors, and to achieve true parallel execution in python, rather than structuring our program to use multiple threads, we'll need to use multiple processes. Fortunately, Python's multiprocessing package makes that pretty straight forward, because it provides an API for spawning additional processes that looks very similar to the threading module. To demonstrate just how similar they are, I'll modify the code from the previous example, which created several threads running the CPU waster function to use several processes instead. First, I'll need to import the multi`processing module. And, since that's a long word, I'll rename it to mp for short. Next, on line 21, I'll replace the threading modules thread class with the multiprocessing packages process class, and finally, when using the multiprocessing package to spawn additional processes, it's important to enclose the main body of the program, in an if statement. To check if the special name variable is equal to main, and I'll explain why this is important later. That's all it takes to convert this program from using multiple threads to multiple processes. I've pulled up a command prompt, alongside the task manager so I can view the CPU usage as that program runs. I'll run it by typing in python multiple processes.py and I can see that the CPU usage quickly rises to around 50% because now those 12 additional processes I created are all executing in parallel. The output from the program, which was printed by the main process, shows that it started with a process ID of 10312 and one thread. Then, after I started 12 CPU Wasters, that same process still only has one thread because the CPU Wasters are now running as separate processes. If I bring up the resource monitor, sort by process name, and then scroll down, I can see there are 13 processes named python.exe. The one on the bottom has the same process ID as the main program, and I can see that it's not using any CPU resources because after kicking off the 12 CPU Wasters processes and printing it's message, the main program is basically done. The other 12 python processes are all executing the CPU Waster function in parallel, so each is utilizing about 4% of the CPU resources. I'll switch back to the combined CPU usage overview on the task manager and then I'll end this program by pressing control break. As those processes are terminated, the CPU usage quickly falls back down to near zero. Now, let's switch back to code and talk about why the main program needed to be put within the if statement on line 12. Let me just add an extra space here. When this script creates and starts a new process on line 22, it tells it to execute the CPU Waster function. However, that new python process doesn't jump straight into the CPU Waster function, because it doesn't know what that is. It needs to run through the entire script first to find the CPU waster function and become aware of any other dependencies. To show that happening, I'll add a print statement outside of the CPU Waster function, but before the if statement to check the name. I'll have my program say hi my name is, and then print out the special name variable. So back in the console, I'll press the up key to bring up the command to run this program again, I see a hello message at the top of the output. That's from my initial main process as it works its way through the script. Then down at the bottom, I have 12 other hello messages from mp main. Python gives those processes that I spawn a different name than the main module so that I can tell them apart. Looking back at the code, if I did not include the if statement on line 14, then all of those processes I spawned would execute the section of code within it, including line 23 which spawns more processes. A process that uncontrollably tries to spawn more and more processes is a problem and python will actually throw an error if you do not include the if name as main line, which prevents that from happening.

Execution scheduling
- Threads don't just execute whenever they want to. A computer might have hundreds of processes with thousands of threads that all want their turn to run on just a handful of processors. So how do they decide who goes first? - That's the operating system's job. The OS includes a scheduler that controls when different threads and processes get their turn to execute on the CPU. The scheduler makes it possible for multiple programs to run concurrently on a single processor. When a process is created and ready to run, it gets loaded into memory and placed in the ready queue. Think of these as cooks in the kitchen that are ready to work. The scheduler is like the head chef that tells the other cooks when they get to use the cutting board. It cycles through the ready processes so they get a chance to execute on the processor. If there are multiple processors, then the OS will schedule processes to run on each of them to make the most use of the additional resources. A process will run until it finishes, and then a scheduler will assign another process to execute on that processor. Or, a process might get blocked and have to wait for an I/O event, in which case it'll go into a separate I/O waiting queue so another process can run. Or, the scheduler might determine that a process has spent its fair share of time on the processor, and swap it out for another process from the ready queue. When that occurs, it's called a context switch. The operating system has to save the state or context of the process that was running so it can be resumed later, and it has to load the context of the new process that's about to run. If I'm a process that's executing on this processor to chop cucumbers, when a scheduler decides it's time for a context switch, I'll need to pause what I'm doing and store the state of that task. - And as the new process that just got scheduled, I'll load my state information and then begin executing. (chopping) Now, context switches are not instantaneous. It takes time to save and restore the registers and memory state, so the scheduler needs a strategy for how frequently it switches between processes. There's a wide variety of algorithms that different operating system schedulers implement. Some of these algorithms are preemptive, which means they may pause or preempt a running, low-priority task when a higher priority task enters the ready state. In non-preemptive algorithms, once a process enters the running state, it'll be allowed to run for its allotted time. Which algorithm a scheduler chooses to implement will depend on its goals. Some schedulers might try to maximize throughput, or the amount of work they complete in a given time, whereas others might aim to minimize latency, to improve the system's responsiveness. Different operating systems have different purposes, and a desktop OS like Windows will have a different set of goals and use a different type of scheduler than a real-time OS for embedded systems. Now, while it's important to understand the concept of scheduling, and that it's taking place, you usually don't need to worry about the nitty-gritty details of how the scheduler works because it's often handled under the hood by the operating system. In fact, you might not have any control over when the parts of your program actually execute. - And that's an important thing to keep in mind. Avoid running programs expecting that multiple threads or processes will execute in a certain order, or for an equal amount of time, because the operating system may choose to schedule them differently from run to run.

Thread lifecycle
- When a new process or program begins running, it will start with just one thread, which is called the main thread because it's the main one that runs when the program begins. That main thread can then start or spawn additional threads to help out, referred to as its child threads, which are part of the same process but execute independently to do other tasks. Those threads can spawn their own children if needed and as each of those threads finish executing they'll notify their parent and terminate with the main thread usually being the last to finish execution. Over the life cycle of a thread, from creation through execution and finally termination, threads will usually be in one of four states. If I'm the main thread in this kitchen and I spawn or create another thread to help me, that child thread will begin in the new state. - Hello! - This thread isn't actually running yet so it doesn't take any CPU resources. - I don't even know what I'm supposed to be doing. - Part of creating a new thread is assigning it a function, the code it's going to execute. Olivia, I need you to slice these sausages. We're making soup. - I can do that. I'm ready to start. - Right, you can start now. Some programming languages require you to explicitly start a thread after creating it. - [Olivia] Now that I've started, I'm in the runnable state, which means the operation system can schedule me to execute. Through contact switches, I'll get swapped out with other threads to run on one of the available processors. - Olivia is running independently now, so my thread is free to continue executing my own tasks when it's my turn to get scheduled on the processor. - Aw man, this sausage is frozen. I need to wait for it to thaw before I can continue. When a thread needs to wait for an event to occur, like an external input or a timer, it goes into a blocked state while it waits. The good thing is that while I'm blocked I'm not using any CPU resources. The operating system will return me to the runnable state when the sausage is thawed. - And that frees up the processor for other threads to use. Now my thread may eventually reach a point where I need to wait until one of my children threads has finished for me to continue on. Maybe I've finished preparing everything else. I've completed all of my tasks and I need Olivia to finish slicing the sausage. I can wait for her thread to complete its execution by calling the join method. When I call join, my thread will enter a blocked state waiting until Olivia's done. - Ah, the sausage is finally thawed. Now I'll go back to the runnable state and continue executing. (knife scraping) Now I've finished executing so I'll notify my parent thread that I'm done. Hey Baron, I'm done. And then I'll enter the final terminated state. (Olivia sighing) A thread enters the terminated state when it either completes its execution or it's abnormally aborted. - Since Olivia notified me that she's done, I'll return to the runnable state so I can continue preparing soup. Now, different programming languages may use different names for their states and have a few additional ones, but in general, new, runnable, blocked, and terminated are the four phases of the life cycle of a thread.

Thread lifecycle: Python demo
- To demonstrate the life cycle of a Python thread, from creation to termination, we've created this example program, which recreates the interactions between Olivia and me, where I spawned her as a second thread to help slice sausages to make soup. Now there are two ways to create a thread and specify its activity in Python. In the previous Python examples, we put the code for our thread to execute into a function. And then passed that function to the thread constructor method as a callable object using the target parameter. The other way to create a thread in Python is to define a custom subclass that inherits from the thread class and overrides its run method. That second approach is what we've done with the class named ChefOlivia on line seven. It inherits from threading.Thread and overrides two of its methods, init and run. These are the only two methods you should override from the thread class. Within the init method, we simply use the super function to execute the parent thread class' init method on line nine. Python will raise an error if you don't do that. The run method below it contains the code to execute when the thread is started. On line 12, ChefOlivia will print a message that she's started and is waiting for the sausage to thaw. Then she waits for three seconds and then finally prints a message that she's done cutting sausage. After printing her last message on line 14, the ChefOlivia thread will terminate because its run method is finished. Down below that, the program's main thread represents me, Barron, in the kitchen. It starts by printing a message on line 18 that Barron has started and requests Olivia's help and then creates a new ChefOlivia object. At this point the ChefOlivia has not been started, so Barron tells her to start by calling her start method on line 22. And then after that, Barron continues cooking soup, which is represented by sleeping for half a second on line 25. Since Barron only sleeps for half a second after staring Olivia's thread, but Olivia sleeps for three whole seconds, Barron will be done well before Olivia. But he needs to wait until she's finished to continue on. This is where the join method comes into play, causing Barron to wait until after Olivia has completed everything she needs to do before he can continue on. Barron calls the join method on line 28. Notice that we're calling the join method on the Olivia thread object from within the main Barron thread. That'll block Barron's execution at that point until Olivia terminates. After Olivia's thread terminates, the main Barron thread will be able to continue on to print its final message on line 30 that they're both done. We'll run that program in the console by typing Python thread_lifecycle.py and pressing enter. I get these series of messages as output. Messages that begin with the word Barron were printed by the main thread. And messages that begin with Olivia came from the second ChefOlivia thread. After Barron told Olivia to start, Olivia printed her first message and then began to sleep for three seconds. Barron continued independently cooking soup for half a second and then waited for Olivia to join. Olivia finally finished which allowed Barron to continue, print its last message and terminate the program. Moving back to code, we can actually watch the status of Olivia's thread change throughout this sequence, by adding print statements with the somewhat morbid sounding is alive method, which returns a Boolean indicating whether or not that thread is alive. I'll add that immediately after creating the ChefOlivia thread object. Print Olivia alive? And then olivia.is_alive. And then open and closed parentheses. Notice that I've included an extra space on the front to make it easier to read among the other output text. Now I'll copy and paste this line, again after I start the Olivia thread. Then after the main thread is done with its half second sleep, Olivia should still be sleeping, so I'll add a third print statement here. And finally after Olivia joins, I'll print whether or not she's alive one final time. Back in the console, I'll press up to run this program again. And I get those alive messages. I see that Olivia's thread is initially not alive after first being created. Then after the Barron thread calls her start method, she is alive. Even though Olivia's thread is sleeping, at the time of the third is alive message, Python considers her thread to still be alive. But after Olivia terminates and joins the main method, the last call to the is alive method returns false.

Daemon thread
- We often create threads to provide some sort of service or perform a periodic task in support of the main program. A common example of that is garbage collection. A garbage collector is a form of automatic memory management that runs in the background and attempts to reclaim garbage or memory that's no longer being used by the program. Many languages include garbage collection as a standard part of their runtime environment, but for this demonstration I'll spawn my own new thread to handle garbage collection. - Man, what a mess. - Olivia is a separate child thread that will execute independently of my main thread. So, I can continue doing what I'm doing here, getting my soup ingredients ready. - While I try to reclaim some memory, or counter space, by clearing up Barron's garbage. - This setup, with Olivia running as a separate thread to provide that garbage collection service, will work fine until I'm ready to finish executing. Bam! Now my soup's spiced and ready, my main thread is done executing, and I'm ready to exit the program. But I can't. - Because I'm still running. Since Barron spawned me as a normal child thread, he won't be able to exit until I have terminated. - And since Olivia's thread is designed to collect garbage in a continuous loop, she'll never exit. I'll be stuck here waiting forever and this process will never terminate. Threads that are performing background tasks, like garbage collection, can be detached from the main program by making them what's called a demon thread. A demon thread, which you may also hear pronounced as daemon, is a thread that will not prevent the program from exiting if it's still running. By default, new threads are usually spawned as non-demon or normal threads and you have to explicitly turn a thread into a demon or background thread. Olivia, I forgot to tell you this earlier but, you're a demon thread. - Oh man, you detached me. - When my main thread is finished executing and there aren't any non-demon threads left running, this process can terminate. (thudding) And Olivia's demon thread will terminate with it. - Since I was terminated abruptly with the process, I didn't have a chance to gracefully shut down and stop what I was doing. That's fine in the case of a garbage collection routine because all of the memory this process was using will get cleared as part of terminating it. But if I was doing some sort of IO operation, like writing to a file, then terminating in the middle of that operation could end up corrupting data. If you detach a thread to make it a background task, make sure it won't have any negative side effects if it prematurely exits.

Daemon thread: Python demo
- In this program, to demonstrate a daemon thread, I defined a function called kitchen cleaner on line seven, which represents a periodic background task like garbage collection. The kitchen cleaner uses an infinite while loop to continuously print a message that Olivia cleaned the kitchen once every second. Down in the program's main section, I create and start a new kitchen cleaner thread named Olivia on lines 13 and 14. Then the main thread prints a series of messages that Barron is cooking, which are split up by sleep statements and then finally a message that Barron is done on line 22. Switching over to the console, I'll type python daemon_thread.py. And press enter to run this program. And I see those messages from Barron and Olivia displayed. But after the main thread reaches the end, and prints its final Barron is done message, the program doesn't exit because the kitchen cleaner thread is still going strong. And it will continue to run forever. I'll stop it by pressing control+break. Back in the code, to prevent that from happening, I'll set Olivia to be a daemon thread before she gets started by typing olivia.daemon equals true. To set the thread object's daemon property to true. I'll save that change. And switch back to the console. And when I run that program again, now when the main thread is done executing, Olivia's thread is also terminated so the process can exit. A few things to note here. When a new thread is created, it'll inherit the daemon status from its parent. The main thread is normal non-daemon thread, so by default, any threads that it creates will be non-daemon threads. You must set the daemon property to configure a thread to be daemon or non-daemon before starting it. Otherwise Python will raise runtime error. Finally, daemon threads do not gracefully exit like normal threads. When all of the non-daemon threads in a program are done executing, any remaining daemon threads will be abandoned as Python exits.

Data race
- One of the main challenges of writing concurrent programs is identifying the possible dependencies between threads to make sure they don't interfere with each other and cause problems. Data races are a common problem that can occur when two or more threads are concurrently accessing the same location in memory and at least one of those threads is writing to that location to modify its value. Fortunately you can protect your program against data races by using synchronization techniques which we'll show you later but to eventually use those techniques you'll first need to know how to recognize the data race. Olivia and I are two concurrent threads working together to figure out what we need to buy from the grocery store. I'll take inventory of the pantry and when I see that we're running low on something, I'll add more of that item to our shared shopping list. - And while Ben does that, I'll look through my recipe book and I'll add ingredients to our shopping list for the meals I wanted us to cook this week. - Even though we're two separate threads doing different tasks, we run the risk of a data race because we're both accessing and modifying the same shared resource, our shopping list. Now to the pantry! - Oh, this garlic mash potato recipe looks delicious I'll need five potatoes for it. I see that our shopping list already has three potatoes on it. Three plus five is eight so I'll erase three and write down eight. As you can see even a simple operation like adding two numbers is actually a multiple step process. First I had to read the existing value of that item from the shopping list then I modified the value by adding what I needed to it and finally I wrote the result back to the shopping list. - It looks like we're running low on garlic in the pantry. I think we should restock it with two more cloves. I see that there's currently one clove of garlic on the list one plus two is, err ... err - My garlic mash potato recipe calls for five cloves of garlic. I see there's currently one clove of garlic on the list. One plus five is six so I'll update the list to have six cloves on it. - Three! One plus two is three. We need three cloves of garlic.

Data race: Python demo
- To demonstrate a data race and code, we've created this simple Python program that uses two threads to concurrently increment a shared variable. The variable on line six is a counter for the amount of garlic we should buy, and gets initialized to zero. Below that, the shopper function defined on line eight represents a shopper adding garlic to the shopping list. It uses the for loop on line 10 to increment that global count variable 10 times. Down in the program's main section, starting on line 14, I create two shopper threads called barron and olivia, start them both and then use the join method to wait until they're both done. Finally, I print out the value of the garlic count variable on line 20, indicating how much garlic we should buy. Switching over to the console I'll run this program with the command python data_race.py. And the output tells me we should buy 20 garlic. That makes sense because I have two threads that are each adding 10 garlic to the counter. And if I press the up arrow to recall that command and run this program several more times, there's a reasonable chance that I'll continue to get that correct answer of 20. It's not guaranteed, but with each shopper only incrementing the count variable 10 times, there are not a whole lot of opportunities for the data race to cause a problem. So to help make trouble, I'll switch back to the code and I'm going to modify the for loop so that each of the shopper increments that count variable 10 million times. Now the expected output should be 20 million garlic. But when I save that change and run the program again I get a value that's way lower than that, only around 11 million. And if I run the program one more time, I get a different, incorrect value. Even though the simple garlic count plus equal one operation is only a single line of code in the background, the computer is actually performing a three step read, modify, write process. My two concurrent threads end up stepping on each other's toes, and unintentionally overriding each other's changes, which produces an incorrect result. When you realize that one of your programs has a database, grab a cup of coffee because they can take a while to hunt down and fix. There are tools that exists to help detect data races, but they are specific to different languages and environments and beyond the scope of this course. In my opinion, the best defense against data races is a good offense, preventing them from occurring in the first place. Since a data race only occurs when at least one of the concurrent threads is modifying the value of a memory location, pay close attention to anywhere you use an assignment operation, or an operator like the plus equal incrementor that changes a variable's value. If there's a potential for two or more threads to access that variable and make changes to it, then you'll almost certainly need to use some sort of mechanism to protect it.

Mutual exclusion
- Anytime multiple threads are concurrently reading and writing a shared resource, it creates the potential for incorrect behavior, like a data erase, but we can defend against that by identifying and protecting critical sections of code. A critical section or critical region is part of a program that accesses a shared resource, such as a data structured memory or an external device, and it may not operate correctly if multiple threads concurrently access it. The critical section needs to be protected so that it only allows one thread or process to execute in it at a time. - Barren and I experienced a data erase as we added garlic to our shared shopping list, because incrementing a value is actually a three step process. Read the current value, modify it, and then write back the result. Those three steps are a critical section and they need to execute as an uninterrupted action, so we don't accidentally override each other. - I have an idea, give me your pencil. - Hey, I was using that. - Now there's only one pencil for us to share, and the rule is that only the person holding the pencil can access the shopping list, either to read or write it. That way one of us won't accidentally read a wrong value, because the other one is only halfway done updating it. In this arrangement, the pencil is serving as a mechanism called a mutex, short for mutual exclusion, which you'll also hear referred to as a lock. Only one thread or process can have possession of the lock at a time so it can be used to prevent multiple threads from simultaneously accessing a shared resource, forcing them to take turns. If either of us wants to access the shopping list, we first need to pick up the pencil to acquire the lock on it. We do whatever we need to with the shared notepad, and then when we're done, release the lock by putting down the pencil. The operation to acquire the lock is an atomic operation, which means it's always executed as a single, indivisible action. To the rest of the system, an atomic operation appears to happen instantaneously, even if under the hood it really takes multiple steps. The key here is that the atomic operation is un-interruptible. - If I grab the pencil-- - Acquiring the mutex is an atomic action that no other thread can interfere with halfway through. Either you have the mutex, or you don't. Now that you do have a lock on our pencil, you can safely execute in the critical section. - I see we already have ten carrots on the list, I'll add five more to that. - Oh, and we're going to need some more onions too. - Well, I currently possess the mutex, so you will have to wait until I'm done. Threads that try to acquire a lock that's currently possessed by another thread, can pause and wait 'til it's available. There, I'm done with the notepad for now, so I'll release the lock. - And I'll acquire the lock, so I can add onions to the list. - Don't forget to release the mutex when you're done. - Okay. - Since threads can get blocked and stuck waiting for a thread in the critical section to finish executing, it's important to keep the section of code protected with the mutex as short as possible. If I take the pencil, execute a critical section by adding more lettuce, and then hold onto the pencil while I contemplate what else to buy... - I'm stuck waiting for Olivia to return the pencil so I can use it, and getting kind of annoyed. - Only thinking about what I want to buy doesn't actually require the shared notepad. So the operation doesn't require mutual exclusion. I should of returned the pencil as soon as I was done updating the list, that way Baron could use it to execute the critical section while I'm busy doing other things.

Mutual exclusion: Python demo
- To demonstrate how to manually enforce mutual exclusion with locks in Python, we'll modify the example program from earlier with two shoppers that have a data race as they can currently increment the amount of garlic to buy. Locks are included as part of Python's threading module which I've already imported on line four. So, I'll create a new lock object using the constructor method from that module. Threading dot lock. And I've assigned it the unimaginative name pencil, because Olivia and I used a pencil in our example to serve as a lock. Now, to keep the two shopper threads from modifying the garlic count variable at the same time, I'll call the pencil's acquire method before entering the for loop on line eleven and then I'll call its release method immediately after the for loop has finished. Pencil dot release. I'll save those changes, switch over to the console and run the program. The first shopper thread to begin executing will take the pencil, increment the garlic count 10 million times and then unlock the pencil so the other thread can do the same. That prevents the data race and I get the expected output of 20 million. Now, that solution worked fine here because incrementing the garlic count is a fairly quick operation and that's all the shopper threads needed to do, 10 million times each. But what if there was a longer I/O operation involved? Perhaps each of these shoppers needed some time to stop and think and ponder the meaning of life every time they add an item to the list. Back in the code, I'll simulate that by adding a print statement before I increment the garlic count variable that says the thread is thinking. I'll say, print threading dot current underscore thread dot get name and that'll give me the name of the current thread and then, is thinking. After that, I'll add a sleep statement to make the thread sleep for half a second. And that also requires me to import the time module at the top of my program. Finally, since I put that sleep method in there, I'll also reduce the number of loop iterations on line 13 from 10 million down to just five, so this program doesn't take forever to run. I'll save that change, go back to the console and run the program again. Now I see a new thinking message appear every half a second. Five from one thread and then five from the other. So this program takes about five seconds to finish executing. Since each thread does its thinking while holding onto the pencil, the other thread is stuck waiting outside of the critical section during that time. The critical section here is way bigger than it needs to be. I only really need to protect line 16 which increments the garlic count. Sleeping and thinking doesn't affect the shared data, so the shopper does not need to be holding the pencil while they do that. So to improve this program, I'll move the pencil's acquire and release statements so that the shopper locks it immediately before incrementing the garlic count by copying, or cutting and pasting that and then they release it immediately after incrementing the garlic count. I'll save and run that change. Now the program will run twice as fast because the threads are not holding onto the pencil while they're busy thinking. I see the thinking messages from both threads appear in pairs and it only takes about two and a half seconds to execute. We've minimized the critical section to only protect the part of this program that truly requires mutual exclusion.

Reentrant lock
- Olivia and I have been using this pencil as a mutex. Only one person at a time can own, or have a lock on it, and only that person can access our shared resource, this notepad. - If I attempt to lock the mutex while another thread has it, my thread will be blocked, and I need to wait until he unlocks it so it becomes available. - And if I attempt to lock the mutex, it doesn't appear to be available, so my thread will just have to wait, too. (drumming) - It's behind your ear. You already locked it. - Oh, well, my thread can't unlock the mutex while I'm blocked waiting on it, and I'll be waiting on the mutex forever because I'll never be able to unlock it. I'm stuck and so are you. If a thread tries to lock a mutex that it's already locked, it'll enter into a waiting list for that mutex, which results in something called a deadlock, because no other thread can unlock that mutex. - There may be times when a program needs to lock a mutex multiple times before unlocking it. In that case, you should use a reentrant mutex to prevent this type of problem. A reentrant mutex is a particular type of mutex that can be locked multiple times by the same process or thread. Internally, the reentrant mutex keeps track of how many times it's been locked by the owning thread, and it has to be unlocked an equal number of times before another thread can lock it. If this pencil is a reentrant mutex, when I pick it up, I lock it. - Now Olivia's thread has a hold on the mutex so she's the only one that can lock or unlock it. - Since the pencil is reentrant, I can lock it again. Now, the pencil has been locked twice by me, which means I'll have to unlock it twice to fully release my hold on it. If your program needs to lock a mutex multiple times, using a reentrant mutex may seem like an easy way to avoid a deadlock, but if you don't unlock the reentrant mutex the same number of times, you can still end up stuck. - I'm waiting. Thanks. Many programmers like using reentrant locks because it can make things easier. You don't need to worry as much about what's already been locked, and they make it easier to retrofit locks into existing code. As an example, say I have a function to increment a shared counter, and it uses a mutex to protect that operation. If later I create another function that uses the same mutex to protect some other section of code, and that section of code uses the increment counter function, since those functions are nested, when I execute my function, it'll end up locking the mutex twice before unlocking it. If I was using a regular non-reentrant lock, that would produce a deadlock, but with a reentrant mutex this works just fine. Now, like many things in the world of programmers, there are some very strong opinions about whether reentrant locks are good or evil. Some opponents of using reentrant locks will argue that the example I just showed you should be refactored to avoid having nested locks by using a third function that increments the counter and only gets called from within a protected section. I'm not going to advocate either way on this debate. There are pros and cons to both sides. - One use case where reentrant locks are really needed is when writing a recursive function. That is a function that calls itself. If the function makes a recursive call to itself from within a locked section, it will lock the mutex multiple times as it repeats itself, and then unlock the mutex an equal number of times as it returns and unwinds. Since a reentrant mutex can be used recursively like this, you'll often hear it referred to as a recursive mutex or a recursive lock. Different languages use different terms, but these all basically mean the same thing.

Rlock: Python demo
- To demonstrate using a reentrant lock in Python, we've modified the previous example we used to demonstrate a data race and mutual exclusion with two shoppers that are concurrently incrementing the amount of items to buy. In this version, we're counting the amount of garlic and potatoes to buy with the variables that are initialized on lines six and seven. There are two helper functions on lines 10 and 16 called add_garlic and add_potato, which increment the corresponding garlic_count or potato_count variables. And each of those functions acquire and release the same lock object, named pencil, to enforce mutual exclusion around those operations and prevent a data race. The shopper function simply uses a for loop on line 23 to execute those add_garlic and add_potato functions 10,000 times each. Down in the program's main section, we create and start two shopper threads, and then after they finish running, I print out the amount of garlic and potatoes to buy. I'll switch over to the console and run this program by typing python reentrance_lock.py. When it finishes, it indicates that we need 20,000 garlic and 20,000 potatoes, so, quite a feast. Now, let's say every time we add a potato, we also want to add an additional garlic, because potatoes and garlic go really well together. To do that, we might add a call to the add_garlic function inside of the add_potato function, immediately after incrementing the potato_count. By inserting the add_garlic function inside of the add_potato function, we've effectively nested two calls to the pencil's acquire method. When the add_potato function gets called, it will acquire the pencil, then, when the add_potato function calls add_garlic, the add_garlic function will acquire the pencil again. So, the pencil ends up being locked twice in a row by those nested functions, before then being released twice as the functions complete. When I run the program with those nested calls, it gets stuck as soon as the pencil is acquired twice in a row. It doesn't have any output, so, I can't really tell looking at it, but if I wait for this program to finish, I'll be waiting here forever. So, I'll press Control and the Break key to manually end it. Fortunately, the fix for this only requires adding one letter. On line eight, I'll change the lock constructor method to Rlock, which is Python's implementation of a reentrant lock that can be acquired multiple time before being released. I'll save that change. And now when I run the program, I get the output that we should buy 40,000 garlic and 20,000 potatoes. That makes sense because I increment the garlic count in both the add_garlic function and the add_potato functions. One interesting difference between the regular lock and Rlock in Python is that the regular lock can be released by different threads than the one that acquired it, but the reentrant lock must be released by the same thread that acquired it. And of course, it must be released by that thread as many times as it was acquired before it will be available for another thread to take. This is just the difference between how lock and Rlock are implemented in Python. Now, you might be tempted to devise a complex scheme using regular locks in which some threads acquire the lock and other threads release it. And if you find yourself thinking about that, stop. Trying to split lock operations across multiple threads is generally a bad idea, and a surefire way to create problems. If a thread tries to release a lock that has not already been acquired, Python will raise an error and can crash your program.

Try lock
- When multiple threads each have multiple tasks to perform, making those threads block and wait every time they attempt to acquire a lock that's already taken may not be necessary or efficient. Olivia and I are two threads doing several different tasks. My thread will be taking an inventory of the fridge to see what things we're running low on, and then add those to the shopping list on our shared notepad. I'll go back and forth between those two tasks. - And my thread is searching through the newspaper for grocery coupons and then adding those items to the shared shopping list. Ooh, there are some good deals this week. Now that I've found some items that I want, I'll take the pencil, which is our mutex, to lock access to the shared notepad so I can add them. - I saw we're low on milk, so now I'll go to acquire the pencil, and I see Olivia has it. If I attempt to lock a mutex in a regular blocking fashion, my thread would enter a waiting state at this point, doing nothing until Olivia unlocks it. If I don't have anything else to do, so I can't continue with other things until after I've accessed the shared notepad, that's okay, it's what has to happen. But in this scenario, I do have other useful things to do that don't require the notepad. I can keep searching the fridge for other things we need. So rather than using the standard locking method to acquire the mutex, I'll use what's called Try Lock, or Try Enter, which is a non-blocking version of the lock or acquire method. It returns immediately and one of two things will happen. If the mutex you're trying to lock is available, it will get locked and the method will return true. Otherwise, if the mutex is already possessed by another thread, the Try Lock method will immediately return false. That return value of true or false lets the thread know whether or not it was successful in acquiring the lock. So if I try to lock the pencil that Olivia currently has, I know immediately that my attempt has failed, so I can go back to searching the fridge. - There, I'm done writing for now. So I'll unlock the pencil and go back to searching for coupons. Since Barron wasn't blocked waiting for this mutex, it's just sitting unlocked, available for anyone to take. Now, Barron likes to explain Try Lock with pencils and notepads. I think of it more like being at a house party with a bunch of your friends, your fellow threads. There's one restroom at the house that everyone at the party will need to use at some point. But only one person can use it at a time. When you try to use it and try opening the door, you realize it's locked because someone's already inside. You could stand there and wait until they come out, or you could go back to the party, keeping having fun, and try again later.

Non-blocking acquire: Python demo
- In Python, the common try mock operation can be implemented by configuring the acquire method to be non-blocking. To demonstrate that in use, we're creating this example which simulates two shoppers searching for items they need, and then adding them to a shared notepad. The global variable on line seven represents the number of items on their notepad. Within the shopper function, each thread has a local variable on line 13 for the number of items to add to the notepad. How many items they've found in a coupon book, or perhaps missing from the fridge. The while loop on line 14 will keep the shoppers searching for items and adding them to the notepad until there are at least 20 items. If the shopper has items to add to the notepad, they'll execute the if clause on line 15. In which they acquire the pencil, add all of their items to the list, and print a message with how many items they added. That then resets their items to add count back to zero. The thread sleeps for 300 milliseconds to simulate time spent writing. And then finally unlocks the pencil on line 21. If the shopper did not have anything to add, then they would have executed the else clause on line 22, where they spend 100 milliseconds to search for and find an item they need, which then increments their own items to add counter. Down in the main section, I create two shoppers named Barron and Olivia. Then record the time from when I start their threads until they both finish to see how long it takes for them to find at least 20 items. Now, I'll run the program in the console by typing python non-blocking underscore acquire. And I see that it took them a little over six and a half seconds to find all those items. Notice that the two threads are never adding more than one item at a time. Since the default acquire method blocks execution, these two threads end up taking turns. Swapping places between the top and bottom sections of the if else statement. Now, within the acquire method on line 16, I'll configure the optional blocking parameter to be false. That will cause the acquire method to always immediately return a Boolean value to indicate whether or not the lock was successfully acquired. So, I'm going to move and incorporate that call into the logic of the if statement on line 15 by cutting it, adding an and statement, and then pasting that acquire spot here, and delete the line below it. Now, the order of these statements on line 15 on each side of the and operator matters. Because Python evaluates the statement from left to right. It will first check to see if there are items to add. And only if the left side of the and is true, will it evaluate the right side, and execute the non-blocking acquire method. If the lock is available, then calling acquire will lock it, and return true. And the program will execute the code between line 16 to 20 to add items to the shared notepad. If the lock was not available, then the non-blocking acquire method will immediately return false, and that thread will execute the else clause from lines 22 to 24 to look for other things to buy. When I run the program now, it executes much faster. Finishing in a little over two seconds, which is less than half the amount of time as before. Notice that now, when one of the threads gets its turn in the critical section, it's often adding more than one item to the notepad. With the non-blocking acquire method in place, as one thread takes its time writing to the notepad, the other thread is able to jump past that section of code to search for other things to buy multiple times. It's been freed up to accomplish other things.

Read-write lock
- We use a lock or mutex to protect a critical section of code to defend against data erases, which can occur when multiple threads are concurrently accessing the same location in memory and at least one of those threads is writing to that location. That second part is key, because if we have a bunch of threads and none of them are writing, they all just want to read from the same location, that's fine. It's okay to let multiple threads read the same shared value as long as no one else can change it. They'll all safely see the same thing. Danger only exists when you add a thread that's writing to the mix. When we use a basic lock or mutex to protect the shared resource, we limit access so that only one of the threads can use it at a time, regardless of whether that thread is reading or writing or both. - That works, but it's not necessarily the most efficient way to do things, especially when there are lots of threads that only need to read. This is where reader-writer locks can be useful. A reader-writer lock, or shared mutex, can be locked in one of two ways. It can be locked in a shared read mode that allows multiple threads that only need to read simultaneously to lock it. Or it can be locked in an exclusive write mode that limits access to only one thread at a time, allowing that thread to safely write to the shared resource. A read-write lock is useful for protecting a shared resource like our calendar, because Barron and I frequently need to read the calendar throughout the day. - I can never remember what day it is. - But we rarely need to modify it. - Only once a day. - This marker represents our shared mutex. When my thread wants to read the calendar, I'll lock the mutex in the read only mode by placing my finger on it. - I also want to read the calendar. I can also place my finger on the marker. Now we both have a shared lock on it, so we can both concurrently read it. - When I'm done checking the date, I'll release my lock on the mutex. Now I think it's time to increment the calendar's date. In other words, I want to modify it. And to do that, I'll need to lock the shared mutex in the exclusive write mode by picking it up. - But I'm still holding on to the lock to read. - A thread trying to acquire the lock in write mode can't do so as long as it's still being held by any other threads in the read mode. So I'll have to wait. - Done. - Now the shared mutex is completely free, so I'll pick it up to place a write lock on it and update the calendar. - Ah, I already forgot what day it is. And I can't read the calendar now, because Olivia has an exclusive hold on the lock to write. Since only one thread can have the write lock at a time, all other threads wanting to read or write will have to wait until the lock becomes available again. - Now recognizing when to use a read-write lock is just as important as knowing how to use it. In certain scenarios, read-write locks can improve a program's performance versus using a standard mutex. But they are more complicated to implement, and they typically use more resources under the hood to keep track of the number of readers. - And there can be language-dependent differences in how they're implemented that affect performance. Do they give preference to readers or writers that are trying to acquire the lock? Deciding which type of mutex to use is a complicated decision, but as a general rule of thumb, it makes sense to use a shared reader-writer lock when you have a lot more threads that will be reading from the shared data than the number of threads that will be writing to it, such as certain types of database applications. If the majority of your threads are writing, then there's not much, if any, advantage to using a read-write lock.

Read-write lock: Python demo
- Reader-writer locks are a common feature in many programming languages that support concurrency, however they're not included by default in Python. When I need something that isn't standard in Python, my first stop is always the Python package index at pypi.org and sure enough, someone has already implemented a reader-writer lock package for python. I'll install it for this demonstration using the Python package manager with the console command pip install reader writer lock. And it's that easy. In this example program, we create 10 threads to concurrently read what day it is from a shared calendar while two other threads update it. The calendar here is just a list of strings to represent the seven days of the week on line six, as well as an integer to indicate which day today is on line seven. I've created a single regular lock object named marker on line eight which all of the threads will use to enforce mutual exclusion when interacting with the today variable. There are two functions on lines 10 and 18 for the threads that will be reading the calendar value and those that will be writing to update it. They both use a while loop to continuously run until they see that it's the end of the week. In the calendar reader's while loop, the thread acquires the marker on line 14, prints out a message about what day it sees it is and then releases the marker. In the calendar writer's while loop, the thread acquires the marker on line 22, changes the calendar to the next day, prints a message about doing so and then releases the marker lock. Finally, down in the program's main section, I use a pair of for loops to create and start ten reader threads and two writer threads. So there are five times as many readers as there are writers. Notice that I'm using the args keyword in the thread constructor on lines 30 and 33 to pass in the loop iteration counter, i, as an argument to the calendar reader and calendar writer functions. I use that to give each of the calendar readers and writers a unique ID number. The args keyword takes the tuple of arguments to use when invoking the target function. I'll run this program in the console by typing Python, read write underscore lock dot pi and the threads get schedules to execute and each thread usually ends up getting to run through the while loop several times before another thread gets its turn. If I scroll up through the output, I see a lot of messages from different readers looking at what day it is and occasionally the writer threads get their chance to get scheduled. Since I'm using a single lock to protect both critical sections, only one thread can ever be reading or writing at a time. Now, to upgrade this program from using a basic lock to a reader-writer lock, I'll need to import the reader-writer lock package I installed earlier with the command from readerwriterlock import rwlock. Next, I'll change the lock constructor on line nine to use the RW lock package instead of the threading module and I'll create and RWLockFair object instead of a regular lock. Fair means this version of the RW lock will give fair and equal priority to readers and writers. There are also other implementations of the RW lock that can give priority to either the readers or to writers. The RW lock object has two methods. Gen r lock generates one lock for reading which can be held by multiple threads at once and gen w lock which generates another lock object for writers which can only be held by a single thread at once. Back in the code, I'll create and object called read marker using the marker's gen r lock method and down in the calendar writer function I'll create a write marker using the marker's gen w lock method. Now I just need to update the acquire and release statements in the calendar reader and writer to use their corresponding read marker and write marker. So I'll put read markers in the calendar reader and use the write marker in the calendar writer. Finally I'll add one last feature to the calendar reader's print statement to display the number of threads that are currently holding the read marker. I'll say read count and the reader-writer lock doesn't make that information easily accessible but I can pull the amount of threads that are currently holding the counter by going to read underscore marker dot c underscore rw underscore lock dot v underscore read underscore count. Bit of a long statement there. I'll save that change, switch back over to the console and run that program again. Now the output stream I get is much shorter that the first time I ran it. I can see that several of the readers are able to check the date and that multiple readers are able to be in the critical section at the same time. Up to 10 readers at several points. Finally, the writer got its chance to get in there and update the calendar. The readers got scheduled again to view that and after both writers made their updates, the program was finally able to exit.

Deadlock
- Olivia and I decided to take a snack break to demonstrate some of the problems that can occur when using locks. A classic example that's used to illustrate synchronization issues when multiple threads are competing for multiple locks is the Dining Philosopher's problem. In this scenario Olivia and I are two philosophers or threads doing what philosophers do best; Thinking and eating. We both need to access a shared resource this plate of sushi and each time one of us takes a piece of sushi we're modifying it's value the number of pieces that are left. The act of taking sushi from the plate is a critical section so to protect it we've devised a mutual exclusion process using these two chopsticks as mutexes. When I want to take a bite of sushi I'll first pick up the chopstick closest to me to acquire a lock on it then I pick up the farther chopstick. Now I have possession of both locks I'm in the critical section so I'll take a piece of sushi and then put down the far chopstick to release my lock on it and then the close chopstick. And finally, since I'm a philosopher, I'll go back to philosophizing. - Ah! Eureka! That was an interesting thought. Well, I'm feeling hungry now so I'll acquire the chopstick closest to me and then the one further away. I'll take a piece of sushi and then release the far chopstick and then the one closer to me. As dining philosophers we'll both continue to alternate between eating and thinking but since we're operating as concurrent threads neither one of us knows when the other one wants to eat or think And that can lead to problems. If I get hungry again and pick up the chopstick closest to me. - And I also get hungry and pick up my close chopstick. I see we've come to an impasse. We've both acquired one of the two locks that we need so we're both stuck waiting on the other thread to release the other lock to make progress. This is one example of a situation called deadlock. Each member of a group is waiting for some other member to take action and as a result neither member is able to make progress. Avoiding deadlock is a common challenge in concurrent programs that use mutual exclusion mechanisms to protect critical sections of code. We want our program to be free from deadlock to guarantee liveness which is a set of properties that require concurrent programs to make progress. Some processes or threads may have to take turns in a critical section but a well written program with liveness guarantees that all processes will eventually make progress. But, we're clearly not making any progress here and I'm getting hangry. - Eureka! I have another idea. - Oh. Do tell, do tell. - Well, our deadlock occurred because we both reached for the chopstick closest to us first. I grabbed this chopstick first and you grabbed this chopstick first. That set us up for a deadlock. But, if we prioritize these locks so that we both try to acquire this chopstick first then we won't have this problem because we'll both be competing for the same first lock. - Well, let's give that a try. Ah! Brilliant. If you acquire the first chopstick and then I try to acquire it I'll just be stuck here waiting until you finish taking your sushi and release both locks. Then, I can grab that chopstick and this one and take my turn in the critical section. - In this example, we had two separate locks protecting a single shared resource. That may not be the most realistic scenario but it demonstrates the point. Now, imagine something like a banking application with a set of bank accounts where each one has it's own mutex. To ensure that only one thread will be withdrawing from or depositing funds to that account at a time. To transfer funds between two accounts a thread needs to acquire the locks for both the sender and the receiver since it would be modifying the value of both accounts. If there are multiple threads concurrently making transfers between the accounts then there's a real chance that the could end up competing for the same locks and run into this sort of deadlock scenario.

Deadlock: Python demo
- To demonstrate a deadlock with a dining philosophers problem and how to resolve it will expand our scenario from having just two philosophers, Olivia and me, to having three philosophers. Olivia, me and our buddy Steve, each competing for two of the three chopsticks placed around the table, labeled here as A, B and C. In our example Python program, we instantiate those three lock objects on lines six through eight and name them chopstick A, B, and C. We also create a variable named sushi count to represent the amount of sushi left between the philosophers. Below that, the philosopher function on line 11 is used to represent our philosopher's who think and eat sushi. The function has three input parameters. The philosopher's name and two locks, named first chopstick and second chopstick to indicate the order in which the philosopher will acquire them. The while loop on line 13 will make the philosopher continue taking sushi as long as there's some left on the plate. Within the loop, the philosopher will pick up and acquire a lock on their first chopstick followed by their second chopstick. Then if there's still sushi left on the plate, they'll take a piece by decrementing the sushi count variable on line 18 and print a message showing how much is left. Finally, the philosopher releases both chopsticks to put them down for someone else to take. Down in the programs main section, we create and start three philosopher threads named Baron, Olivia and Steve. Each of those philosophers is past two of the three chopstick locks instantiated at the top of the program. Notice that each philosopher has a different first and second chopstick. Baron will acquire A first and then B. Olivia acquires chopstick B then C. And Steve acquires C first followed by chopstick A. Now when I run this program by typing python deadlock dot py, it seems to run fine. I get a message that Baron took a piece of sushi five times and then that seems pretty normal. Due to scheduling, only one philosopher is getting to eat here but all of the sushi is eventually eaten and the program finishes, which is valid behavior. And this highlights the tricky nature of deadlocks and why they're hard to detect and debug. Just like a race condition, you might get lucky and never experience a problem with your program. Even if the potential for a deadlock exists. To give this program more opportunities to deadlock I'm going to increase the amount of sushi from five to 500, we're three really hungry philosophers. Now if I run the program again, it locks up with 445 pieces of sushi remaining. Our philosophers are in a deadlock. If I press Control, Shift Escape to open the Windows task manager, and go to the performance tab, I see that the CPU is not overly busy, it's only at about one percent. Since the threads are stuck waiting on each other, the deadlock program doesn't use up CPU cycles. Now my program will be stuck in this state forever so I'll need to manually terminate it by pressing Control Break. Running this program again, will result in a deadlock after a different amount of sushi. The amount of progress it makes before the deadlock will vary depending on how the threads get scheduled to execute. If we're really lucky, it is possible that the program could make it to the end of the 500 piece sushi plate. But luck is not a good strategy for programming so let's implement Olivia's solution of prioritizing the locks. We'll say that chopstick A has the highest priority, B is second and C is third. And each philosopher should always acquire their highest priority chopstick first. I can see on line 25 that Baron acquires A before B. Olivia acquires B before C but Steve is causing the problem here because he acquires chopstick C before A. So I'll swap the order of those. Acquire A first and then C. I'll save that change. Now when I run the program after making that change, it runs to the end. and we have three very well fed philosophers. Now we designed this example to be as simple as possible by only including a single shared resource, the sushi plate. In practice, this program only really needed one lock to protect it. We intentionally created the potential for deadlock because I nested two locks inside of each other to demonstrate the concept. As your program grows in complexity to include more critical sections, locks and parallel threads with intertwined dependencies it becomes increasingly more difficult to identify and prevent potential deadlocks. The simplest technique to prevent deadlocks is the one we used in this video. Which is to ensure that locks are always taken in the same order by every thread. However, lock ordering may not always be feasible. For example, a thread may not know all of the locks it will need to acquire ahead of taking any of them. Another technique for preventing deadlocks is to put a timeout on lock attempts. If a thread is not able to successfully acquire all of the locks it needs within a certain amount of time, it will back up, free all of the locks that it did take and then wait for a random amount of time before trying again to give other threads a chance to take the locks they need.

Abandoned lock
- Now that we've figured out how to avoid a deadlock between our two philosophers using chopsticks we can return to our routine of eating and philosophizing. I'm ready for another piece of sushi so I'll pick up the first chopstick, then the second one. And I think I left the refrigerator open. - Well that was rude of him. We've entered another form of a deadlock through thread death. If one thread or process acquires a lock and then terminates because of some unexpected reason it may not automatically release the lock before it disappears. That leaves others tasks like me stuck waiting for a lock that will never be released and getting hungry. - Sorry about that. Let's look at some code.

Abandoned lock: Python demo
- To demonstrate what happens if a lock gets abandoned in Python, we'll be modifying the previous dining philosophers example that we used to demonstrate a dead lock with three philosophers using chopsticks to take sushi from a shared plate. We've already fixed the dead lock in this version of the code, so if I run this program, the philosophers successfully take turns eating sushi until all 500 pieces are gone. The critical section for this program exists between the acquire methods on lines 14 and 15 and the release methods on lines 21 and 22. Now, if one of the philosopher's threads acquires the locks, and then something goes wrong in that critical section to cause an unexpected error, that could kill its thread before it gets a chance to release the lock. To simulate that happening, I'll add another if statement to check if there are exactly 10 pieces of sushi left, and if so, I'll do my favorite technique for intentionally crashing a program, and that's to divide by zero. Now, you should never divide by zero, but I'm doing it here to trigger an exception that will cause Python to crash the currently executing thread. I'll save that change. When I run this program it gets all the way down to 10 pieces remaining. And then the thread that happens to be executing at that time, in this case that's the Barron thread, it hits that divide by zero and crashes. The other threads are stuck waiting on the locks that will never get released by Barron, so the program is stuck here forever. This scenario is not the same as the dead lock we looked at in a previous video, because the threads here are not waiting on each other to release a lock, but it's a related scenario and the impact is the same, the program isn't making any progress. So, to prevent this type of situation from occurring, we should put the critical section within a try block. If we have any exception handling code, we can optionally include an except clause after the try block to catch and deal with that error, but what we really care about here is making sure that the locks always get released before the current thread gets terminated if it crashes. And to do that, I'll add a finally block after that try statement, and put the calls to unlock the chopsticks in it. I'll save that change. And now when I run that program, an exception still occurs when one of the threads takes that tenth piece of sushi, in this case that was Olivia, but thanks to the finally clause, that thread is able to release the lock before it terminates. I can see that after the Olivia thread took the tenth piece of sushi and crashed, the Barron thread took over to finish eating the remaining sushi. It's good practice to always make sure locks will be released, if something goes wrong and unexpectedly crashes a thread. And Python makes that especially easy, because lock objects support working with context managers. Using the with statement on a lock, as shown on the right, is equivalent to using the try and finally blocks shown on the left. Using a context manager is the more pythonic way to program, so let's modify this example to use the with statement instead. I'll delete the entire finally block, and I can also delete the try statement, I won't need that now. I'll replace the acquire method on line 14 to say with first_chopstick:, then nested within that goes another call with second_chopstick:. And I'll move this whole block to within there. Now I'm using context managers, so if I save and run this program, it runs just the same as before, but now the code is more pythonically structured.

Starvation
- It would be nice if Olivia and I took turns acquiring and releasing the pair of chopsticks so we could take an equal amount of sushi from the shared plate, but that's not guaranteed to happen. The operating system decides when each of our threads gets scheduled to execute and depending on the timing of that, it can lead to problems. If Olivia puts down the chopsticks to release her lock on the critical section, but my thread doesn't get a chance to acquire them before she takes them again then I'll be stuck waiting again until she takes another piece. If that happens occasionally, it's probably not a big deal, but if it happens regularly. - Too slow. - Then my thread's going to starve. Starvation occurs when a thread is unable to gain access to a necessary resource and is therefore unable to make progress. If another greedy thread is frequently holding a lock on the shared resource, then the starved thread won't get a chance to execute. - In a simple scenario like ours, with two equal threads competing for execution time, starvation probably isn't a concern. Both of our threads should get plenty of chances to take sushi. However, if our two threads are given different priorities then that may not be the case. Barron knows I get grumpy when I don't eat. - And I certainly don't want that, so I'll give Olivia's thread a higher priority. - Thanks! - How different thread priorities get treated will depend on the operating system, but generally higher priority threads will be scheduled to execute more often and that can leave low priority threads, like me, feeling hungry. Another thing that can lead to starvation is having too many concurrent threads. - Oh, I forgot to mention that I invited some friends over. (scoffing) - Well with this many competing threads, I may never get any sushi.

Starvation: Python demo
- To demonstrate thread starvation in Python we'll modify the dining philosophers example by adding a local variable within the philosopher function to keep track of how many pieces of sushi each philosopher thread gets to eat. I'm going to name that sushi_eaten and I'll initialize it to zero. We'll increment the sushi eaten variable every time the philosopher takes a piece of sushi. sushi_eaten += 1, and at the end of the philosopher function after the while loop had finished we'll print out the number of pieces that it took. print(name), 'took', sushi_eaten, 'pieces' There are currently 5000 pieces of sushi up for grabs on line 9, so I'll switch over to a command prompt and run this program. (keys clacking) When it finishes, we see that each of the three philosophers got a different amount of sushi and it's not particularly fair. Olivia took way more sushi than Barron or Steve. She has over 3000 of the pieces. But it's not because she's greedy. In this case, it's because of the order in which the three philosophers are taking chopsticks. Barron and Steve are both competing for chopstick_a as their first chopstick but Olivia is the only philosopher going for chopstick_b first. Since she has less competition to get her first chopstick she ends up getting to take sushi more frequently. So, to make things fair for this example, I'll make all of the philosophers acquire the same two chopsticks, a first and then b. I'll save and run that program again. And this time Steve was especially greedy but there's still plenty to go around so all of the philosophers do get a chance to eat some. Now, let's see what happens if we drastically increase the number of philosophers at the dinner party using a for loop to create 50 instances of Barron, Olivia, and Steve. (keyboard clacking) That means we'll have 150 philosophers competing for 5000 pieces of sushi. I'll go back to the console and run that change and after running this program if I look at the final counts for each thread I can see that some philosophers, like Steve here, got a lot of sushi compared to others and many of these philosophers starve completely taking zero All of these threads were created with the same default priority but because there were so many some threads never got a chance to execute and this represents a real problem in programs containing a large number of threads. For example, if instead of feeding sushi to a bunch of philosophers this program was a web server that created new threads to handle a huge number of incoming requests some of those requests may never get processed. And that would lead to some very impatient and angry users on the other end. There are techniques that can be used to improve or even guarantee fairness among threads but that type of workload management is very situation dependent and beyond the scope of this course

Livelock
- Our greed and competition for sushi has led us to a life of deadlocks and starvation. There's only one piece of sushi left and I can see you're still hungry. You should have it. - Thank you my dear, but I can see you're still hungry too and I would feel like a lousy husband if I allowed my wife to go hungry. You should have the last bite. - Oh, but I can't bear to see you hungry. You shall have the final bite. - Well, this is annoying. We've entered into another tricky situation known as a livelock. A livelock looks similar to a deadlock in the sense that two threads are blocking each other from making progress, but the difference is that the threads in a livelock are actively trying to resolve the problem. A livelock can occur when two or more threads are designed to respond to the actions of each other. Both threads are busy doing something, but the combination of their efforts prevent them from actually making progress and accomplishing anything useful. The program will never reach the end. The ironic thing about livelocks is that they're often caused by algorithms that are intended to detect and recover from deadlock. If one or more process or thread takes action to resolve the deadlock, then those threads can end up being overly polite and stuck in a livelock. To avoid that, ensure that only one process takes action chosen by priority or some other mechanism, like random selection. - Rock, paper, scissors for it? - One, two, three shoot! - I win! - Now Olivia gets the last piece of sushi and that resolves our livelock.

Livelock: Python demo
- To demonstrate a livelock in Python we'll modify the original dining philosopher's example that we used to demonstrate a deadlock. Now, we have not implemented the priority strategy for lock ordering to prevent a deadlock. So if I run this program with Python livelock dot py, sure enough, after several iterations it hits a deadlock. The philosophers have acquired their first chopstick lock and they're stuck waiting for their second one to become available. One possible way to fix this problem would be to have the philosophers release the lock on their first chopstick if the second chopstick is not available when they try to take it. That will give another philosopher a chance to take that first chopstick and hopefully prevent a deadlock. To implement that, I'll modify the acquire method for the second chopstick on line 15 to be non-blocking by setting the blocking value to false. I'll also add an if statement to execute if that lock was not taken. And inside of that if statement, I'll print a message that says the philosopher released their chopstick. And then I'll release their first chopstick. Otherwise, I'll create a corresponding else clause. And if that happens we'll execute the try finally block to take a piece of sushi and then release both chopsticks. I'll save that change and then run the program. And I just see a lot of output messages that the philosophers are releasing their first chopstick. Occasionally, the scheduling might work out so that one of the philosophers is able to acquire both of the locks they need to take a bite of sushi but the majority of the time here, these philosophers are picking up their first chopstick, seeing that the second one is not available, so they politely put the first chopstick back so another philosopher can use it. If I press control-shift-escape to bring up the Windows task manager, over on the performance tab I can see the CPU usage is sitting at around 6%. And that corresponds to using one of the 24 processors available on this system. All of the threads are actively working to pick up and put down chopsticks. They're just not making any useful progress. Livelocks can be even harder to locate and debug than deadlocks but when your multi-threaded program seems to be stuck in a lock looking at the CPU utilization may give you some insight into whether it's a live or a deadlock. I'll stop this program by pressing control-break. Now, to resolve this livelock, I'm going to introduce some randomness into the locking process by importing the time and random modules. Then after a philosopher decides to put back their first chopstick, I'll make them sleep or philosophize for a random amount of time. With time dot sleep, random, and I'll divide by 10. So a tenth of a second. Now, when I run the program one final time if I scroll through it, I'll see way fewer instances that the philosophers returned their chopsticks. So they're able to keep taking pieces of sushi. The program makes progress and is able to finish without running into a deadlock or a livelock. 
