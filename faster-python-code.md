## Faster Python Code

https://www.linkedin.com/learning/faster-python-code

https://github.com/manwar/faster-python-code-661762

### Introduction

Welcome
- Hi there. I'm Miki Tebeka, a developer, mentor, instructor, and author. I love technology and write code daily, most of the time in Python. Optimized Python code will make your site or application more responsive, which means customers will stay longer and be more likely to return. This course is a compilation of optimization tips and tricks I've learned in the last 20 years. In this course, we'll cover how to find performance bottlenecks by profiling CPU and memory, picking the right data structures and algorithms, assorted tips and tricks to make your code faster, caching techniques, how to cheat the factory, parallel computation, using tools and languages such as C, Cython, Lambda, and others, and how to integrate performance into a process. Let's roll.

What you should know
- To get the most out of this course should have some familiarity with Python and the command line. I'll be using Python 3.6. But the code should work with other Python versions with slight modifications. But if you would like to follow along with me exactly I recommend using version 3.6. But the principles you learn here are applicable to any version of Python. I'll be using packages that are not in the Python Summit library. Should know how to install third-party modules. I've included scripts to install all of the requirements needed for all of the exercises. One for installation with pip. One for installation with conda. For some exercises you need a C compiler. There are great free compilers for all major platforms Linux, GCC, Os x, Xcode and Windows Visual Studio. You should be familiar with the command line. I'll be using a Mac machine to demonstrate. But this code should work on Linux and Windows as well. The exercise files contains code and an auxiliary file is required to run them. I'll mostly be using an optimized and unoptimized version of the code in 2 different files. So you'll be able to see the differences. These files are available in the exercise file folder for reference.

Use Codespaces with this course
- We have refreshed this course for you. What does it mean? First, we added support for Codespaces, so now you can head over to the GitHub repository for the course, click on the green Code button, select the Codespaces tab, and click on Create codespace on main. If there is this warning while you start out, just ignore it. Once your browser is inside Visual Studio Code, you're ready to go. Everything is there. You can follow along without the need to install anything. There are a couple of other changes. First, I've updated the requirements to be the latest version of every package, and I also updated python --version to the latest version of Python. So now you can start, follow along, and enjoy the ride.

### Tools of the Trade

Always profile first
- Here are the three rules of optimization. The first rule is don't. Most of the time, your code is fast enough. You should always have measurable performance goals before optimizing code. I've been in several situations where people told me, "make it as fast as you can." I usually answer this by saying, "okay, I'll get the team to create a special hardware then." Clarifying performance goal would effect the design of your system. The system that responds in one minute is very different than one that responds in one millisecond. After clarifying performance goals, you'll often find that Python is fast enough without any optimization at all. The second rule is don't yet. This rule usually means that since machine time is much cheaper than developer time, sometimes the solution is to get a better machine, faster network, or other solutions that don't involve changing code. The third rule is profile before optimizing. Profiling is the act of measuring where your code spends it's time. I've been doing code optimization for many years and the initial performance results still surprise me. Many times I found that the bottlenecks are in different place than the code I initially guessed. There are many tools for measuring how fast the code is running and where it spends it's time. Learning to use them effectively will make the process of finding your bottlenecks much easier. I'll also encourage you to read Rob Pike's Five Rules of Programming which is a very useful resource. One of the main takeaways is that bottlenecks occur in surprising places.

Always profile first
- Here are the three rules of optimization. The first rule is don't. Most of the time, your code is fast enough. You should always have measurable performance goals before optimizing code. I've been in several situations where people told me, "make it as fast as you can." I usually answer this by saying, "okay, I'll get the team to create a special hardware then." Clarifying performance goal would effect the design of your system. The system that responds in one minute is very different than one that responds in one millisecond. After clarifying performance goals, you'll often find that Python is fast enough without any optimization at all. The second rule is don't yet. This rule usually means that since machine time is much cheaper than developer time, sometimes the solution is to get a better machine, faster network, or other solutions that don't involve changing code. The third rule is profile before optimizing. Profiling is the act of measuring where your code spends it's time. I've been doing code optimization for many years and the initial performance results still surprise me. Many times I found that the bottlenecks are in different place than the code I initially guessed. There are many tools for measuring how fast the code is running and where it spends it's time. Learning to use them effectively will make the process of finding your bottlenecks much easier. I'll also encourage you to read Rob Pike's Five Rules of Programming which is a very useful resource. One of the main takeaways is that bottlenecks occur in surprising places.

Measuring time
- One of the simplest things we can do, is to measure time. Measuring time is helpful in many scenarios. When deciding between two alternatives, when gauging improvement, and to get metrics on run time, just to name a few. In Python, we have two basic methods of measuring time. The time module and the timeit module. Time is surprisingly tricky in computers. For example, if your computer updates its clock from the network while measuring, you might get negative results. I encourage you to read the awesome Falsehoods Programmers Believe About Time article to learn many more surprising aspects of how computers handle time. When measuring elapsed time, times when your process was sleeping due to rescheduling by the operating system are also included. You need to decide if you're interested in measuring sleeping time or not. In Python, people usually use the time function to measure time. However, in Python 3, we got monotonic and perf_counter functions for high-resolution monotonic timers. Let's see an example. Say we'd like to sum numbers up to n and we want to know whether to use a for loop or the built-in sum function. You can find the following Python file in the exercise files. So we input perf-counter from the time module. And then we define two functions. In line six, the function using the for loop, and in line 14, function using the built-in sum. And then we define in line 20 the constant n of how many times we want to calculate the upto. Noted in Python 3.6 you can add underscores to numbers to separate the thousandths. In line 22 we start the performance counter. In line 23 we run our code. And then in line 24 we calculate the duration by running performance counter again and subtracting the start time. And then at line 25 we print the duration, which will be in seconds. And we do the same thing again for the other function in line 27. Let's switch to the terminal and run this code. So, Python, using time dot py. And we see that the sum function is much faster. Sometimes the run time of a single call is too small and we would like to run the function many times and average the result. In this case, we are going to use the timeit module. Timeit also does some warmup code to make results more meaningful. Say you'd like to find the value associated with the key in a dictionary, and return a default if the key is not found. You can either use the dictionary built-in get method, or try to catch a key. So here in our code, we have items, which is the dictionary, which have two keys, a and b. And a default of minus one. In line 11 we define one function that uses try and catch. And in line 19, we define a different function which uses the dictionary's get method. In line 24, we start using the timeit function. If you look at line 26, you see that we code timeit with two arguments. The first argument is the actual call, and it should be a stream. And the second argument is a setup code. Repeat two scenarios. One when the key is in the dictionary, and the second one when the key is not in the dictionary. Let's run this from the terminal. Python using timeit dot py. We see that when the key is found in the dictionary, the try/catch method is faster. However, when the key is missing, get is faster. You should know which case is more common in your data. If you're using IPython or Jupyter Notebook, you can use time and timeit magic commands. This is how I usually work. Let's redo our timeit calculation in IPython Notebook. So I'll run the IPython Notebook, IPython. To load the code I'm going to use the run magic. So, percent run dash n, dash n tells run not to run the main part of the code, just to load the functions. And then name of the file. And now I can run the timeit, and this time I'm going to write the function just like a regular function code. Use get of a. And we see that we got 214 nanoseconds. And then we do percent timeit, use catch of a. This time we got 126 nanoseconds. We get the fancier output with units of measurement using the magic commands.

CPU profiling
- A profiler monitors the execution of your code and records where it spends its time. There are several profilers in the Python standard typer. cProfile, which is a deterministic profile is currently recommended by Python. Deterministic profiler record every function call and return, as well as exceptions. This is in contrast to statistical profilers, which record where the program is at small intervals. The documentation on the Python profilers is great and include important information on their limitation. Do read it. The profile generates a file with statistics on the run time and we use the pstats module to display them. The basic display is textual, but there are visual displays as well. Let's say we have a system that checks user login. To be secure, we store the user password in an encrypted form in a database. When the user logs in, we encrypt the password sent and compare it with the one stored in the database. Here is the code which you can find in the exercise file. You see in line two we use the crypt module to encrypt the passwords. And the main function is in line 25, where we do our login. At line 28 we fetch the password, the encrypted one, from the database. And in line 32 we encrypt the password sent from the user. And in line 33 we compare those passwords. To get meaningful statistics we'll create code that emulates good and bad logins. In gen cases we try to emulate real data. Most logins, 90% of them, are okay. And this is in line nine. And then the other cases, we'll divide into two cases. One of them with missing users or no such user. And the other one with bad password. In line 18, we have a function which gets all these cases and calls login on all of them. Line 25, we define how many cases we want. And in line 26, we generate the test cases for us. I'm going to show three options. So I'm going to use ifs to define between them. Let's switch to the terminal and run the call with cProfile. So we're going to switch to the terminal and write python -m cProfile and the name of our file. And we see that we get a lot of input. And we get the number of calls, the total time, the time per call, etc., etc. If you look closely, you see that there's a lot of setup code. This is because we are profiling the whole file, including the generation of the test cases. We can do better by targeting the code we like to profile. So I'm going to comment this one out on comment line 31. So now we're going to run cProfile just here after we generated the test cases. So let's go back to the terminal. And this time we're just going to write Python without the -m cProfile. And now we see mostly our call at the top. We can even generate an output file and use pstats to drill it. So let's go back to our code. And we use the last if statement. And here we tell cProfile to save the results in a file called, prof.out. Let's go to the terminal and run our call again. This time there is no out, so we'll do Python -m pstats and the name of the output file, which in our case is prof.out. Then we can have a look at the stats. Say the top 10 ones. We can even sort the stats. So I'm going to sort by cumulative time. And then show the stats again. This textual UI is good enough and I end up using it most of the time. However, sometimes we'd like a fancier UI. There are several models that do that. Let's have a look at SnakeViz. It's not in the start library. So you can install it via peep or conda. Once you install SnakeViz, we can run it on our output file. SnakeViz and the name of the output file, which is prof.out. We'll get a webpage with the results. At the bottom, you can see a table with the results in the textual format. And you can click on column headers to sort by them. On the top, you see these circles, which represent the execution. They start from the middle. This is the top most function that was executed. And go in layers after the functions that we had. We're interested in the batch login function so let's click on it. And then we see that the circles have changed. Now the middle one is the batch login and these are the parts that we're looking. We see that most of the time, we spent around here, which is the crypt method that we used. We can also have a look at the call stack for the current function. How did we get here? On prof, etc., etc. And you can always click Reset to reset the display. iPython in Jupiter Notebook have a magic command for profile. So we'll run iPython. And then we use the run magic with -n. We tell it not run the main code of prof.py. Now we'll generate some test cases. Cases equal list of gen-cases of 1000. And now we use the prun magic. Bench prun bench_login and rcases. And we see that we get the same textual output as before. By default, pron sorts the results by time. But you can also sort results by other values. To view the options you can do prun and a question mark. And if you scroll down a bit, we can see that we have the dash s for sorting and dividing arguments that we can use. So let's try it out. We can click on Q and then prun -s cumulative and bench_login of case. And now the results are sort in cumulative order. Using cProfile will slow down your program. If you need a more lightweight statistical profiler you can take a look at the vn prof package.

line_profiler
- Sometimes a finer granularity than a function is required. In these cases, line_profiler can be used. Line_profiler is not in the standard library. You can install it with pip or Conda. When we pip install line_profiler, we also get the command-line program called kernprof which is used to run our program under the line_profiler. Line_profiler doesn't automatically profile anything. You need to tell it which functions you'd like to monitor. We do this by adding a profile decorator. If you look at our code for logging in user, which you can find in the exercise file, at line 26 we have a login function. We want to profile it, so we'll add a profile decorator. A decorator lets you extends a function behavior without changing its code. If you're not familiar with Python decorators, check out our Python essential training course. And now, once we added the profile decorator and save the file, we are going to switch to the terminal and we're going to write kernprof and then dash L prof.py. Prof.py is a file that exercise the login function. And we see it wrote the results to a file called prof.py.lprof. To view this file we are going to write python dash N line_profiler and then prof.py.lprof. And now we can see line by line how much time was spent and what is the percentage of time. And we can see for example that line 33, the encrypting of the password, took 23% of the time, which is a lot of time. Line_profiler also adds a magic command to IPython or Jupyter notebooks, let's have a look. First we need to go back to our code and remove the profile decorator, and save. Then we'll go to the terminal and open IPython. To load the code we are going to do the magic run command dash N, not to run the main, and prof.py. Then we'll generate cases, with cases equal list of gen_cases of 1,000. And now we're going to load the line profile extension with load_ex and line_profiler. Once we have that we can now run the lprun which is the short magic command for running the line_profiler. We need to tell it with dash F which functions we are interested in. In our case the login, and we are going to call bench_login with our cases. And again, we see a similar out. It's time to try faster encryption algorithm. Let's see if replacing crypt with SHA256 will be faster. So we have our code and encrypt_passwd2 is using SHA256 to encrypt the password. Let's go back to IPython. We need to load the code. Let's use the run method and login.py. And now, we can use the timeit magic, to time it out. So, let's define a password. Password equal, let's say duck season. Let's load our new code as well. So, run enc256.py. First let's check that our function works. You'd be surprised how many times people optimize buggy algorithms. So, encrypt_passwd2 of the passwd. And we got some out. And now some time. So first let's time, timeit the original function. Timeit encrypt_passwd of the passwd. And now let's time this second version. So, percent timeit encrypt_passwd2 of our passwd. So 1.29 microseconds, versus 3.2 microseconds. Almost three times faster. If you're not familiar with microseconds and other units of measurements, here's a table that will help you understand it. We have milliseconds, which are abbreviated with ms and we have 1,000 of these in one second. After that we have microseconds which are abbreviated with mu, the Greek letter mu S, and there are 1,000,000 of these in a second. And then we have nanoseconds, abbreviated with ns, and there are 1,000,000,000 of these in a second. So pay attention to the unit of measurement timeit is showing you.

Tracing memory allocations
- Sometimes you'll hear people talking about memory leaks and memory efficient programs. How is memory connected to program runtime? There is data in a program. Numbers, lists, objects, et cetera. These are stored in the computer memory, known as the heap. Every time we create a new object, we need to allocate storage for it and this operation takes time. This is one reason for why we care about memory allocation. Another reason is that accessing memory in modern computers is done in layers. We have the CPU, and then we have L1 and L2 caches. Then we have the memory. And the access times are very different. Accessing layer 1 cache is about 0.5 ns, while accessing the main memory is 100 ns. Every time we try to access a piece of data, the CPU will first try to fetch from the cache, then from main memory. Difference, also known as latency, is huge. If we keep our data small, there's a good chance it will fit inside the cache line, and then it will be much faster to access. Sometimes algorithms that seem faster on paper are much slower than dumb cache friendly or sometimes known as cache oblivious algorithms. The last reason is that modern operating systems give us the appearance of having infinite memory. The computer I use right now has 16 GB of memory, but programs can use much more than that. This is done by swapping some section of memory into the hard drive. When we try to access the section memory that is currently swapped to disk, the operating system will pick another section of memory, will write to the disk, and then load the section memory we require into actual memory. This is also known as a page fault, and it's very costly operation. Hardware latency, even solid-state one, is order of magnitude slower than memory. All of this means we'd like to keep our memory small so we don't cause page faults. I hope you're convinced that you need to profile and monitor your memory as well. Tracemalloc is a library that was added in Python 3.4 as a tool to understand memory allocations. Let's look at an example. Assume we have our own encoding scheme for an event object. In reality you should use an existing encoding scheme like JSON, YAML, MessagePack or others. We are creating our own encoding just to demonstrate how to trace memory. Let's take a look at enc.py, which is found in the exercise file. The details are not that important, but in line 39 we have encode_event, which creates an encoder, and then calls the enc.encode method to encode the event. In main.py, we use tracemalloc. We import tracemalloc and we use a temporary file to encode the data. In line 50 to line 55, we create some sample data to test our code. And in line 59, we start the tracemalloc. In line 61 and 62, we encode all these events, and from line 64 onwards, we tell tracemalloc to print the statistics. Let's run this code. We're going to switch to the terminal, and run python enc.py. And we get some measurements about where the memory was allocated. If we look closely, we see that line 41 produces data. If we go back to our code, this is the line where we create a new object for encoding. One usual optimization to this kind of problems is to use an object pool. If you suspect that you have a memory issue in your program, tracemalloc is a good tool to have in your arsenal.

memory_profiler
- If you're running five from two and also need memory profile or need some memory user statistics, you can use memory profiler. Memory profiler is not in the stomach library. And we need to install it before we can use it. You want to compute the sum of differences of values in a list. The file is called sos.py and can find it in the exercise file. Once you have your function, you need to add the profile decorator before the function so memory profile will know to profile this function. At profile, and save the file. And now we switch the terminal and run python dash M memory profiler and our code sos.py. And we got some out. As we can see, line seven is the one that generates most of the memory. We can easily fix this by looking over intercedes and not over values, thus avoiding the location of vials to farming. When we install memory profiler, we also get amplify utility, which is time based profiling. Time based profiling will help us see if our memory continues to grow. We have the usual wavelike pattern of our location and garbage collection. Let's go back to our code. We remove the decorator and I'm going to increase the number from one million to 100 million. And save the file. Now we go back to the terminal and you run mprof run sos.py. Mprof generated a profile date. We can view this file after installing the Macbook library with mprof, plot, and the name of the file. And we get the graph and we see that our memory here continues to grow. This is just one function call so we should be fine. Usually you want to measure loops over time with mprof. Memory profiler also add a magic mp run command to python and others. Check the commentation for details.

### Picking the Right Data Structure

Big-O notation
- You'll often hear the term Big O in relation to performance. Big O Notation basically means in the worst-case, how long will our code run given a specific input? How long usually means the number of operation or, if you're looking at the memory, the size of memory. For example, let's say we'd like to know if a given number is inside a list. Ideally, it will be the first number on the list and we'll do one comparison. In the worst-case, the number won't be on the list and will iterate over the whole list to find out it's not there. Assuming the size of the list is N, we say the complexity is O of N. This is a very simple case and for more complex algorithms determining the complexity can be very difficult task. Lucky for us, someone else already figure out the complexity of most of the algorithms and data structures in use. For example, sorting is O of N times log N where log stands for logarithm. The nice folks at Python created a list on the complexity of various operations in the most common data structures. For example, appending to a list is O of one and sorting the list is O of N times log N. One question you should always ask is what does N mean? In our search example, it means the length of the list, however, if you're looking for a string in a list and strings there might be very long, N might be the length of the longest string in the list. And, sometimes, the complexity can depend on both the length of the list and the length of the longest string within it. Here a chart that shows how functions behave when N grows bigger. As we can see, 2 to power of N is growing much bigger than N squared. Big O ignores constants. For example, O of one means constant time. And, O of 1,000 is also considered to be O of one. Theoretically, this makes sense. But, in practice, if our code run a thousand times slower, it makes a difference. How does this help us? Knowing the complexity of operations is important for choosing the right data structure for the job. For example, if you'd like to know if an item has been seen, we'll pick a set or a digit which have O of one lookup over a list or a toppal which have O of N lookup time. If you know the theoretical limitation of an operation, we won't waste time trying to find a better solution that is better than this limit.

bisect
- Say you'd like to convert numerical scores to grades in the US system, where A is above 90, B is between 80 to 90, and so forth until the failed score below 60, which is F. Let's start with a simple implementation. I'll place this in a file called grades.py. As you see, we have a list called cutoffs and the names, and then a function on line six called grade, which just goes over the cutoffs, and once it find the grade, which is below the cutoff, returns the name. And we also have a test in line 17, which make sure that our code is working as expected. Once we have a test that passes, we can measure our code. We're going to use timeit module with ipython. So, we'll start ipython and we'll run the code grades.py. This will also run the test and since there are no errors, we're good. And now, let's time our code, timeit grade and let's say 74. This was 486 nanoseconds. If this time is acceptable, you should stop and work on something else. However, if you identified that this code is a bottleneck and we need to get faster, optimization is a good idea. Searching in a sorted list like cutoffs can be much faster. This is known as a binary search and its complexity is O of log N, which is much smaller than the O of N we do currently. And since we're using Python, we don't need to call this ourselves. We can use the bisect module. Bisect has a function called bisect that given a sorted list and a value, will return the index in the list where we should insert the value to keep the list sorted. Let's use it in the code. Before making changes, it's a good idea to commit the current point of code to our source control, Git, for example. I usually start with keeping the original code around and then creating a new one. This helps me compare runtime on different scenarios. So in our code now, we import bisect and I wrote a function called grade2, which just uses bisect. In line 24, I am asking bisect what is the index where we should insert this score, and I'm getting it from the names list. Also changed a little bit is the test. Now it gets the function to call and make sure that the function on the score is meeting the expectation. And now we run two tests, one for grade and one for grades2. Let's go back to our code. Don't forget to save. So, we're going to run grades again and since there's no errors, there's no output, which is good. And now, we can time our new function. Time grade2 of 74 and it's in 309 nanoseconds. We got about 33% speedup and as a bonus, shorter code. If you're not using ipython, you can use the timeit module from Python. Don't forget to divide the runtime you get by the number of loops timeit runs, which by default is one million.

```
  $ ipython
  $ %run grades.py
  $ %timeit grade(74)
  $ %timeit grade2(74)
```

deque
- Say we have requests coming in and we'd like to process them in order. For this we'll use a queue, also known as FIFO, first in first out data structure. Here's our initial implementation with a test. We'll place it in tasks.py. In line three I define a class called TaskQueue. In line six I use a list for the tasks, and in line nine, whenever someone pushes a task, I inserted it to the beginning of the list, and in line 12 when someone pops a task, we'll just tasks.pop, which would pop it from the end of the list, and then in line 18 I have a test for that to make sure that everything works. Let's run it. So ipython, and then run tasks.py, and the tests are passing since there is no output, which is a good start. Now let's measure. We're going to use our test function as the benchmark. So timeit test_queue with let's say 1000. So 1.3 milliseconds. If this bit is good enough, you should stop here. Python lists are optimized to work as a stack, also known as LIFO, last in first out. Items are added or removed from the right side. When changes are made to the left side of the list, the whole list is reallocated in memory. To solve this problem we have collections.deque, short for double ended queue, which behaves like a list but is optimized to work on both sides. The down sides of using deque is that item access time might be slower. Let's change our implementation to use deque. I'm going to leave the original code for comparison, and we'll replace the old code once I'm sure it's correct and faster. Here is the new code. I'm importing deque from collections model in line two, and then in line 18 I define DTaskQueue, which uses deque, and the main difference is that in line 21 we create a deque instead of a list. In line 24 we append, and in line 27 we do a popleft, which is a method of deque. I also changed the test. So now it get a class as the argument, and it test this class for correction. In the main part at line 46 and 47 I'm testing both classes. Don't forget to save the new code and let's run it again. So we'll do run tasks.py, and again there's no output, which mean the tests are passing, and now let's try again. We'll test the original queue, and now we'll benchmark the new queue. So we'll say that the class is the DTaskQueue. So 1.14 milliseconds versus 1.42 milliseconds. We got about 20% speed up.

```
  $ ipython
  In [1]: %run tasks.py
  In [2]: %timeit test_queue(1000)
  In [3]: %timeit test_queue(1000, cls=DTaskQueue)
```

heapq
- In some cases we'd like to order our tasks by priority. This is known as a priority queue. Our initial implementation will get a task and a priority, add them to the list, and then sort the list by priority. This way, when we pop the next task to execute, we'll get the one with the highest priority. Priority is given in decreasing numbers, meaning priority one is higher than three. If we look at the code, at line six we have the priority queue, and we create in line nine the tasks, which is a list. And every time we push some task with a priority, we create a topple with the priority and the task and then sort it in line 13. When we pop, we pop the first item on the list and get item number one from it which is the task to execute. In line 22 we add a test, the test's a priority queue. And we also have a benchmark. In line 49, we have a function called gen_cases, which generate test cases for us. And in line 61, we have a benchmark that gets the cases and executes the priority queue on them. And at the end, in the main, we run the test. Let's check our code. So we run ipython and then we run ptasks.py. It completed without an error, which means the test is passing. Now let's find a benchmark, so cases will be gen_cases, let's say 1000 test cases. And this is done to avoid measuring the time of generating the test cases in the benchmark. And now we will want to time it benchmark_pq with our cases. About 2.73 milliseconds. Let's see what's taking most of the time. We'll use the prun major command, which runs our code under profile benchmark_pq with our cases. We see that the list sorting we do which is off and login, is taking most of the time. We can do better with a heap, which is a data structure optimized for this and others. The complexity of pushing and popping from a heap is o off login. In Python, you'll find a heap implementation in the heap queue model. Let's change our code. So in line three, we are importing heap, pop and heap push from heap queue. In line 22, we define age priority queue which is a heap priority queue. It chooses a heap, and in line 28, when we push, we do a heap push. In line 31 we did a heap pop. We also change the test a bit to get the cls to test as an argument. And the same for the benchmark, we also change the benchmark to get the cls to benchmark. And in the main we are running both tests, one for the initial queue and the second one with the one from here. Don't forget to save your code. And now let's run it again. It completed without errors which is good. And now we can benchmark, so let's time the old code. Time it, benchmark_pq with cases. Okay, now the new code, time it benchmark_pq with cases and the class is the high priority queue. If you look closely, you see that the units now are in microseconds, so this is much faster than the initial implementation. About four time faster, to be exact. Every time you need to pick the highest priority object, think about heap.

```
  $ ipython
  In [1]: %run ptasks.py
  In [2]: cases = gen_cases(1000)
  In [3]: %timeit benchmark_pq(cases)
  In [4]: %prun benchmark_pq(cases)
  In [5]: %prun benchmark_pq(cases, cls=HPriorityQueue)
```

Beyond the standard library
- As the saying goes, "If all you have "is a hammer, everything looks like a nail." We must learn and enhance our toolbox so we can solve problems with the right tool every time. There are many algorithms and I don't expect you to remember them all, I know I don't. But knowing that there are a lot out there is a good start. And between Google, Stack Overflow, and other forums, there are plenty of resources available to find what you need. The Python standard library can't contain implementation of every algorithm or data structure out there. But there are many third-party packages that you can use. Let's explore one case. Say we are starting a competitor to Uber or Lyft. When a user requests a ride we need to find the closest available car and send it their way. Looking at our code, we have at line seven a distance function that calculates a distance between two points. And then at line 14 we have the "find closest" function, that gets the location and list of drivers and return their closest driver to this location. And then in line 22 we have the benchmarking code, which generates the list of drivers from a given location. In line 28 we have a test for testing our code. And in line 46 we run this test in the main function. Let's try it out. So ipython. And then run drivers.py. And since there is no output it means that the tests are passing. Let's take a point, which happens to be the location of the Lynda office. And we'll generate some drivers. So drivers = gen_drivers from this location. And now let's time. So timeit find_closest with latitude and longitude and the list of drivers. So about 478 microseconds. You can profile it to see what's taking most of the time. So instead of the time it magic, we are going to use the prun magic, which runs the code under profiler. We call this stance for every driver to find out the closest one. There is a data structure called, KDTree. You set a tree with locations. And then looking for ones close to a given point is of login. Looking outside the standard library for packages I highly recommend picking ones that are well established. You can learn about the status of a package by looking at GitHub stars, latest commit, number of developers, etc. You should also ask around. We're going to use the KDTree implementation in psy pi, which is over 16 years old and used by thousands of developers. You can install psy pi with Peep or Condo. Once installed, we can start using it. This is going to be the new code. So in our new code at line five, we import KDTree from psy pi.special. And then at line 23 we have find closest KD, which gets a location entry and returns the closest one from the tree. We'll also change our test function to accommodate for testing the KDTree. And in the main in line 57 and 58, we do test both for the old class and the new class. Don't forget to save your code. And let's try it out. So we run the drivers and it completed without errors, which is good. And now, let's benchmark. So let's run the benchmark for the old class first. Another I'm going to rerun the (mumbles) and then we'll continue from there. Run drivers. Lat and Ing. Drivers = gen_drivers. And then we time it. And then the prun. And now running drivers again. And now we're at the point where I want it to be. So first, we're going to benchmark the old code. So we want to time it on defined classes. And next, we're going to benchmark our new code, which needs a tree. So we'll create a tree. Tree = KDTree of drivers. And then timeit find_closest_kd of our latitude and longitude and the tree itself. So to calculate, let's see. So 485 divided by 80.8. So we're about six times faster by switching to KDTree. Every time you face a new problem, do your homework and find the right data structure to solve it.

```
  $ ipython
  In [1]: %run drivers.py
  In [2]: lat, lng = 34.3852712, -119.487444
  In [3]: drivers = gen_drivers(lat, lng)
  In [4]: %timeit find_closest((lat, lng), drivers)
  In [5]: %prun find_closest((lat, lng), drivers)
  In [6]: tree = KDTree(drivers)
  In [7]: %prun find_closest_kd((lat, lng), tree)
```

### Tricks of the Trade

Local caching of names
- Knowing how Python operates will give you more tools for optimization. Let's take a look at some language features and tools that might give your code a boost. Let's say we want to normalize a list of numbers. So, we have a configuration at line three for the factor and the threshold. And then in line nine, we have a normalize. We'd go over the numbers and if they're above the threshold, we divide them by the factor. And then return. In line 23, we just regenerate a list of numbers, so we can test. Let's run the code. So, ipython and then run norm.py. Let's time it, timeit normalize of numbers. So, 198 microsecond. At first, it doesn't look like there's much delay but let's look at how Python compiles this function. So, we import the dis module and we do dis.dis of normalize. We see that twice we have LOAD_GLOBAL and after it, LOAD_ATTR. So, Python is looking for the name config in the global and then looking for the attribute threshold inside. The same here, we have LOAD_GLOBAL for config, load attribute for the factor. Python does this lookup on every duration of the loop. Attribute lookup is fast, but it's still a dictionary lookup. And since config is global, we also look for its name in the global's dictionary, which is another lookup. Let's save these values at the beginning of the function. So here we have normalize2. normalize2, at line 21 and 22, we save locally the threshold and the factor, and then run. Let's see how it's affect. Don't forget to save, and then let's run again. And let's first disassemble to see what we have. So, dis.dis of normalize2 and we see that we do the LOAD_GLOBAL and the LOAD_ATTR outside of the loop, which starts here. And let's measure our old code and the new code. And we can do 120 divided by 184. So, it's about 35% speedup. This optimization comes at the cost of some code complexity, so consider if the trade-off is worthy. If we look at disassembly, we still do one more lookup, which is for norm.append. We can get rid of it as well. So here we have normalize3, which in line 38 we also cache the norm.append, so we don't have to look it up. Let's save the code and run it again. And let's time it. So, this is normalize3 now and it's about half the time of the original implementation. So, attribute caching can help you with optimization.

```
  $ ipython
  In [1]: %run norm.py
  In [2]: %timeit normalize(numbers)
  In [3]: import dis
  In [4]: dis.dis(normalize)
  In [5]: %run norm.py
  In [6]: %timeit normalize2(numbers)
  In [7]: %run norm.py
  In [8]: %timeit normalize3(numbers)
```

Remove function calls
- Functions are a great way to organize your code. You get modularity, re-usability, readability, and more. You might have seen this acronym, T-A-N-S-T-A-A-F-L. It means there ain't no such thing as a free lunch. There is a cost to functions, and it's the overhead of calling them. How much time are we talking about? Let's see. So ipython, we'll define an empty function that does nothing, and we'll use timeit to see how much time it takes to call it. So 66.3 nanoseconds, which is pretty fast, but still considered a lot for a function call. Here's an example. We'd like to fix a list of numbers and make the even numbers absolute. So we define a function for abs_even in line four, and then in line 17 we just do a list comprehension to create this number. And in line 22 I'm creating such a list of numbers so we can time it. Let's benchmark. So run fncall.py, and then we did timeit on fix_nums of numbers. Please note that the numbers might be a little bit different when you edit the code. And this is the new code. And now let's run it. So, we're going to run again fncall.py, and let's time this time the inline inversion. Right, then you can do 95.3 divided by 148. So we get about 35% speed up. This comes at a great cost to program structure and make code harder to test. However, sometimes to squeeze more speed from your program, you do inline function. Sometimes function calls are hidden from us. This is mostly the case of properties. I see people use properties all over the place without realizing the cost. In Python we use properties only if you have a good reason and not by default like with Java or C++ where you add getter and setter to everything. Here is a small example. In this case I have two classes. One is a simple point with X and Y, and one, which I called PPoint, which have properties for the X and the Y for setter and get. Let's time this code. So run prop.py, and then I'm creating a regular point, and I'm doing timeit of accessing the X. Now I'm going to create another point and this time one with the properties class, and, again, timeit p2.x. This is about four times slower. Try to use properties only if you have a good reason and be conscious of the performance penalties.

```
  $ ipython
  In [1]: def noop():
  ...:        pass
  ...:
  In [2]: %timeit noop()
  In [3]: %run fncall.py
  In [4]: %timeit fix_nums(numbers)
  In [5]: %run fncall.py
  In [6]: %timeit fix_nums_inline(numbers)
  In [7]: %run prop.py
  In [8]: p1 = Point(1, 2)
  In [9]: %timeit p1.x
  In [10]: p2 = PPoint(1, 2)
  In [11]: %timeit p2.x
```

Using __slots__
- There are cases, when we create a lot of small objects. In most cases, this is fine, but it might be an issue, when the number of objects becomes very big. Python Objects store attributes in a dictionary called __dict__. Or dunder dict for short. Dictionaries in Python are optimized for access speed, and most of the time are 1/3 empty. Carrying a 1/3 empty dictionary per instance, can be a big overhead sometimes. For just these cases, Python offers us dunder slots, which remove the dunder dict. The downside of using dunder slots, is that you can't add new attributes to objects. Let's have a look at our code, in slots.py We have a class Point, which is the usual class. And we have the class SPoint, which, the only difference, is that we define, in line 17 dunder slots. In the main, we define two functions to allocate points. Once, in line 29, we allocate regular points, and in line 32, we allocate SPoints, with the dunder slots. And in line 35 to 37, we allocate the points themselves. Let's see how much memory are we talking about, ipython, and then we %run slots.py And then, let's check the memory. So we import the sys model, and sys.getsizeof(points) And then, we look at the slot points. So, sys.getsizeof(SPoints) Looks like there is no improvement, but once we remember that the list holds references to objects, this makes sense. These are lists of one million pointers. And these pointers are of the same size. Understanding how much memory an object consumes in Python can be tricky. A lot of objects hold reference or pointers to other objects. There are objects shared by several other objects, and other considerations. I'm going to use Memory Profiler, to see how much memory we allocate. You can install Memory Profiler using Pip or Conda. Once you have Memory Profiler installed, you can start using it by %load_ext and memory_profiler. And now we're going to run, using the the mprun magic command. %mprun -f alloc_points so it will profile the alloc points. And we call alloc_points with that number that we gave before, which is one million. If you look at the Increment column, we see that we allocated around 190 megabytes. And now we'll do the same for the SPoints. So the alloc_spoints and alloc_spoints. And run this one. We're down to 85.2 megabytes. This is less than half of the memory, by adding one line of code. Smaller objects might also make your program faster, since they can fit in a CPU cache line, which means that when the CPU tries to access them, it will be much faster than fetching them from main memory.

```
  $ ipython
  In [1]: %run slots.py
  In [2]: import sys
  In [3]: sys.getsizeof(points)
  In [4]: sys.getsizeof(spoints)
  In [5]: %load_ext memory_profiler
  In [6]: %mprun -f alloc_points alloc_points(n)
  In [7]: %mprun -f alloc_spoints alloc_spoints(n)
```

Built-ins
- Let's say that we have a list of tasks. Each task is a tuple, where the first field is the task name and the second field is the priority. You can see it in our code in builtin.py at line 14. We'd like to solve the tasks by priority. In Python, tuples are sorted in lexicographical order, which means if we'll sort as-is, we'll get the tasks sorted by their name. Tasks names are strings, and the string '3' is bigger than string "200". Let's see an example. "iphython" and when we do three bigger than 200, we get True. Python provides a "key" argument to sort. Key is a function that gets the object we'd like to compare and returns a value representing this object. For example, if we have names equal "tweety" and "bugs" and "daffy" and "elmer", if we'll do sorted of names, we'll get them sorted by lexicographical order. However, we can do sorted names and the key equal len, and then we'll get the names sorted by their length. We can use that to sort our tasks. In line 5 we define a function called sort_tasks, which the key function is a lambda, an anonymous function that gets a task and return the second element which is the priority of the task. Let's run it. So let's time it using the timeit magic. timeit sort_tasks of tasks, so 286 microseconds. How can we do better? Our lambda function is called once per object, and yes, it's once per object, not n log n, since Python caches the results. Since sorting by different fields is very common, Python provides functions that do that but are written in C and will be a bit faster. Let's see. Here we have the new code. We import itemgetter from the model operator in line 3. And in line 10, I define the new function that uses this itemgetter. Let's save it and see the difference. So, we do run of builtin.py and this time we're going to check our new function, sort_tasks_ig, and 252 microseconds. So we got about 10% speedup and less code. Python also provides attrgetter that gets the attribute to return and there are many functions Python already provides which are written in C, such as sum, max and others. If you'd some time learning what the standard library offers and try to avoid writing your own functions.

```
  $ ipython
  In [1]: %run builtin.py
  In [2]: %timeit sort_tasks(tasks)
  In [3]: %run builtin.py
  In [4]: %timeit sort_tasks_ig(tasks)
```

Allocate
- Let's say we have a pool of workers and we'd like to know how many tasks each worker has done. We can start with a list in the length of the workers, filled with zeros, and have each worker increment their value in their index. This allocation can happen many time if you frequently allocate the pool of workers. Let's write a function to create this list. The code is in alloc.py. So in line four, we define allocz which creates a list filled with zero for a given size. And let's time it, ipython and then we run alloc.py. And let's time it of 1000. How can we do better? In Python, if you multiply a list with a number, we get n copies of this list. For example, if I take the list of one, two, and three and I multiply it by three, I'll get this list three times. Let's use this in our code. In a new code, I wrote a function called allocz fixed, which returns a list of one element with zero times the size. Don't forget to save the code and let's try it out. So we run again the code to load it, and then we're going to time the new function. Okay, this is about 10 times faster than the previous code. When you create a list and append to it, Python needs to reallocate memory for the list. To avoid reallocation on every append, the list grows in fixed sizes. The sizes are zero, four, eight, 16, 25, etc. However, when we create a list with multiplication, Python knows the size of the list in advance and creates it in one allocation. Note that the initial value is duplicated, and in Python because everything is a reference, this might have a surprising effect. If you have an array, which is the empty list times five. And then we appending to the first list in the array the number one, and we look at the array we got the number one in all of them. Make sure you initialized with immutable volumes, like numbers, strings, and tables, otherwise you will have to get back to our initial implementation. Another option is to use numpy, which provides ultra fast arrays. So, we import numpy as np and time it np.zeros of 1000. So we got 968 nanoseconds, versus 2.24 microseconds in the allocz fixed, so it's about two and a half times faster. For faster memory allocation, consider using fixed size allocation or numpy.

```
  $ ipython
  In [1]: %run alloc.py
  In [2]: %timeit allocz(1000)
  In [3]: %run alloc.py
  In [4]: %timeit allocz_fixed(1000)
  In [5]: import numpy as np
  In [6]: %timeit np.zeros(1000)
```

### Caching

Overview
- Caching is the act of saving computation results and reusing them instead of running the calculations again. It is very common practice and will speed up your code in many scenarios. The price you pay is the memory used to hold the computed values. This runtime versus memory trade off is very common in computer science. Let's see an example. The Fibonacci sequence is defined as the first two values of one and every value after it is the sum of the two values before it. In fib.py, we have an implementation which is a cause. If the number in line six is smaller than two, we return one otherwise we return Fibonacci of N minus one plus Fibonacci of N minus two. Let's time it. So, ipython and then we run fib.py. And then, let's run timeit of fib of 20. So, it's 2.82 milliseconds. This implementation is slow since we do a lot of repeated calculation. When we compute Fibonacci of four, we'll call Fibonacci of one three times, Fibonacci of two two times, et cetera, et cetera. By the way, don't try to run Fibonacci of 100. You'll wait a long time. Let's cache the results. In the new code, in line 11, we have Fibonacci cache which is dictionary holding the computed values. And then, we have the function fibc in line 14. And, in line 19, we try to see if we have the value already in the cache. And only if it's not in the cache, we compute it and place it in the cache. Don't forget to save your code. And, let's try it out. So, we run again the Fibonacci dot py and this time, we're going to time the fibc function, which is much faster. Note that these are 240 nanoseconds which is 2.18 milliseconds. So, it's about a thousand time faster. Our naive cache implementation will grow indefinitely which might become a problem. Also, if our functions are not pure, meaning they might return different values at different times. For example, when we ask the database how many users there were in the last 24 hours. We need to update the cache. There are methods of evicting or invalidating values from the cache. They can get tricky. There's a saying that the two hardest problems in computer science are naming things and caching validation. And, I find it to be true. Depending on your data, you need to pick a good cache eviction strategy. There's a lot of research out there on the subject. Do your homework.

```
  $ ipython
  In [1]: %run fib.py
  In [2]: %timeit fib(20)
  In [3]: %run fib.py
  In [4]: %timeit fibc(20)
```

Pre-calculating
- We'd like to count how many beats in a number for set one. This is sometimes known as pop count for population count. Knowing the number of set base is useful in several algorithms such as hemmingway. Here's an implementation. In line four, we have nbits, which does the wire loop on the number and checks how many beats are there and in line 16, we define a test for that, which checks the number of beats. And in the main in line 28, we call this test. Let's try it out. So ipython. And then we run nbits.py. And since we don't have an error, we're good to go. And now we can time. Time it nbits of, let's say, 350. It's 1.07 microseconds. How can we speed this up? The test might be less than ideal. We can precalculate the value for each number once and then we turn these values. This works well with the numbers are bounded by size. Let's assume that our numbers can have a maximum of 16 beats. What's the largest value you can get with 16 beats? So we can use python for that. You do int of one times 16, so we'll get a string of 16 ones. And we tell python that this is in phase two. So this is the largest number you can get for 16 beats. Now, we can initialize an array with a number of beats and return them. We'll also update our test. Here's the new code. We update the test to get the function and check that the function returns the right value. In line 32 we define a cache. We run the original function for all 16 beats now. And then in line 35, the nbits fixed, just look at the number in this cache. And we edit the test for this one as well in line 44. Don't forget to save your code before trying it out. So we'll do a run of nbits.py and it finished without an error, which means that the test is passing. And now we can time it. So this time, we're going to time nbits fixed. So 147 nanoseconds versus one microsecond. Much faster. We traded memory for speed. This will work if the function we precalculate is pure, meaning it will return the same value for the same input every time.

