## Faster Python Code

https://www.linkedin.com/learning/faster-python-code

https://github.com/manwar/faster-python-code-661762

### Introduction

Welcome
- Hi there. I'm Miki Tebeka, a developer, mentor, instructor, and author. I love technology and write code daily, most of the time in Python. Optimized Python code will make your site or application more responsive, which means customers will stay longer and be more likely to return. This course is a compilation of optimization tips and tricks I've learned in the last 20 years. In this course, we'll cover how to find performance bottlenecks by profiling CPU and memory, picking the right data structures and algorithms, assorted tips and tricks to make your code faster, caching techniques, how to cheat the factory, parallel computation, using tools and languages such as C, Cython, Lambda, and others, and how to integrate performance into a process. Let's roll.

What you should know
- To get the most out of this course should have some familiarity with Python and the command line. I'll be using Python 3.6. But the code should work with other Python versions with slight modifications. But if you would like to follow along with me exactly I recommend using version 3.6. But the principles you learn here are applicable to any version of Python. I'll be using packages that are not in the Python Summit library. Should know how to install third-party modules. I've included scripts to install all of the requirements needed for all of the exercises. One for installation with pip. One for installation with conda. For some exercises you need a C compiler. There are great free compilers for all major platforms Linux, GCC, Os x, Xcode and Windows Visual Studio. You should be familiar with the command line. I'll be using a Mac machine to demonstrate. But this code should work on Linux and Windows as well. The exercise files contains code and an auxiliary file is required to run them. I'll mostly be using an optimized and unoptimized version of the code in 2 different files. So you'll be able to see the differences. These files are available in the exercise file folder for reference.

Use Codespaces with this course
- We have refreshed this course for you. What does it mean? First, we added support for Codespaces, so now you can head over to the GitHub repository for the course, click on the green Code button, select the Codespaces tab, and click on Create codespace on main. If there is this warning while you start out, just ignore it. Once your browser is inside Visual Studio Code, you're ready to go. Everything is there. You can follow along without the need to install anything. There are a couple of other changes. First, I've updated the requirements to be the latest version of every package, and I also updated python --version to the latest version of Python. So now you can start, follow along, and enjoy the ride.

### Tools of the Trade

Always profile first
- Here are the three rules of optimization. The first rule is don't. Most of the time, your code is fast enough. You should always have measurable performance goals before optimizing code. I've been in several situations where people told me, "make it as fast as you can." I usually answer this by saying, "okay, I'll get the team to create a special hardware then." Clarifying performance goal would effect the design of your system. The system that responds in one minute is very different than one that responds in one millisecond. After clarifying performance goals, you'll often find that Python is fast enough without any optimization at all. The second rule is don't yet. This rule usually means that since machine time is much cheaper than developer time, sometimes the solution is to get a better machine, faster network, or other solutions that don't involve changing code. The third rule is profile before optimizing. Profiling is the act of measuring where your code spends it's time. I've been doing code optimization for many years and the initial performance results still surprise me. Many times I found that the bottlenecks are in different place than the code I initially guessed. There are many tools for measuring how fast the code is running and where it spends it's time. Learning to use them effectively will make the process of finding your bottlenecks much easier. I'll also encourage you to read Rob Pike's Five Rules of Programming which is a very useful resource. One of the main takeaways is that bottlenecks occur in surprising places.

Always profile first
- Here are the three rules of optimization. The first rule is don't. Most of the time, your code is fast enough. You should always have measurable performance goals before optimizing code. I've been in several situations where people told me, "make it as fast as you can." I usually answer this by saying, "okay, I'll get the team to create a special hardware then." Clarifying performance goal would effect the design of your system. The system that responds in one minute is very different than one that responds in one millisecond. After clarifying performance goals, you'll often find that Python is fast enough without any optimization at all. The second rule is don't yet. This rule usually means that since machine time is much cheaper than developer time, sometimes the solution is to get a better machine, faster network, or other solutions that don't involve changing code. The third rule is profile before optimizing. Profiling is the act of measuring where your code spends it's time. I've been doing code optimization for many years and the initial performance results still surprise me. Many times I found that the bottlenecks are in different place than the code I initially guessed. There are many tools for measuring how fast the code is running and where it spends it's time. Learning to use them effectively will make the process of finding your bottlenecks much easier. I'll also encourage you to read Rob Pike's Five Rules of Programming which is a very useful resource. One of the main takeaways is that bottlenecks occur in surprising places.

Measuring time
- One of the simplest things we can do, is to measure time. Measuring time is helpful in many scenarios. When deciding between two alternatives, when gauging improvement, and to get metrics on run time, just to name a few. In Python, we have two basic methods of measuring time. The time module and the timeit module. Time is surprisingly tricky in computers. For example, if your computer updates its clock from the network while measuring, you might get negative results. I encourage you to read the awesome Falsehoods Programmers Believe About Time article to learn many more surprising aspects of how computers handle time. When measuring elapsed time, times when your process was sleeping due to rescheduling by the operating system are also included. You need to decide if you're interested in measuring sleeping time or not. In Python, people usually use the time function to measure time. However, in Python 3, we got monotonic and perf_counter functions for high-resolution monotonic timers. Let's see an example. Say we'd like to sum numbers up to n and we want to know whether to use a for loop or the built-in sum function. You can find the following Python file in the exercise files. So we input perf-counter from the time module. And then we define two functions. In line six, the function using the for loop, and in line 14, function using the built-in sum. And then we define in line 20 the constant n of how many times we want to calculate the upto. Noted in Python 3.6 you can add underscores to numbers to separate the thousandths. In line 22 we start the performance counter. In line 23 we run our code. And then in line 24 we calculate the duration by running performance counter again and subtracting the start time. And then at line 25 we print the duration, which will be in seconds. And we do the same thing again for the other function in line 27. Let's switch to the terminal and run this code. So, Python, using time dot py. And we see that the sum function is much faster. Sometimes the run time of a single call is too small and we would like to run the function many times and average the result. In this case, we are going to use the timeit module. Timeit also does some warmup code to make results more meaningful. Say you'd like to find the value associated with the key in a dictionary, and return a default if the key is not found. You can either use the dictionary built-in get method, or try to catch a key. So here in our code, we have items, which is the dictionary, which have two keys, a and b. And a default of minus one. In line 11 we define one function that uses try and catch. And in line 19, we define a different function which uses the dictionary's get method. In line 24, we start using the timeit function. If you look at line 26, you see that we code timeit with two arguments. The first argument is the actual call, and it should be a stream. And the second argument is a setup code. Repeat two scenarios. One when the key is in the dictionary, and the second one when the key is not in the dictionary. Let's run this from the terminal. Python using timeit dot py. We see that when the key is found in the dictionary, the try/catch method is faster. However, when the key is missing, get is faster. You should know which case is more common in your data. If you're using IPython or Jupyter Notebook, you can use time and timeit magic commands. This is how I usually work. Let's redo our timeit calculation in IPython Notebook. So I'll run the IPython Notebook, IPython. To load the code I'm going to use the run magic. So, percent run dash n, dash n tells run not to run the main part of the code, just to load the functions. And then name of the file. And now I can run the timeit, and this time I'm going to write the function just like a regular function code. Use get of a. And we see that we got 214 nanoseconds. And then we do percent timeit, use catch of a. This time we got 126 nanoseconds. We get the fancier output with units of measurement using the magic commands.

CPU profiling
- A profiler monitors the execution of your code and records where it spends its time. There are several profilers in the Python standard typer. cProfile, which is a deterministic profile is currently recommended by Python. Deterministic profiler record every function call and return, as well as exceptions. This is in contrast to statistical profilers, which record where the program is at small intervals. The documentation on the Python profilers is great and include important information on their limitation. Do read it. The profile generates a file with statistics on the run time and we use the pstats module to display them. The basic display is textual, but there are visual displays as well. Let's say we have a system that checks user login. To be secure, we store the user password in an encrypted form in a database. When the user logs in, we encrypt the password sent and compare it with the one stored in the database. Here is the code which you can find in the exercise file. You see in line two we use the crypt module to encrypt the passwords. And the main function is in line 25, where we do our login. At line 28 we fetch the password, the encrypted one, from the database. And in line 32 we encrypt the password sent from the user. And in line 33 we compare those passwords. To get meaningful statistics we'll create code that emulates good and bad logins. In gen cases we try to emulate real data. Most logins, 90% of them, are okay. And this is in line nine. And then the other cases, we'll divide into two cases. One of them with missing users or no such user. And the other one with bad password. In line 18, we have a function which gets all these cases and calls login on all of them. Line 25, we define how many cases we want. And in line 26, we generate the test cases for us. I'm going to show three options. So I'm going to use ifs to define between them. Let's switch to the terminal and run the call with cProfile. So we're going to switch to the terminal and write python -m cProfile and the name of our file. And we see that we get a lot of input. And we get the number of calls, the total time, the time per call, etc., etc. If you look closely, you see that there's a lot of setup code. This is because we are profiling the whole file, including the generation of the test cases. We can do better by targeting the code we like to profile. So I'm going to comment this one out on comment line 31. So now we're going to run cProfile just here after we generated the test cases. So let's go back to the terminal. And this time we're just going to write Python without the -m cProfile. And now we see mostly our call at the top. We can even generate an output file and use pstats to drill it. So let's go back to our code. And we use the last if statement. And here we tell cProfile to save the results in a file called, prof.out. Let's go to the terminal and run our call again. This time there is no out, so we'll do Python -m pstats and the name of the output file, which in our case is prof.out. Then we can have a look at the stats. Say the top 10 ones. We can even sort the stats. So I'm going to sort by cumulative time. And then show the stats again. This textual UI is good enough and I end up using it most of the time. However, sometimes we'd like a fancier UI. There are several models that do that. Let's have a look at SnakeViz. It's not in the start library. So you can install it via peep or conda. Once you install SnakeViz, we can run it on our output file. SnakeViz and the name of the output file, which is prof.out. We'll get a webpage with the results. At the bottom, you can see a table with the results in the textual format. And you can click on column headers to sort by them. On the top, you see these circles, which represent the execution. They start from the middle. This is the top most function that was executed. And go in layers after the functions that we had. We're interested in the batch login function so let's click on it. And then we see that the circles have changed. Now the middle one is the batch login and these are the parts that we're looking. We see that most of the time, we spent around here, which is the crypt method that we used. We can also have a look at the call stack for the current function. How did we get here? On prof, etc., etc. And you can always click Reset to reset the display. iPython in Jupiter Notebook have a magic command for profile. So we'll run iPython. And then we use the run magic with -n. We tell it not run the main code of prof.py. Now we'll generate some test cases. Cases equal list of gen-cases of 1000. And now we use the prun magic. Bench prun bench_login and rcases. And we see that we get the same textual output as before. By default, pron sorts the results by time. But you can also sort results by other values. To view the options you can do prun and a question mark. And if you scroll down a bit, we can see that we have the dash s for sorting and dividing arguments that we can use. So let's try it out. We can click on Q and then prun -s cumulative and bench_login of case. And now the results are sort in cumulative order. Using cProfile will slow down your program. If you need a more lightweight statistical profiler you can take a look at the vn prof package.

line_profiler
- Sometimes a finer granularity than a function is required. In these cases, line_profiler can be used. Line_profiler is not in the standard library. You can install it with pip or Conda. When we pip install line_profiler, we also get the command-line program called kernprof which is used to run our program under the line_profiler. Line_profiler doesn't automatically profile anything. You need to tell it which functions you'd like to monitor. We do this by adding a profile decorator. If you look at our code for logging in user, which you can find in the exercise file, at line 26 we have a login function. We want to profile it, so we'll add a profile decorator. A decorator lets you extends a function behavior without changing its code. If you're not familiar with Python decorators, check out our Python essential training course. And now, once we added the profile decorator and save the file, we are going to switch to the terminal and we're going to write kernprof and then dash L prof.py. Prof.py is a file that exercise the login function. And we see it wrote the results to a file called prof.py.lprof. To view this file we are going to write python dash N line_profiler and then prof.py.lprof. And now we can see line by line how much time was spent and what is the percentage of time. And we can see for example that line 33, the encrypting of the password, took 23% of the time, which is a lot of time. Line_profiler also adds a magic command to IPython or Jupyter notebooks, let's have a look. First we need to go back to our code and remove the profile decorator, and save. Then we'll go to the terminal and open IPython. To load the code we are going to do the magic run command dash N, not to run the main, and prof.py. Then we'll generate cases, with cases equal list of gen_cases of 1,000. And now we're going to load the line profile extension with load_ex and line_profiler. Once we have that we can now run the lprun which is the short magic command for running the line_profiler. We need to tell it with dash F which functions we are interested in. In our case the login, and we are going to call bench_login with our cases. And again, we see a similar out. It's time to try faster encryption algorithm. Let's see if replacing crypt with SHA256 will be faster. So we have our code and encrypt_passwd2 is using SHA256 to encrypt the password. Let's go back to IPython. We need to load the code. Let's use the run method and login.py. And now, we can use the timeit magic, to time it out. So, let's define a password. Password equal, let's say duck season. Let's load our new code as well. So, run enc256.py. First let's check that our function works. You'd be surprised how many times people optimize buggy algorithms. So, encrypt_passwd2 of the passwd. And we got some out. And now some time. So first let's time, timeit the original function. Timeit encrypt_passwd of the passwd. And now let's time this second version. So, percent timeit encrypt_passwd2 of our passwd. So 1.29 microseconds, versus 3.2 microseconds. Almost three times faster. If you're not familiar with microseconds and other units of measurements, here's a table that will help you understand it. We have milliseconds, which are abbreviated with ms and we have 1,000 of these in one second. After that we have microseconds which are abbreviated with mu, the Greek letter mu S, and there are 1,000,000 of these in a second. And then we have nanoseconds, abbreviated with ns, and there are 1,000,000,000 of these in a second. So pay attention to the unit of measurement timeit is showing you.

Tracing memory allocations
- Sometimes you'll hear people talking about memory leaks and memory efficient programs. How is memory connected to program runtime? There is data in a program. Numbers, lists, objects, et cetera. These are stored in the computer memory, known as the heap. Every time we create a new object, we need to allocate storage for it and this operation takes time. This is one reason for why we care about memory allocation. Another reason is that accessing memory in modern computers is done in layers. We have the CPU, and then we have L1 and L2 caches. Then we have the memory. And the access times are very different. Accessing layer 1 cache is about 0.5 ns, while accessing the main memory is 100 ns. Every time we try to access a piece of data, the CPU will first try to fetch from the cache, then from main memory. Difference, also known as latency, is huge. If we keep our data small, there's a good chance it will fit inside the cache line, and then it will be much faster to access. Sometimes algorithms that seem faster on paper are much slower than dumb cache friendly or sometimes known as cache oblivious algorithms. The last reason is that modern operating systems give us the appearance of having infinite memory. The computer I use right now has 16 GB of memory, but programs can use much more than that. This is done by swapping some section of memory into the hard drive. When we try to access the section memory that is currently swapped to disk, the operating system will pick another section of memory, will write to the disk, and then load the section memory we require into actual memory. This is also known as a page fault, and it's very costly operation. Hardware latency, even solid-state one, is order of magnitude slower than memory. All of this means we'd like to keep our memory small so we don't cause page faults. I hope you're convinced that you need to profile and monitor your memory as well. Tracemalloc is a library that was added in Python 3.4 as a tool to understand memory allocations. Let's look at an example. Assume we have our own encoding scheme for an event object. In reality you should use an existing encoding scheme like JSON, YAML, MessagePack or others. We are creating our own encoding just to demonstrate how to trace memory. Let's take a look at enc.py, which is found in the exercise file. The details are not that important, but in line 39 we have encode_event, which creates an encoder, and then calls the enc.encode method to encode the event. In main.py, we use tracemalloc. We import tracemalloc and we use a temporary file to encode the data. In line 50 to line 55, we create some sample data to test our code. And in line 59, we start the tracemalloc. In line 61 and 62, we encode all these events, and from line 64 onwards, we tell tracemalloc to print the statistics. Let's run this code. We're going to switch to the terminal, and run python enc.py. And we get some measurements about where the memory was allocated. If we look closely, we see that line 41 produces data. If we go back to our code, this is the line where we create a new object for encoding. One usual optimization to this kind of problems is to use an object pool. If you suspect that you have a memory issue in your program, tracemalloc is a good tool to have in your arsenal.

memory_profiler
- If you're running five from two and also need memory profile or need some memory user statistics, you can use memory profiler. Memory profiler is not in the stomach library. And we need to install it before we can use it. You want to compute the sum of differences of values in a list. The file is called sos.py and can find it in the exercise file. Once you have your function, you need to add the profile decorator before the function so memory profile will know to profile this function. At profile, and save the file. And now we switch the terminal and run python dash M memory profiler and our code sos.py. And we got some out. As we can see, line seven is the one that generates most of the memory. We can easily fix this by looking over intercedes and not over values, thus avoiding the location of vials to farming. When we install memory profiler, we also get amplify utility, which is time based profiling. Time based profiling will help us see if our memory continues to grow. We have the usual wavelike pattern of our location and garbage collection. Let's go back to our code. We remove the decorator and I'm going to increase the number from one million to 100 million. And save the file. Now we go back to the terminal and you run mprof run sos.py. Mprof generated a profile date. We can view this file after installing the Macbook library with mprof, plot, and the name of the file. And we get the graph and we see that our memory here continues to grow. This is just one function call so we should be fine. Usually you want to measure loops over time with mprof. Memory profiler also add a magic mp run command to python and others. Check the commentation for details.

### Picking the Right Data Structure

Big-O notation
- You'll often hear the term Big O in relation to performance. Big O Notation basically means in the worst-case, how long will our code run given a specific input? How long usually means the number of operation or, if you're looking at the memory, the size of memory. For example, let's say we'd like to know if a given number is inside a list. Ideally, it will be the first number on the list and we'll do one comparison. In the worst-case, the number won't be on the list and will iterate over the whole list to find out it's not there. Assuming the size of the list is N, we say the complexity is O of N. This is a very simple case and for more complex algorithms determining the complexity can be very difficult task. Lucky for us, someone else already figure out the complexity of most of the algorithms and data structures in use. For example, sorting is O of N times log N where log stands for logarithm. The nice folks at Python created a list on the complexity of various operations in the most common data structures. For example, appending to a list is O of one and sorting the list is O of N times log N. One question you should always ask is what does N mean? In our search example, it means the length of the list, however, if you're looking for a string in a list and strings there might be very long, N might be the length of the longest string in the list. And, sometimes, the complexity can depend on both the length of the list and the length of the longest string within it. Here a chart that shows how functions behave when N grows bigger. As we can see, 2 to power of N is growing much bigger than N squared. Big O ignores constants. For example, O of one means constant time. And, O of 1,000 is also considered to be O of one. Theoretically, this makes sense. But, in practice, if our code run a thousand times slower, it makes a difference. How does this help us? Knowing the complexity of operations is important for choosing the right data structure for the job. For example, if you'd like to know if an item has been seen, we'll pick a set or a digit which have O of one lookup over a list or a toppal which have O of N lookup time. If you know the theoretical limitation of an operation, we won't waste time trying to find a better solution that is better than this limit.

bisect
- Say you'd like to convert numerical scores to grades in the US system, where A is above 90, B is between 80 to 90, and so forth until the failed score below 60, which is F. Let's start with a simple implementation. I'll place this in a file called grades.py. As you see, we have a list called cutoffs and the names, and then a function on line six called grade, which just goes over the cutoffs, and once it find the grade, which is below the cutoff, returns the name. And we also have a test in line 17, which make sure that our code is working as expected. Once we have a test that passes, we can measure our code. We're going to use timeit module with ipython. So, we'll start ipython and we'll run the code grades.py. This will also run the test and since there are no errors, we're good. And now, let's time our code, timeit grade and let's say 74. This was 486 nanoseconds. If this time is acceptable, you should stop and work on something else. However, if you identified that this code is a bottleneck and we need to get faster, optimization is a good idea. Searching in a sorted list like cutoffs can be much faster. This is known as a binary search and its complexity is O of log N, which is much smaller than the O of N we do currently. And since we're using Python, we don't need to call this ourselves. We can use the bisect module. Bisect has a function called bisect that given a sorted list and a value, will return the index in the list where we should insert the value to keep the list sorted. Let's use it in the code. Before making changes, it's a good idea to commit the current point of code to our source control, Git, for example. I usually start with keeping the original code around and then creating a new one. This helps me compare runtime on different scenarios. So in our code now, we import bisect and I wrote a function called grade2, which just uses bisect. In line 24, I am asking bisect what is the index where we should insert this score, and I'm getting it from the names list. Also changed a little bit is the test. Now it gets the function to call and make sure that the function on the score is meeting the expectation. And now we run two tests, one for grade and one for grades2. Let's go back to our code. Don't forget to save. So, we're going to run grades again and since there's no errors, there's no output, which is good. And now, we can time our new function. Time grade2 of 74 and it's in 309 nanoseconds. We got about 33% speedup and as a bonus, shorter code. If you're not using ipython, you can use the timeit module from Python. Don't forget to divide the runtime you get by the number of loops timeit runs, which by default is one million.

```
  $ ipython
  $ %run grades.py
  $ %timeit grade(74)
  $ %timeit grade2(74)
```

deque
- Say we have requests coming in and we'd like to process them in order. For this we'll use a queue, also known as FIFO, first in first out data structure. Here's our initial implementation with a test. We'll place it in tasks.py. In line three I define a class called TaskQueue. In line six I use a list for the tasks, and in line nine, whenever someone pushes a task, I inserted it to the beginning of the list, and in line 12 when someone pops a task, we'll just tasks.pop, which would pop it from the end of the list, and then in line 18 I have a test for that to make sure that everything works. Let's run it. So ipython, and then run tasks.py, and the tests are passing since there is no output, which is a good start. Now let's measure. We're going to use our test function as the benchmark. So timeit test_queue with let's say 1000. So 1.3 milliseconds. If this bit is good enough, you should stop here. Python lists are optimized to work as a stack, also known as LIFO, last in first out. Items are added or removed from the right side. When changes are made to the left side of the list, the whole list is reallocated in memory. To solve this problem we have collections.deque, short for double ended queue, which behaves like a list but is optimized to work on both sides. The down sides of using deque is that item access time might be slower. Let's change our implementation to use deque. I'm going to leave the original code for comparison, and we'll replace the old code once I'm sure it's correct and faster. Here is the new code. I'm importing deque from collections model in line two, and then in line 18 I define DTaskQueue, which uses deque, and the main difference is that in line 21 we create a deque instead of a list. In line 24 we append, and in line 27 we do a popleft, which is a method of deque. I also changed the test. So now it get a class as the argument, and it test this class for correction. In the main part at line 46 and 47 I'm testing both classes. Don't forget to save the new code and let's run it again. So we'll do run tasks.py, and again there's no output, which mean the tests are passing, and now let's try again. We'll test the original queue, and now we'll benchmark the new queue. So we'll say that the class is the DTaskQueue. So 1.14 milliseconds versus 1.42 milliseconds. We got about 20% speed up.

```
  $ ipython
  In [1]: %run tasks.py
  In [2]: %timeit test_queue(1000)
  In [3]: %timeit test_queue(1000, cls=DTaskQueue)
```

heapq
- In some cases we'd like to order our tasks by priority. This is known as a priority queue. Our initial implementation will get a task and a priority, add them to the list, and then sort the list by priority. This way, when we pop the next task to execute, we'll get the one with the highest priority. Priority is given in decreasing numbers, meaning priority one is higher than three. If we look at the code, at line six we have the priority queue, and we create in line nine the tasks, which is a list. And every time we push some task with a priority, we create a topple with the priority and the task and then sort it in line 13. When we pop, we pop the first item on the list and get item number one from it which is the task to execute. In line 22 we add a test, the test's a priority queue. And we also have a benchmark. In line 49, we have a function called gen_cases, which generate test cases for us. And in line 61, we have a benchmark that gets the cases and executes the priority queue on them. And at the end, in the main, we run the test. Let's check our code. So we run ipython and then we run ptasks.py. It completed without an error, which means the test is passing. Now let's find a benchmark, so cases will be gen_cases, let's say 1000 test cases. And this is done to avoid measuring the time of generating the test cases in the benchmark. And now we will want to time it benchmark_pq with our cases. About 2.73 milliseconds. Let's see what's taking most of the time. We'll use the prun major command, which runs our code under profile benchmark_pq with our cases. We see that the list sorting we do which is off and login, is taking most of the time. We can do better with a heap, which is a data structure optimized for this and others. The complexity of pushing and popping from a heap is o off login. In Python, you'll find a heap implementation in the heap queue model. Let's change our code. So in line three, we are importing heap, pop and heap push from heap queue. In line 22, we define age priority queue which is a heap priority queue. It chooses a heap, and in line 28, when we push, we do a heap push. In line 31 we did a heap pop. We also change the test a bit to get the cls to test as an argument. And the same for the benchmark, we also change the benchmark to get the cls to benchmark. And in the main we are running both tests, one for the initial queue and the second one with the one from here. Don't forget to save your code. And now let's run it again. It completed without errors which is good. And now we can benchmark, so let's time the old code. Time it, benchmark_pq with cases. Okay, now the new code, time it benchmark_pq with cases and the class is the high priority queue. If you look closely, you see that the units now are in microseconds, so this is much faster than the initial implementation. About four time faster, to be exact. Every time you need to pick the highest priority object, think about heap.

```
  $ ipython
  In [1]: %run ptasks.py
  In [2]: cases = gen_cases(1000)
  In [3]: %timeit benchmark_pq(cases)
  In [4]: %prun benchmark_pq(cases)
  In [5]: %prun benchmark_pq(cases, cls=HPriorityQueue)
```

Beyond the standard library
- As the saying goes, "If all you have "is a hammer, everything looks like a nail." We must learn and enhance our toolbox so we can solve problems with the right tool every time. There are many algorithms and I don't expect you to remember them all, I know I don't. But knowing that there are a lot out there is a good start. And between Google, Stack Overflow, and other forums, there are plenty of resources available to find what you need. The Python standard library can't contain implementation of every algorithm or data structure out there. But there are many third-party packages that you can use. Let's explore one case. Say we are starting a competitor to Uber or Lyft. When a user requests a ride we need to find the closest available car and send it their way. Looking at our code, we have at line seven a distance function that calculates a distance between two points. And then at line 14 we have the "find closest" function, that gets the location and list of drivers and return their closest driver to this location. And then in line 22 we have the benchmarking code, which generates the list of drivers from a given location. In line 28 we have a test for testing our code. And in line 46 we run this test in the main function. Let's try it out. So ipython. And then run drivers.py. And since there is no output it means that the tests are passing. Let's take a point, which happens to be the location of the Lynda office. And we'll generate some drivers. So drivers = gen_drivers from this location. And now let's time. So timeit find_closest with latitude and longitude and the list of drivers. So about 478 microseconds. You can profile it to see what's taking most of the time. So instead of the time it magic, we are going to use the prun magic, which runs the code under profiler. We call this stance for every driver to find out the closest one. There is a data structure called, KDTree. You set a tree with locations. And then looking for ones close to a given point is of login. Looking outside the standard library for packages I highly recommend picking ones that are well established. You can learn about the status of a package by looking at GitHub stars, latest commit, number of developers, etc. You should also ask around. We're going to use the KDTree implementation in psy pi, which is over 16 years old and used by thousands of developers. You can install psy pi with Peep or Condo. Once installed, we can start using it. This is going to be the new code. So in our new code at line five, we import KDTree from psy pi.special. And then at line 23 we have find closest KD, which gets a location entry and returns the closest one from the tree. We'll also change our test function to accommodate for testing the KDTree. And in the main in line 57 and 58, we do test both for the old class and the new class. Don't forget to save your code. And let's try it out. So we run the drivers and it completed without errors, which is good. And now, let's benchmark. So let's run the benchmark for the old class first. Another I'm going to rerun the (mumbles) and then we'll continue from there. Run drivers. Lat and Ing. Drivers = gen_drivers. And then we time it. And then the prun. And now running drivers again. And now we're at the point where I want it to be. So first, we're going to benchmark the old code. So we want to time it on defined classes. And next, we're going to benchmark our new code, which needs a tree. So we'll create a tree. Tree = KDTree of drivers. And then timeit find_closest_kd of our latitude and longitude and the tree itself. So to calculate, let's see. So 485 divided by 80.8. So we're about six times faster by switching to KDTree. Every time you face a new problem, do your homework and find the right data structure to solve it.

```
  $ ipython
  In [1]: %run drivers.py
  In [2]: lat, lng = 34.3852712, -119.487444
  In [3]: drivers = gen_drivers(lat, lng)
  In [4]: %timeit find_closest((lat, lng), drivers)
  In [5]: %prun find_closest((lat, lng), drivers)
  In [6]: tree = KDTree(drivers)
  In [7]: %prun find_closest_kd((lat, lng), tree)
```

### Tricks of the Trade

Local caching of names
- Knowing how Python operates will give you more tools for optimization. Let's take a look at some language features and tools that might give your code a boost. Let's say we want to normalize a list of numbers. So, we have a configuration at line three for the factor and the threshold. And then in line nine, we have a normalize. We'd go over the numbers and if they're above the threshold, we divide them by the factor. And then return. In line 23, we just regenerate a list of numbers, so we can test. Let's run the code. So, ipython and then run norm.py. Let's time it, timeit normalize of numbers. So, 198 microsecond. At first, it doesn't look like there's much delay but let's look at how Python compiles this function. So, we import the dis module and we do dis.dis of normalize. We see that twice we have LOAD_GLOBAL and after it, LOAD_ATTR. So, Python is looking for the name config in the global and then looking for the attribute threshold inside. The same here, we have LOAD_GLOBAL for config, load attribute for the factor. Python does this lookup on every duration of the loop. Attribute lookup is fast, but it's still a dictionary lookup. And since config is global, we also look for its name in the global's dictionary, which is another lookup. Let's save these values at the beginning of the function. So here we have normalize2. normalize2, at line 21 and 22, we save locally the threshold and the factor, and then run. Let's see how it's affect. Don't forget to save, and then let's run again. And let's first disassemble to see what we have. So, dis.dis of normalize2 and we see that we do the LOAD_GLOBAL and the LOAD_ATTR outside of the loop, which starts here. And let's measure our old code and the new code. And we can do 120 divided by 184. So, it's about 35% speedup. This optimization comes at the cost of some code complexity, so consider if the trade-off is worthy. If we look at disassembly, we still do one more lookup, which is for norm.append. We can get rid of it as well. So here we have normalize3, which in line 38 we also cache the norm.append, so we don't have to look it up. Let's save the code and run it again. And let's time it. So, this is normalize3 now and it's about half the time of the original implementation. So, attribute caching can help you with optimization.

```
  $ ipython
  In [1]: %run norm.py
  In [2]: %timeit normalize(numbers)
  In [3]: import dis
  In [4]: dis.dis(normalize)
  In [5]: %run norm.py
  In [6]: %timeit normalize2(numbers)
  In [7]: %run norm.py
  In [8]: %timeit normalize3(numbers)
```

Remove function calls
- Functions are a great way to organize your code. You get modularity, re-usability, readability, and more. You might have seen this acronym, T-A-N-S-T-A-A-F-L. It means there ain't no such thing as a free lunch. There is a cost to functions, and it's the overhead of calling them. How much time are we talking about? Let's see. So ipython, we'll define an empty function that does nothing, and we'll use timeit to see how much time it takes to call it. So 66.3 nanoseconds, which is pretty fast, but still considered a lot for a function call. Here's an example. We'd like to fix a list of numbers and make the even numbers absolute. So we define a function for abs_even in line four, and then in line 17 we just do a list comprehension to create this number. And in line 22 I'm creating such a list of numbers so we can time it. Let's benchmark. So run fncall.py, and then we did timeit on fix_nums of numbers. Please note that the numbers might be a little bit different when you edit the code. And this is the new code. And now let's run it. So, we're going to run again fncall.py, and let's time this time the inline inversion. Right, then you can do 95.3 divided by 148. So we get about 35% speed up. This comes at a great cost to program structure and make code harder to test. However, sometimes to squeeze more speed from your program, you do inline function. Sometimes function calls are hidden from us. This is mostly the case of properties. I see people use properties all over the place without realizing the cost. In Python we use properties only if you have a good reason and not by default like with Java or C++ where you add getter and setter to everything. Here is a small example. In this case I have two classes. One is a simple point with X and Y, and one, which I called PPoint, which have properties for the X and the Y for setter and get. Let's time this code. So run prop.py, and then I'm creating a regular point, and I'm doing timeit of accessing the X. Now I'm going to create another point and this time one with the properties class, and, again, timeit p2.x. This is about four times slower. Try to use properties only if you have a good reason and be conscious of the performance penalties.

```
  $ ipython
  In [1]: def noop():
  ...:        pass
  ...:
  In [2]: %timeit noop()
  In [3]: %run fncall.py
  In [4]: %timeit fix_nums(numbers)
  In [5]: %run fncall.py
  In [6]: %timeit fix_nums_inline(numbers)
  In [7]: %run prop.py
  In [8]: p1 = Point(1, 2)
  In [9]: %timeit p1.x
  In [10]: p2 = PPoint(1, 2)
  In [11]: %timeit p2.x
```

Using __slots__
- There are cases, when we create a lot of small objects. In most cases, this is fine, but it might be an issue, when the number of objects becomes very big. Python Objects store attributes in a dictionary called __dict__. Or dunder dict for short. Dictionaries in Python are optimized for access speed, and most of the time are 1/3 empty. Carrying a 1/3 empty dictionary per instance, can be a big overhead sometimes. For just these cases, Python offers us dunder slots, which remove the dunder dict. The downside of using dunder slots, is that you can't add new attributes to objects. Let's have a look at our code, in slots.py We have a class Point, which is the usual class. And we have the class SPoint, which, the only difference, is that we define, in line 17 dunder slots. In the main, we define two functions to allocate points. Once, in line 29, we allocate regular points, and in line 32, we allocate SPoints, with the dunder slots. And in line 35 to 37, we allocate the points themselves. Let's see how much memory are we talking about, ipython, and then we %run slots.py And then, let's check the memory. So we import the sys model, and sys.getsizeof(points) And then, we look at the slot points. So, sys.getsizeof(SPoints) Looks like there is no improvement, but once we remember that the list holds references to objects, this makes sense. These are lists of one million pointers. And these pointers are of the same size. Understanding how much memory an object consumes in Python can be tricky. A lot of objects hold reference or pointers to other objects. There are objects shared by several other objects, and other considerations. I'm going to use Memory Profiler, to see how much memory we allocate. You can install Memory Profiler using Pip or Conda. Once you have Memory Profiler installed, you can start using it by %load_ext and memory_profiler. And now we're going to run, using the the mprun magic command. %mprun -f alloc_points so it will profile the alloc points. And we call alloc_points with that number that we gave before, which is one million. If you look at the Increment column, we see that we allocated around 190 megabytes. And now we'll do the same for the SPoints. So the alloc_spoints and alloc_spoints. And run this one. We're down to 85.2 megabytes. This is less than half of the memory, by adding one line of code. Smaller objects might also make your program faster, since they can fit in a CPU cache line, which means that when the CPU tries to access them, it will be much faster than fetching them from main memory.

```
  $ ipython
  In [1]: %run slots.py
  In [2]: import sys
  In [3]: sys.getsizeof(points)
  In [4]: sys.getsizeof(spoints)
  In [5]: %load_ext memory_profiler
  In [6]: %mprun -f alloc_points alloc_points(n)
  In [7]: %mprun -f alloc_spoints alloc_spoints(n)
```

Built-ins
- Let's say that we have a list of tasks. Each task is a tuple, where the first field is the task name and the second field is the priority. You can see it in our code in builtin.py at line 14. We'd like to solve the tasks by priority. In Python, tuples are sorted in lexicographical order, which means if we'll sort as-is, we'll get the tasks sorted by their name. Tasks names are strings, and the string '3' is bigger than string "200". Let's see an example. "iphython" and when we do three bigger than 200, we get True. Python provides a "key" argument to sort. Key is a function that gets the object we'd like to compare and returns a value representing this object. For example, if we have names equal "tweety" and "bugs" and "daffy" and "elmer", if we'll do sorted of names, we'll get them sorted by lexicographical order. However, we can do sorted names and the key equal len, and then we'll get the names sorted by their length. We can use that to sort our tasks. In line 5 we define a function called sort_tasks, which the key function is a lambda, an anonymous function that gets a task and return the second element which is the priority of the task. Let's run it. So let's time it using the timeit magic. timeit sort_tasks of tasks, so 286 microseconds. How can we do better? Our lambda function is called once per object, and yes, it's once per object, not n log n, since Python caches the results. Since sorting by different fields is very common, Python provides functions that do that but are written in C and will be a bit faster. Let's see. Here we have the new code. We import itemgetter from the model operator in line 3. And in line 10, I define the new function that uses this itemgetter. Let's save it and see the difference. So, we do run of builtin.py and this time we're going to check our new function, sort_tasks_ig, and 252 microseconds. So we got about 10% speedup and less code. Python also provides attrgetter that gets the attribute to return and there are many functions Python already provides which are written in C, such as sum, max and others. If you'd some time learning what the standard library offers and try to avoid writing your own functions.

```
  $ ipython
  In [1]: %run builtin.py
  In [2]: %timeit sort_tasks(tasks)
  In [3]: %run builtin.py
  In [4]: %timeit sort_tasks_ig(tasks)
```

Allocate
- Let's say we have a pool of workers and we'd like to know how many tasks each worker has done. We can start with a list in the length of the workers, filled with zeros, and have each worker increment their value in their index. This allocation can happen many time if you frequently allocate the pool of workers. Let's write a function to create this list. The code is in alloc.py. So in line four, we define allocz which creates a list filled with zero for a given size. And let's time it, ipython and then we run alloc.py. And let's time it of 1000. How can we do better? In Python, if you multiply a list with a number, we get n copies of this list. For example, if I take the list of one, two, and three and I multiply it by three, I'll get this list three times. Let's use this in our code. In a new code, I wrote a function called allocz fixed, which returns a list of one element with zero times the size. Don't forget to save the code and let's try it out. So we run again the code to load it, and then we're going to time the new function. Okay, this is about 10 times faster than the previous code. When you create a list and append to it, Python needs to reallocate memory for the list. To avoid reallocation on every append, the list grows in fixed sizes. The sizes are zero, four, eight, 16, 25, etc. However, when we create a list with multiplication, Python knows the size of the list in advance and creates it in one allocation. Note that the initial value is duplicated, and in Python because everything is a reference, this might have a surprising effect. If you have an array, which is the empty list times five. And then we appending to the first list in the array the number one, and we look at the array we got the number one in all of them. Make sure you initialized with immutable volumes, like numbers, strings, and tables, otherwise you will have to get back to our initial implementation. Another option is to use numpy, which provides ultra fast arrays. So, we import numpy as np and time it np.zeros of 1000. So we got 968 nanoseconds, versus 2.24 microseconds in the allocz fixed, so it's about two and a half times faster. For faster memory allocation, consider using fixed size allocation or numpy.

```
  $ ipython
  In [1]: %run alloc.py
  In [2]: %timeit allocz(1000)
  In [3]: %run alloc.py
  In [4]: %timeit allocz_fixed(1000)
  In [5]: import numpy as np
  In [6]: %timeit np.zeros(1000)
```

### Caching

Overview
- Caching is the act of saving computation results and reusing them instead of running the calculations again. It is very common practice and will speed up your code in many scenarios. The price you pay is the memory used to hold the computed values. This runtime versus memory trade off is very common in computer science. Let's see an example. The Fibonacci sequence is defined as the first two values of one and every value after it is the sum of the two values before it. In fib.py, we have an implementation which is a cause. If the number in line six is smaller than two, we return one otherwise we return Fibonacci of N minus one plus Fibonacci of N minus two. Let's time it. So, ipython and then we run fib.py. And then, let's run timeit of fib of 20. So, it's 2.82 milliseconds. This implementation is slow since we do a lot of repeated calculation. When we compute Fibonacci of four, we'll call Fibonacci of one three times, Fibonacci of two two times, et cetera, et cetera. By the way, don't try to run Fibonacci of 100. You'll wait a long time. Let's cache the results. In the new code, in line 11, we have Fibonacci cache which is dictionary holding the computed values. And then, we have the function fibc in line 14. And, in line 19, we try to see if we have the value already in the cache. And only if it's not in the cache, we compute it and place it in the cache. Don't forget to save your code. And, let's try it out. So, we run again the Fibonacci dot py and this time, we're going to time the fibc function, which is much faster. Note that these are 240 nanoseconds which is 2.18 milliseconds. So, it's about a thousand time faster. Our naive cache implementation will grow indefinitely which might become a problem. Also, if our functions are not pure, meaning they might return different values at different times. For example, when we ask the database how many users there were in the last 24 hours. We need to update the cache. There are methods of evicting or invalidating values from the cache. They can get tricky. There's a saying that the two hardest problems in computer science are naming things and caching validation. And, I find it to be true. Depending on your data, you need to pick a good cache eviction strategy. There's a lot of research out there on the subject. Do your homework.

```
  $ ipython
  In [1]: %run fib.py
  In [2]: %timeit fib(20)
  In [3]: %run fib.py
  In [4]: %timeit fibc(20)
```

Pre-calculating
- We'd like to count how many beats in a number for set one. This is sometimes known as pop count for population count. Knowing the number of set base is useful in several algorithms such as hemmingway. Here's an implementation. In line four, we have nbits, which does the wire loop on the number and checks how many beats are there and in line 16, we define a test for that, which checks the number of beats. And in the main in line 28, we call this test. Let's try it out. So ipython. And then we run nbits.py. And since we don't have an error, we're good to go. And now we can time. Time it nbits of, let's say, 350. It's 1.07 microseconds. How can we speed this up? The test might be less than ideal. We can precalculate the value for each number once and then we turn these values. This works well with the numbers are bounded by size. Let's assume that our numbers can have a maximum of 16 beats. What's the largest value you can get with 16 beats? So we can use python for that. You do int of one times 16, so we'll get a string of 16 ones. And we tell python that this is in phase two. So this is the largest number you can get for 16 beats. Now, we can initialize an array with a number of beats and return them. We'll also update our test. Here's the new code. We update the test to get the function and check that the function returns the right value. In line 32 we define a cache. We run the original function for all 16 beats now. And then in line 35, the nbits fixed, just look at the number in this cache. And we edit the test for this one as well in line 44. Don't forget to save your code before trying it out. So we'll do a run of nbits.py and it finished without an error, which means that the test is passing. And now we can time it. So this time, we're going to time nbits fixed. So 147 nanoseconds versus one microsecond. Much faster. We traded memory for speed. This will work if the function we precalculate is pure, meaning it will return the same value for the same input every time.

lru_cache
- They have a slight API and as a good security practice demand we don't store keys in plain text but encrypt them. This means that every time someone makes an API call we need to encrypt the key and check if it matches the one we have in our key database. Here's the initial code. We use the crypt method from the crypt library to encrypt keys and we have a database in memory of users and function user from key in line 16 takes a key, encrypts it and get the user from the user database. Let's check the run time. So ipython and we run LRU and we use time it to get user from key of bugs. And it's 3.37 microseconds. This is an excellent candidate for caching. The problem is that if we have many users and maybe many fraud attempts our cache can get rid of it. Once the cache grows over the size of physical memory it will start swapping to this and performance will degrade. We'd like to limit the size of the cache and the built-in LRU cache can do just that. LRU cache is Python 3 and if you're using Python 2 you can pip install it. As the name says the eviction algorithm used in LRU cache is LRU, lease recently used. When the cache is full and we need to store in a new key we'll evict the key that was least recently used. There are many other cache replacement algorithms, least frequently used, most frequently used, random and others. Picking the right algorithm can have a big effect on code performance and you should know your data in order to pick the right one. LRU is very widely-used and is considered a good default policy. Let's change our code. So in a new code we import LRU cache from functools in line four and we use the LRU cache decorator with a maximum size of 1,024 entries on line 17. Let's save the code and see how it affects the run time. So run lru.py and run again our function. So these are 96.4 nanoseconds versus 3.37 microseconds. Warning, LRU cache uses all the function arguments as keys to the cache. It will fail if one or more of the argument is not possible, say a list, a dig and others. Sometimes in order to be able to use LRU cache you need to modify your functions and make the arguments harshly. Keep LRU cache in your memory. It's an easy way to add caching to your code.

Joblib
- There are time that we like to keep the computation cache between program runs. For example, when I do some data analysis, it's common to have several stages that takes a long time. There are cases when I need to stop and move on to another task, and when I get back, I'd like to avoid running these low computations again. There are cache services like Redis and memcached, but I'd like to avoid installing servers and configuring them. Also, both of these services will lose all data if they are restarted. Let's look at the simplest solution. Joblib is a third party package that offers two things, on disk caching and parallel computing. We're going to look at the former now. You can install joblib with pip or Condor. Let's write a small spellchecker. We're going to compare a word to a list of dictionary words. For distance between the word, we're going to use Levenshtein distance, also known as the edit distance. It counts how many transformations, add, delete, change, are required to get from the original word, the destination word. I encourage you to go and read about Levenshtein distance and see the nice dynamic programming technique used to implement it. Since this is Python, we'll use the Python Levenshtein package, which you can pip or Condor install. Here's the file called spell.py. In line three we import Memory from joblib, and in line eight we define a Memory object, which will be our caching on disk. We define two functions in line 12 and line 18, and both of them are cached using the memory.cache decorator. In line 23 onward we have the main implementation. We are using argparse to parse the arguments passed to the function, and then in line 42 to 43 we look at the spell words that the speller is suggesting and print them out. Let's run it this time from the command line. On Mac or Linux we have a built in time command that runs a program and reports how much time it took. So we'll do time python spell.py, and let's give it a word that is not in the dictionary. And this time it tooks almost a second to run. Now let's run it a second time. This time everything is cached so it should be faster, and this time we got it in 0.17 of a second. 0.1 second is that magic number that makes human perceive something happening in an instant. It's a great goal to reach. If our dictionary changes, we might return wrong results. Joblib supports clearing the cache. If we go back to our code, we see that we added an option in line 30 and 31 to clear the cache. In line 35 we check if the user asked us to clear the cache. We do memory.clear, which clears the cache from the list. Let's try it out. Python spell.py --clear-cache, and now when we run it again, we see against that we get the 0.87 seconds. If you need to store cache results between runs, joblib is a great library to use.

### Cheating

When approximation is good enough
- Here is a riddle for you. How much is 1.1 times 1.1? Let's see what Python says. So we'll do 1.1 times 1.1. I find the bargain python posts from time to time. A lot of them are similar to what you just saw. Usually applies to RDM failing. Which stands for redefine manual. We send them to read about floating point implementation. It is not the bargain python, if you run the same code in C, Java, Go and other languages, you see the same results. The reason is a design creative. To make floating point calculations fast, some degree of accuracy is sacrificed. Look at the answer, the error is very small. But still, it is an error. As a side note, every time you do testing that involves checking results of floating point numbers, you should allow for some error. Use the built-in round function with the number of digits you think is an acceptable error. What I'm trying to show you that even in something as mainstream as floating points, using approximation or we cheat. The world is okay with it. Which means we can do the same in our code, to split it up from time to time. It should be okay as well.

Cheating example
- Let's say you're competing with Amazon and use robot walkers with an algorithm to pack shipments into a minimum number of boxes. We're going to simplify the problem and look only at volume. We'll get a list of items, each with its own volume, and the volume of the bin, called bin size, and return the optimal solution with the minimum number of bins that can hold all of these items. Let's see a solution to this problem. This is a recursive algorithm that tries all options and return the best one. So at line four, we have bin pack of items bin size and a list of bins, and we initialize the list of bins to be empty if it's the first call. And then we recursively try all the combinations and at line 23, we return the minimal with the key of len. And we want the minimal number of bins. Let's save this code and try it out. So ipython. And run pack of py. So we do a bin pack of the items with volumes one, two, and three, in bins of size four. And we pack two items in one bin and the third item in the next one. Can try it again, let's try a bigger list. This time with five items and we get the packing, which is optimal. All the bins are full except the last one. And let's time it now. So I'm going to use time just for one function code. We'll do bin pack of one, two, and three times 10 so it's 30 items in bins of size of four. This is almost one and a half seconds for 30 items. That's not good. This problem is known as bin packing or the nap sack problem. These problems are what's called empty heart problems. We assume they cannot be solved in the polynomial type. Looking at the complexity chart, we see that two to the power of n is rising much, much higher than n to the power of two. The bin packing algorithm has been studied a lot, and there are many approximation algorithms for it. Let's try a greedy first fit algorithm. If an item fits in a bin, we continue from there and don't try more solutions. So here is our code with greedy bin pack. We go over every item and if it fits in the bin, we edit for the current bin. And if not, we create a new bin for it. And we return the number of bins. Let's save this code and try it out. So run pack.py. And now we can do a greedy bin pack of one, two, and three in bins of four, and we get something which is close to what we did before. Next two out number. However, if you len of greedy bin pack of one, two, and three times three in bins of four, we'll get six bins. Now let's compare it to the optimal solution. The len of bin pack of one, two, and three, and it's five. So we have one box more. And let's time. So time equal greedy bin pack of one, two, and three times ten and four. And this is 92.7 microseconds, which is roughly a millisecond, which means comparing to our run time of 1.3 seconds, we are about 1500 times faster. It was shown that in the worst case, the first fit solution returns twice as many boxes as the optimal solution. As the business owner, it's your call if this extra speed is worth the trade off for extra boxes. Think of places in your code where you can return an approximation. It might speed it up a lot.

### Parallel Computing

Amdahl's Law
- Modern computers have more than one core and can execute several computations in parallel. Currently, my machine is running more than 200 processes on four cores. The operating system gives each process a slice of time to run on a specific core and then suspends that process. This gives the illusion of parallelism and is known as concurrency. If you want to learn more, I highly recommend watching Rob Pike's excellent talk: Concurrency is not Parallelism. Spreading our computation over several cores or even letting some computation happen while other units of work are waiting can significantly speed up our code. An example of waiting will be database returning results. Before you start parallelizing all your code, take some time to learn about the theory behind it. One very useful piece of knowledge is Amdahl's law, which gives the theoretical limit on speedup or latency you can get from going parallel. For example, if you have a program that runs in 10 minutes and there's a one-minute section that can't be parallelized, then our program will run for at least one minute. This means we can get up to 10x faster if we go parallel. Another piece of information you need to know is where your code is spending most of its time. We roughly split programs into two categories, I/O-bound and CPU-bound. I/O-bound programs spend most of their time waiting on I/O, input/output, such as network or disk. CPU-bound programs spend most of their time doing calculations. Python's standard library offers three options for concurrency or parallelism. These are threads, processes, and asyncio, asynchronous input/output. Each have their own characteristics and you should know when to use with tool. Threads are an independent unit of execution that share the same memory space. This means that global variables are visible to all threads. Since most of Python's data structures are not thread-safe, this means that if two or more threads will access the same data structure, say, append to a list, the behavior is undefined. Threads are great for I/O-bound programs but not so much for CPU-bound. One reason is that CPython can use only one core from the CPU. This is due to a global interpreter lock, known as the GIL. There have been many attempts to remove the GIL, so far without much success. Processes are independent unit of execution. Each process have its own memory space, which is not shared with other processes. Using multiple processes means you can use all the cores on your machine, which is great news for CPU-bound programs. Sadly, this comes with a price. Since processes don't share memory, the communication between them is expensive. You need to serialize your data, send it over a socket or pipe, whatever, and deserialize it on the other side. Asyncio, asynchronous input/output, is a way to handle a lot of connections. Since each thread or processor require some resources, spending a lot of them could be a problem. You definitely don't want to spend 10,000 threads or processes on your machine. Asyncio deals with that many connections using a single thread that listens to all connections. Once a connection is ready, it wakes the core that deals with this connection and run it until it needs to read or write again. Asyncio is not unique to Python. This is how NGINX, Node.js, and other frameworks work as well. To summarize, use threads if you have mostly I/O-bound applications and you use traditional networking or database drivers. Use processes if you have mostly CPU-bound applications and don't pass a lot of information between processes. Use asyncio if you need to support many concurrent connections and have async drivers to other services.

Threads
- Since threads share the same memory, we need to be careful when accessing data structures from several threads. Python's data structures are not thread safe. You might think that's an odd design decision, however, most Python programs, even these days, run in a single thread. And there's a big performance penalty in making every data structure in Python thread safe. Getting rocks right is very tricky. Newer Python versions comes with concurrent.futures which simplifies working with threads. Say we'd like to get some information about GitHub users. We have their log in name and would like to know their real name and when they joined GitHub. GitHub provides a rest API we can query. Looking at our code in thr_pool.py, we have, in line 11, a function that gets the user info from the GitHub API. And, in line 20, we have function that gets the user information for several people in one go. In line 25, we define several people that we'd like to query about. Let's try it out. So, ipython and we run the thread pool. And then, we going to use the time manage, so time and then, users info on all the logins. So, this took about 1.18 seconds. When the CPU time is very small compared to the wall time, it usually means we waiting on IO a lot. We can run our code with a Python profiler to check. So, the magic is prun. We say dash L 20, so just 20 lines. And then, users info of logins. We see that, by far, we spend most of our time on IO doing socket operations. Which means using threads will probably speed it up significantly. Let's go back to our code. Here's the new version of our code. In line 4, I import thread pool executer from concurrent futures. And, in line 26, I have a function users info_thr which uses a thread pool executer to run all the requests concurrently. Don't forget to save. And, let's try it out. So, we do run thr pool.py to reload the code. And then, we'll do time users info and this time the threaded version of logins. And we get it back in 314 milliseconds. Thread pool executer is very flexible and you can use it in many ways, not just by calling map. Spend some time reading the documentation.

Processes
- Using processes allows us to utilize all the cores in our machine. The cost of pay is in communication. When we'd like to send data to a process we need to serialize the data, send it over a socket, a pipe, or any other communication method, and then deserialize it on the hand. This is known as IPC, or inter-process communication. Say we store data compressed with a BZ2 algorithm. When user ask for the data, we decompress it and send it to the user. Here's our code in proc_pool.py. We have a function called unpack, gets a list of requests, and return a new list with the decompressed requests inside. In line 10 through 18 we create some compressed data from Huckleberry Finn book, and in line 15 we print out the compression rate, and in line 16 we show that BZ2 is a loseless decompression meaning when we decompress, we get exactly exactly the same data as the uncompressed version. Let's try it out, ipython, and then we run the proc_pool, and we see that the compression rate is about 27%, and we got back the same data that we compressed. I'm going to time it, and since the output is very long, I'm going to assign the output to the underscore variable. So time underscore equal unpack of requests. 6.79 seconds. When the CPU time and the wall time are about the same it usually means the code spends most of its time doing computation. Let's see where we spend the time. So we can use the prun magic command -l and 20. We say we want only 20 line, and unpack of requests. And we see that most of the time is spent in decompression. Now let's try in multiprocessing. So here is our new code. In line two we input ProcessPoolExecutor from concurrent.futures, and in line 11 we have unpack_proc requests, which uses a ProcessPoolExecutor to decompress all the compressed data. Don't forget to save. Let's run the code again, and now let's time our new function. So time, and again I'm using underscore equal to this card output, unpack_proc of requests. So 2.5 seconds. We're almost three times faster now. ProcessPoolExecutor is very flexible, and you can use it in many ways, not just by calling map. Spend some time reading the documentation.

asyncio
- Both thread and processes are great tools for pryo computing. However, there are cases when we need a lot of work units. And both threads and processes are too much. Spinning thousands of threads or process on a single machine will usually bring it to its knees. If you're I/O-bound, we also have the option of asyncio, which stands for asynchronous input/output. To understand how asyncio works, we first need to understand the difference between concurrency and parallelism. To steal from World Pike, concurrency is about dealing with a lot of things at once. Parallelism is about doing a lot of things at once. For example, in the old days I had on call on one CPU. And still managed to run several programs at once. The processor gave each process some times to run and then switched to another process. Asyncio works in a similar way, however, instead of stopping each work unit, known as a task, at a time interval the work units themselves tell the scheduler, known as the event loop, when they are going to be blocked on i/o. and then the scheduler gives some other task time to run. This is also known as cooperative multitasking. Asyncio is not unique to Python. Nginx, Node.js, and other frameworks are using the same paradigm successfully. In Python, there have been several such libraries, such as twisted in which the initial implementation of bitterant was written on. The tornado web server, GEvent, and others. In recent versions of Python, we got the asyncio module. And even some changes to the language support this paradigm. Let's see a small example. Assume we'd like to get information about GitHub users from the GitHub rest API. Here's the initial implementation. Gh_user.py, which uses the traditional urlopen library. In line 11 we have a function that calls the rest API via urlopen. And line 20, we get information from server users. In line 25 onward, we define some logins we can query. And let's try it out. We run iPython. We run gh_user.py. And then we time it using the time engine command, time. And I'm going to use _= to discard the output users_info of logins. This is 1.38 seconds. Now we'll write the same code using asyncio. Since we don't have a built in asyncio http client in the start library, we'll do our own http requests. So here's the code. In line four we import the asyncio library. And in line eight we define our http request template. In line 20, we define userinfo_aio. And note that we do async def, which means this is a coroutine. It gets a login and accumulator. In line 22 it opens a connection. And it does an await. This lets the schedule know that it can give it time to another task while connection is being open. In line 29 we do an async for. An async for will wait until there's a line ready to read from the circuit and only then continue with the follow. In line 36 to 38, we pass the Json. And then at line 41 we add the the current user's information to that cumulator list. In line 44 we define a function usersinfor_aio that gets several logins. We define a auxiliary task called make task that creates a new future. In line 48 we have a utility function to make an ansyncio task. In line 51 we create a list of these tasks. In line 52 we get the event loop. In line 53 we tell the event loop to run until everything is completed. Line 57 onward, we define some logins to check. And let's try this one. So we do run gh_user_aio.py. And then we time it. And again, using underscore to discard the output users_infor_aio of logins. So 332 milliseconds. If we do 1.38 seconds divide by .332, we see that we have out 4.15 times faster. You need to be careful with asynchio. Since everything is running in the same thread if a task blocks for some reason, everything else is blocked as well. For example, if you access a database you need to find an async version of the driver since the regular driver is probably working in a blocking manner. Asyncio has an option to shut off potentially blocking or just (mumbles) the threads are processing. Asyncio is a new and exciting library. And there are already many tools and libraries built around it. However, if you need a more mature solution with many more features, have a look at Twisted.

### Beyond Python

NumPy
- If you're doing a lot of numerical computation, you should look into NumPy. And my guess is that you already have. At its heart, NumPy provide a matrix data type called an array and a set of mathematical functions. All of these are written in super-optimized C. NumPy is the base for most of python scientific stack. Packages such as Pandas, psychic learn and others heavily use NumPy under the hood. NumPy uses machine-level numbers. Which are different from Python numbers. Python numbers can grow as much as you want. Let's have a look. Ipython. Let's take two to the power of 1,000. This is really long number that doesn't fit in a machine integer. However, import NumPy as np. And then we'll do np.int64 of two to the power of 1,000. And we'll get zero. This is an overflow. In most practical cases this overflow is not a problem, but you should be aware of it. Remember to write test cases before you start optimizing and run them after optimizing to make sure your code is still correct. Let's say we'd like to multiply two vectors. In python, we'll do the following. We have the function vmul for vector multiply in line four. And we'll do an element wise multiplication. Let's time. So run np.py. Let's create a big vector. Vec equals list of range of 1 million. And then we'll do a time it on vmul of the vector by itself. This is 83.4 milliseconds. Now let's see how NumPy is doing. We don't need to write any code since the multiplication operator is defined an umpire raise to do just that. So we'll create a second vector which is np.array of our original data. And now, we can do timeit vec1 times vec1. This is 902 and notice the units. These are microseconds. So we can do 83.4 divide by .9. It is about 92 times faster. Apart from NumPy arrays, we also have mathematical functions. NumPy function work with both scale and arrays. This type of functions is called u funcs, short for universal functions. Let's look at an example. We'll calculate the sin of every element in a vector. So from math import sin. And then timeit sin of v for v in vector. So 169 milliseconds. And now for NumPy. So, timeit np.sin of our vector. And 16.2 milliseconds. About 10 times faster. If you're doing a lot of mathematical computations, NumPy is well worth your time. There are many ways to do a lot of complex computations very fast, but you need to invest time learning. You can check out my course, Data Science Foundations to learn more about NumPy in the python scientific stack.

Numba
- Python is a dynamic interpreted language. This gives us a lot of power and flexibility. Going from zero to working in Python is very fast. However, being that dynamic means it's hard to do optimization at the language level. Languages that have type information, from C, C++, to Haskell, can use this information to gain a lot of performance. However, not all is lost. A technology called JIT, Just-In-Time compiling, has been used very successfully in several dynamic languages to gain performance. The idea behind Just-In-Time is that we collect information on the code at run time and according to this information, we generate specific machine code for these cases. In certain scenarios, JIT compiler can know more than a traditional compiler and make better decisions about optimization. This also means that if you're function runs only once, JIT will not help. But in most cases, bottlenecks are in functions that are called many times. Cpython does not come with a JIT, but does an external project that does that. Numba. Numba, this is a compiler kit called LLVM, under the hood, to generate machine code for Python functions. Let's see an example. Assume we have coefficients of a polynomial, and we'd like to compute the volume for n. So in line four, I have the function. Stay coefficients in n, and calculate the value for n. In line 15, I define some coefficients. Let's time it. Ipython. So we run jit.py, and then we do time it of poly of coefficients and the number seven. And it's 2.05 microseconds. Let's use Numba. This is super simple. Here is our new code. In line two, I import number. In line 15, I define the same function. Right now, it's decorated with number dot jit. But apart from it, the code is exactly the same as the one in the poly in line five. Don't forget to save your code, and let's try that. So, we run it again, jit.py. Let's see if we get the same results. So, poly of coefficients and the number seven, and poly j of coefficients and the number seven. We got the same number. Running poly j also created a jit version of the code. Let's time it. Time it, poly j of the coefficients and seven. So, 769 nanoseconds. So if we do 2.05 divided by .769, you see that we got, what 2.5 times faster with just a simple decorate. Numba is not magic. You need to measure and make sure you get speed up from it. It can do much better if you provide some typings in the decorator and can also run code on the gpu, the graphical processing unit, or the graphics card, which might give even a better boost to your code. Traditionally, when we work with non py, we are told that loops are slow and we should avoid them. It takes some effort to learn how to avoid loops, or vectorize your code. Numba lets you get back to writing loops without too much performance.

Cython
- Sometimes we did all we could in Python and it's still not enough. Instead of writing everything in a more performant language we can rewrite only parts of the code in another language and use it from Python. In the old days we said that we code in C and when we have performance issues we write parts in assembly. Today, we code in Python and when we have performance problems we write parts in C. Writing C with the Python API can be challenging. Cython is a nice middle-ground. It gives both good performance and enjoyable coding experience. Cython is a superset of Python. It's Python plus types and connections to C libraries. We write Cython files in .pyx files and we need a C compiler so that Cython will be able to compile these files to C models that Python can use. We'll go over a short example. Cython is well documented and is used by many projects including some big ones like Pandas and scikit-learn. Let's you're writing your own square root function using newton's algorithm. Here's the python implementation. Let's test and time it. So, ipython, and then we run sqrt.py. Let's see sqrt of two. 1.41, which looks right. And then the timeit magic on sqrt of two. 806 nonoseconds. Let's rewrite this code in Cython. I'm showing this code side by side so you can see the differences. On the right side, we have the Python code and on the left side we have the Cython code. In line four, I added types for the variables coming in. I said that n and epsilon are doubles. In line six I said that guess is a double and it's defined using cdef, meaning it will be a C double. We can't use the Cython code as is. Actually, you can use Cython directly in IPython Jupyter Notebooks with the Cython magic. In our case, we need to compile, and the way to do it is to write a setup file. Here is the setup file. We import setup from distutils and from Cython we import cythonize. And in line six we call the setup function and we tell it we have an extension module and it's cysqrt.pyx. Once we have this file, we are ready to compile. So let's get back to the shell. And now we do python setup.py. We do build_ext which tells it to build an extension, and we say dash i, meaning build the extension in the current library so we can import it. If you look at the list of files, we see now that Cython generated a C file, cysqrt.c and compiled the C file to a .so file which is a dynamic library where a model Python can use. Let's see how we did. Ipython and from cysqrt we import sqrt. Let's see that it behaves the same. So we got again 1.41. And now, timeit sqrt of two. So we were at about 806 nanoseconds before and now we are at 90.9, so we're about 8.8 times faster. The downside is that now we have a build step which in pure Python we didn't. Also, the generated model is specific to the operating system architecture combination. Model built on OSX AMD64 won't work on Windows. You should deploy to the same architecture, this is not a problem. In open source projects, they either generate distribution for every major platform or provide just the sources and assume the user will have a C compiler and the Python header files. Cython also provides easy access to code already written in C or system DLLs, which is another big plus. You can also use all the cores on your machine and other speed tricks. I highly recommend that you get familiar with Cython.

PyPy
- So far, we've been using CPython which is the main implementation of Python written in C. There are several distributions to CPython such as the one from python.org, Anaconda, which is very popular with the scientific community and others. But there are also other implementations of Python. There's Jython, which is written in Java, and it's used Java classes. There's IronPython, which is written in .net and let's you use code written in .net. There's MicroPython, which is optimized for microcontrollers and more. Another implementation is PyPy which is Python written in Python. This sounds crazy at first but PyPy can try out new things quickly and also has a JIT, just-in-time compiler which can speed up things considerably. The Achilles heel of PyPy was the fact it didn't work well with many of the third party models. But the latest versions of PyPy have improved in this area. Now you can use non-Py, pandas, and other projects directly from PyPy. Let's look at an example of a Fibonacci sequence. So here we have the code for a Fibonnaci sequence and let's time it, first in CPython. So ipython, and now we run fib.py and we'll do a timeit of fib of 1000. 86.9 microseconds. Now let's try it in PyPy. Here I have ipython shell with PyPy. Let's run our code from here and time it. The timeit magic fib of 1000. So 23.2 microseconds. If you remember, in our code it was 86.9. 86.9 / 23.2 So we're almost four times faster just by switching an interpreter.

C extensions
- Sometimes no matter what we try, Python is just not fast enough. What we do then is identify the bottleneck in our program and re-write it an a more performant language. Most of the time this language is C. There are less painful options such as cython. However, c wizards know how to squeeze every cycle out of the CPU. I recommend going dowm the C extension path only after you tried other options and if you're familiar with C. A lot of times, changing algorithms at the Python level will yield much better results than rewriting C. The python C API is well documented. It gives you a great flexibility and many hooks into Python internal. However, there is a price. There's no memory management in C and you should read and understand how the Python garbage collector works and what are the differences between a borrowed reference to an owned reference. Let's look at an example. Computing square root with Newton's method. Here is the C file. Line four and five we define a C function called c_sqrt. And what we do there in line eight is unbox. We get the Python integer into a C integer. And then in lines 14-19, we do the actual work. And then in line 21, we do boxing. We transform from C to python. In line 25, we define the methods of this module which in our case is just one method. In line 30, we define the python module structure. And in line 40, we initialize the module. To compile this python extension, we'll write a setup script. Here is a setup script for our extension. We import setup and extension from these two and in line four we create a setup function. We give the name, the version, description. And in line eight we said we have an extension module called csqrt and the sources for it is our c file. To compile, you'll need the c compiler and the python header files. All major operating systems, linux, Osx, and Windows have great free C compilers. Once you have one installed, open the terminal, and type python setup.by build ext, tells it to build an extension, and -i tell you to build the extension at the current directory. If you look at the file we see that we have a .so generated which is the python extension. And now let's see how much we have improved versus the pure python code. Ipython. I have the pure python code in sqrt.py. And let's time it. Timeit sqrt of two. It totaled in 18 nanoseconds. And now let's try the code from our c extension. From csqrt import sqrt. And run it. 123 nanoseconds. So 810 divided by 123, we're 6.6 times faster. We added complexity to the build step and either need to build an extension module per platform or assume our library users have a c compiler. If you want to use existing code written in C, take a look at the swig project which makes using sql from python easier.

### Adding Optimization to Your Process

Why do we need a process?
- I work with a lot of things that are facing growing pains. When you are alone or a part of a small team, process might not seem that important. But as teams grow, working without a process is a problem. Most people don't like a process. They feel like it's just dreading. What I usually ask them is, "When you go shopping, "why don't you park your car at the entrance of the store? "Why do you use the parking lot instead?" This is also a process, and it's essential as size grows. There's much to discuss about process. But we're going to focus on optimization and performance. Having performance integrated into your process will save you from nasty surprises when production starts to create. We'll also notify you about performance problems. It's better to learn that you have performance issues from your own monitoring system, than to learn you have issues from seeing customers leaving your site. Every team has its own unique process. I try to provide some guidelines that should help you develop your process. Remember, that a process should serve you, instead of you serving the process. You should always be trying to make it better.

Design and code reviews
- The earlier you can detect performance issues the easier they are to fix. Design review is a good way to find performance issues before the code is even written. I've been called several times to fix performance issues at companies. Even though I manage to make things faster, the actual best solution is a redesign and then a rewrite of the system. Sadly, most companies are not willing to invest in such an effort. If the code was designed and reviewed with an eye to performance, this problem would have probably been avoided. I recommend working with a checklist of items and always try to think about potential bottle necks and how to avoid them. Here are a few items I recommend you include on your list. What are the performance restrictions? Designing a system that will process data in a minute is very different from designing one that processes data in a millisecond. You should get hard numbers from the stakeholders. How do we measure performance? Do we use averages, medians, quantiles? How can an instrument or code to measure performance and how can we view it? Can we remove any code? The fastest code is the code that is not executed. One of my most productive days was deleting a few thousand lines of in-house code and replacing it with a third-party library that was much, much faster. Do we have too many components? Function calls in Python are expensive. And calling a service is even more expensive. Think of ways you can eliminate calls to components and services. Do we do too much serialization? A common mistake I see is that the function gets the data as a string. It passes the string to another function that passes it to another. Most of these functions are passing the string into a daytime object. It's better that the initial function will do the parsing once and then pass the day time object downward. Same for serialization. You should serialize and de-serialize at the edges of the program and use the right Python data structure in the middle. To use the right algorithms and data structures. People often go with algorithm they know. However, the team as a whole might know much more and come up with a better solution. Even if the team doesn't know, asking people to do research and come up with alternatives, may do wonders to your code performance. After the code is written, I highly recommend you do a code review. Most source control systems like GitHub, Bitbucket, GitLab, and others have an integrated code review system. Here again, I recommend using a checklist. This list should include the design considerations and any other questions your team thinks are relevant to optimal code performance.

Benchmarks
- A lot of teams have a continuous-integration, or CI systems, such as Jenkins. These systems run a test suite every time there's a change to the code. You should also include benchmarks in your code and save the results of these benchmarks to a file or database. This way, you can see if the new code decreased performance or improved it. As a rule of thumb, you want to see benchmarks staying the same up to a given threshold, or getting better. Discussing benchmark figures as a part of the daily standup will do wonders for performance awareness in your team. For an example of how to do this, check out PyPy's Speed Center site. As you can see in this site, they have benchmarks comparing PyPy to CPython. They also have one section for changes. You can analyze the timeline and you can do comparison. A nice tool that can help you is pytest-benchmark. This is a plugin to the popular pytest test suite. This plugin lets you write tests that are benchmarks, save results for comparison, and publish JSON files you can save to a database. You can install pytest-benchmark with pip or conda. Let's see a small example. Say we have the Fibonacci function. So, we saved our code, and then we write a test. So in line four, we define test_fib, which is a test for pytest but it gets a benchmark fixture, meaning it will be a benchmark. In line five, the benchmark is run on fib of 30. And we get the results, so in line six, we can verify that the result is correct. Now, let's run it. So in our code, we can do pytest - -benchmark-autosave. benchmark-autosave tells pytest-benchmark to save the result for future reference in a directory called .benchmarks. So, it's running our code and we see some measurements about median, standard deviation, et cetera, et cetera. Now let's throw a cache on our code. So, we go back to our code and from functools, we import the lru-cache and lru_cache, and we save our code. Now, we're going to run pytest-benchmark again, but this time, we're going to also to tell it - -benchmark-compare, so it will compare this run with the previous one. So now we have statistics for the current one and we see a lot of improvement from the previous one. pytest-benchmark offers many other capabilities, such as plotting histograms, setting profile information, and much more. Since it can also emit JSON output, you can easily build your own tooling around it. Make sure your benchmark is similar to reality as much as possible. Go the extra mile and try to find real world data for your benchmarks.

Monitoring and alerting
- It's always good to do design and code reviews and to benchmark your code. However the real money is in production. Even with the best process in place, bugs and performance issues will slip into production from time to time. And, you should be prepared. The practice of keeping an eye on production is called monitoring and alerting. A lot of companies have monitoring and alerting in place, but only for bugs or the check that services are up. There are many systems from the new and shiny Prometheus, InfluxDB, and Elastic Stack to the good old Nagios inference. Whatever system you use, make sure to add performance metrics to it. It's very easy to add a decorator to some functions that record how much time they take. Here's an example. In line six, we have a time decorator. At line nine, we record the start of the time. And then, at line 11, we call the function. In line 13, we calculate the duration. In line 15, we print out how much time this function took. In real life, this will be saved to some kind of a metrics database such as InfluxDB. In line 24, we have an example. We decorate the function add with the timed and to simulate work, we sleep for some time. Let's see how it works. ipython and then we run the code to load it. And then we are calling add three and four. And we've got the print out that that took 0.3 seconds and the output which is seven. Take care enough to make sending the metrics a performant issue by itself. For example, we can send metrics over a slow network. I tend to save metrics to disk or send them in a non blocking manner such as UD. Plot a graph over time and you have a good sense if something bad is happening or not. Try to mark deployments on these graphs, so we'll know if the new code made things worse or better. A lot of things create dashboards like this. This example is Grafana, but there are many other tools out there. And since over time you'll have many performance metrics, you should also use alerting. For example, if the median time of the last 100 queries was more than a second, someone should get paged. A bot should comment in a chatroom or whatever alert makes your team tick. Make sure to keep the signal versus nodes ratio low in these alerts otherwise people will tune them out.

### Conclusion

Next steps
- Thanks for watching. I hope you found this course useful. Optimization is a never-ending story. Every time you fix one performance issue, a new one will come to light. User behaviors and environment are constantly changing. Raymond Hettinger, who's the speed czar of Python, said, "Much of the doubling speed for core Python "that has occurred over the last 10 years "has occurred one little step at a time, "none of the them being individually dramatic." Get out and learn about new and old technologies that might help you. Go to meetup on algorithms and data structures. Read blogs with the title How I Made X Y Times Faster. Read books such as The Performance of Open Source Applications. Go to conferences. And always think, how can I make this faster? Experiment with all the things you learn, and don't forget to have fun. And of course, feel free to reach us at 353Solutions. We've been helping teams to optimize their code for many years and have a big bag of tricks at our expense. Happy hacking.
