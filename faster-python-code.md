## Faster Python Code

https://www.linkedin.com/learning/faster-python-code

https://github.com/manwar/faster-python-code-661762

### Introduction

Welcome
- Hi there. I'm Miki Tebeka, a developer, mentor, instructor, and author. I love technology and write code daily, most of the time in Python. Optimized Python code will make your site or application more responsive, which means customers will stay longer and be more likely to return. This course is a compilation of optimization tips and tricks I've learned in the last 20 years. In this course, we'll cover how to find performance bottlenecks by profiling CPU and memory, picking the right data structures and algorithms, assorted tips and tricks to make your code faster, caching techniques, how to cheat the factory, parallel computation, using tools and languages such as C, Cython, Lambda, and others, and how to integrate performance into a process. Let's roll.

What you should know
- To get the most out of this course should have some familiarity with Python and the command line. I'll be using Python 3.6. But the code should work with other Python versions with slight modifications. But if you would like to follow along with me exactly I recommend using version 3.6. But the principles you learn here are applicable to any version of Python. I'll be using packages that are not in the Python Summit library. Should know how to install third-party modules. I've included scripts to install all of the requirements needed for all of the exercises. One for installation with pip. One for installation with conda. For some exercises you need a C compiler. There are great free compilers for all major platforms Linux, GCC, Os x, Xcode and Windows Visual Studio. You should be familiar with the command line. I'll be using a Mac machine to demonstrate. But this code should work on Linux and Windows as well. The exercise files contains code and an auxiliary file is required to run them. I'll mostly be using an optimized and unoptimized version of the code in 2 different files. So you'll be able to see the differences. These files are available in the exercise file folder for reference.

Use Codespaces with this course
- We have refreshed this course for you. What does it mean? First, we added support for Codespaces, so now you can head over to the GitHub repository for the course, click on the green Code button, select the Codespaces tab, and click on Create codespace on main. If there is this warning while you start out, just ignore it. Once your browser is inside Visual Studio Code, you're ready to go. Everything is there. You can follow along without the need to install anything. There are a couple of other changes. First, I've updated the requirements to be the latest version of every package, and I also updated python --version to the latest version of Python. So now you can start, follow along, and enjoy the ride.

### Tools of the Trade

Always profile first
- Here are the three rules of optimization. The first rule is don't. Most of the time, your code is fast enough. You should always have measurable performance goals before optimizing code. I've been in several situations where people told me, "make it as fast as you can." I usually answer this by saying, "okay, I'll get the team to create a special hardware then." Clarifying performance goal would effect the design of your system. The system that responds in one minute is very different than one that responds in one millisecond. After clarifying performance goals, you'll often find that Python is fast enough without any optimization at all. The second rule is don't yet. This rule usually means that since machine time is much cheaper than developer time, sometimes the solution is to get a better machine, faster network, or other solutions that don't involve changing code. The third rule is profile before optimizing. Profiling is the act of measuring where your code spends it's time. I've been doing code optimization for many years and the initial performance results still surprise me. Many times I found that the bottlenecks are in different place than the code I initially guessed. There are many tools for measuring how fast the code is running and where it spends it's time. Learning to use them effectively will make the process of finding your bottlenecks much easier. I'll also encourage you to read Rob Pike's Five Rules of Programming which is a very useful resource. One of the main takeaways is that bottlenecks occur in surprising places.

Always profile first
- Here are the three rules of optimization. The first rule is don't. Most of the time, your code is fast enough. You should always have measurable performance goals before optimizing code. I've been in several situations where people told me, "make it as fast as you can." I usually answer this by saying, "okay, I'll get the team to create a special hardware then." Clarifying performance goal would effect the design of your system. The system that responds in one minute is very different than one that responds in one millisecond. After clarifying performance goals, you'll often find that Python is fast enough without any optimization at all. The second rule is don't yet. This rule usually means that since machine time is much cheaper than developer time, sometimes the solution is to get a better machine, faster network, or other solutions that don't involve changing code. The third rule is profile before optimizing. Profiling is the act of measuring where your code spends it's time. I've been doing code optimization for many years and the initial performance results still surprise me. Many times I found that the bottlenecks are in different place than the code I initially guessed. There are many tools for measuring how fast the code is running and where it spends it's time. Learning to use them effectively will make the process of finding your bottlenecks much easier. I'll also encourage you to read Rob Pike's Five Rules of Programming which is a very useful resource. One of the main takeaways is that bottlenecks occur in surprising places.

Measuring time
- One of the simplest things we can do, is to measure time. Measuring time is helpful in many scenarios. When deciding between two alternatives, when gauging improvement, and to get metrics on run time, just to name a few. In Python, we have two basic methods of measuring time. The time module and the timeit module. Time is surprisingly tricky in computers. For example, if your computer updates its clock from the network while measuring, you might get negative results. I encourage you to read the awesome Falsehoods Programmers Believe About Time article to learn many more surprising aspects of how computers handle time. When measuring elapsed time, times when your process was sleeping due to rescheduling by the operating system are also included. You need to decide if you're interested in measuring sleeping time or not. In Python, people usually use the time function to measure time. However, in Python 3, we got monotonic and perf_counter functions for high-resolution monotonic timers. Let's see an example. Say we'd like to sum numbers up to n and we want to know whether to use a for loop or the built-in sum function. You can find the following Python file in the exercise files. So we input perf-counter from the time module. And then we define two functions. In line six, the function using the for loop, and in line 14, function using the built-in sum. And then we define in line 20 the constant n of how many times we want to calculate the upto. Noted in Python 3.6 you can add underscores to numbers to separate the thousandths. In line 22 we start the performance counter. In line 23 we run our code. And then in line 24 we calculate the duration by running performance counter again and subtracting the start time. And then at line 25 we print the duration, which will be in seconds. And we do the same thing again for the other function in line 27. Let's switch to the terminal and run this code. So, Python, using time dot py. And we see that the sum function is much faster. Sometimes the run time of a single call is too small and we would like to run the function many times and average the result. In this case, we are going to use the timeit module. Timeit also does some warmup code to make results more meaningful. Say you'd like to find the value associated with the key in a dictionary, and return a default if the key is not found. You can either use the dictionary built-in get method, or try to catch a key. So here in our code, we have items, which is the dictionary, which have two keys, a and b. And a default of minus one. In line 11 we define one function that uses try and catch. And in line 19, we define a different function which uses the dictionary's get method. In line 24, we start using the timeit function. If you look at line 26, you see that we code timeit with two arguments. The first argument is the actual call, and it should be a stream. And the second argument is a setup code. Repeat two scenarios. One when the key is in the dictionary, and the second one when the key is not in the dictionary. Let's run this from the terminal. Python using timeit dot py. We see that when the key is found in the dictionary, the try/catch method is faster. However, when the key is missing, get is faster. You should know which case is more common in your data. If you're using IPython or Jupyter Notebook, you can use time and timeit magic commands. This is how I usually work. Let's redo our timeit calculation in IPython Notebook. So I'll run the IPython Notebook, IPython. To load the code I'm going to use the run magic. So, percent run dash n, dash n tells run not to run the main part of the code, just to load the functions. And then name of the file. And now I can run the timeit, and this time I'm going to write the function just like a regular function code. Use get of a. And we see that we got 214 nanoseconds. And then we do percent timeit, use catch of a. This time we got 126 nanoseconds. We get the fancier output with units of measurement using the magic commands.

CPU profiling
- A profiler monitors the execution of your code and records where it spends its time. There are several profilers in the Python standard typer. cProfile, which is a deterministic profile is currently recommended by Python. Deterministic profiler record every function call and return, as well as exceptions. This is in contrast to statistical profilers, which record where the program is at small intervals. The documentation on the Python profilers is great and include important information on their limitation. Do read it. The profile generates a file with statistics on the run time and we use the pstats module to display them. The basic display is textual, but there are visual displays as well. Let's say we have a system that checks user login. To be secure, we store the user password in an encrypted form in a database. When the user logs in, we encrypt the password sent and compare it with the one stored in the database. Here is the code which you can find in the exercise file. You see in line two we use the crypt module to encrypt the passwords. And the main function is in line 25, where we do our login. At line 28 we fetch the password, the encrypted one, from the database. And in line 32 we encrypt the password sent from the user. And in line 33 we compare those passwords. To get meaningful statistics we'll create code that emulates good and bad logins. In gen cases we try to emulate real data. Most logins, 90% of them, are okay. And this is in line nine. And then the other cases, we'll divide into two cases. One of them with missing users or no such user. And the other one with bad password. In line 18, we have a function which gets all these cases and calls login on all of them. Line 25, we define how many cases we want. And in line 26, we generate the test cases for us. I'm going to show three options. So I'm going to use ifs to define between them. Let's switch to the terminal and run the call with cProfile. So we're going to switch to the terminal and write python -m cProfile and the name of our file. And we see that we get a lot of input. And we get the number of calls, the total time, the time per call, etc., etc. If you look closely, you see that there's a lot of setup code. This is because we are profiling the whole file, including the generation of the test cases. We can do better by targeting the code we like to profile. So I'm going to comment this one out on comment line 31. So now we're going to run cProfile just here after we generated the test cases. So let's go back to the terminal. And this time we're just going to write Python without the -m cProfile. And now we see mostly our call at the top. We can even generate an output file and use pstats to drill it. So let's go back to our code. And we use the last if statement. And here we tell cProfile to save the results in a file called, prof.out. Let's go to the terminal and run our call again. This time there is no out, so we'll do Python -m pstats and the name of the output file, which in our case is prof.out. Then we can have a look at the stats. Say the top 10 ones. We can even sort the stats. So I'm going to sort by cumulative time. And then show the stats again. This textual UI is good enough and I end up using it most of the time. However, sometimes we'd like a fancier UI. There are several models that do that. Let's have a look at SnakeViz. It's not in the start library. So you can install it via peep or conda. Once you install SnakeViz, we can run it on our output file. SnakeViz and the name of the output file, which is prof.out. We'll get a webpage with the results. At the bottom, you can see a table with the results in the textual format. And you can click on column headers to sort by them. On the top, you see these circles, which represent the execution. They start from the middle. This is the top most function that was executed. And go in layers after the functions that we had. We're interested in the batch login function so let's click on it. And then we see that the circles have changed. Now the middle one is the batch login and these are the parts that we're looking. We see that most of the time, we spent around here, which is the crypt method that we used. We can also have a look at the call stack for the current function. How did we get here? On prof, etc., etc. And you can always click Reset to reset the display. iPython in Jupiter Notebook have a magic command for profile. So we'll run iPython. And then we use the run magic with -n. We tell it not run the main code of prof.py. Now we'll generate some test cases. Cases equal list of gen-cases of 1000. And now we use the prun magic. Bench prun bench_login and rcases. And we see that we get the same textual output as before. By default, pron sorts the results by time. But you can also sort results by other values. To view the options you can do prun and a question mark. And if you scroll down a bit, we can see that we have the dash s for sorting and dividing arguments that we can use. So let's try it out. We can click on Q and then prun -s cumulative and bench_login of case. And now the results are sort in cumulative order. Using cProfile will slow down your program. If you need a more lightweight statistical profiler you can take a look at the vn prof package.

line_profiler
- Sometimes a finer granularity than a function is required. In these cases, line_profiler can be used. Line_profiler is not in the standard library. You can install it with pip or Conda. When we pip install line_profiler, we also get the command-line program called kernprof which is used to run our program under the line_profiler. Line_profiler doesn't automatically profile anything. You need to tell it which functions you'd like to monitor. We do this by adding a profile decorator. If you look at our code for logging in user, which you can find in the exercise file, at line 26 we have a login function. We want to profile it, so we'll add a profile decorator. A decorator lets you extends a function behavior without changing its code. If you're not familiar with Python decorators, check out our Python essential training course. And now, once we added the profile decorator and save the file, we are going to switch to the terminal and we're going to write kernprof and then dash L prof.py. Prof.py is a file that exercise the login function. And we see it wrote the results to a file called prof.py.lprof. To view this file we are going to write python dash N line_profiler and then prof.py.lprof. And now we can see line by line how much time was spent and what is the percentage of time. And we can see for example that line 33, the encrypting of the password, took 23% of the time, which is a lot of time. Line_profiler also adds a magic command to IPython or Jupyter notebooks, let's have a look. First we need to go back to our code and remove the profile decorator, and save. Then we'll go to the terminal and open IPython. To load the code we are going to do the magic run command dash N, not to run the main, and prof.py. Then we'll generate cases, with cases equal list of gen_cases of 1,000. And now we're going to load the line profile extension with load_ex and line_profiler. Once we have that we can now run the lprun which is the short magic command for running the line_profiler. We need to tell it with dash F which functions we are interested in. In our case the login, and we are going to call bench_login with our cases. And again, we see a similar out. It's time to try faster encryption algorithm. Let's see if replacing crypt with SHA256 will be faster. So we have our code and encrypt_passwd2 is using SHA256 to encrypt the password. Let's go back to IPython. We need to load the code. Let's use the run method and login.py. And now, we can use the timeit magic, to time it out. So, let's define a password. Password equal, let's say duck season. Let's load our new code as well. So, run enc256.py. First let's check that our function works. You'd be surprised how many times people optimize buggy algorithms. So, encrypt_passwd2 of the passwd. And we got some out. And now some time. So first let's time, timeit the original function. Timeit encrypt_passwd of the passwd. And now let's time this second version. So, percent timeit encrypt_passwd2 of our passwd. So 1.29 microseconds, versus 3.2 microseconds. Almost three times faster. If you're not familiar with microseconds and other units of measurements, here's a table that will help you understand it. We have milliseconds, which are abbreviated with ms and we have 1,000 of these in one second. After that we have microseconds which are abbreviated with mu, the Greek letter mu S, and there are 1,000,000 of these in a second. And then we have nanoseconds, abbreviated with ns, and there are 1,000,000,000 of these in a second. So pay attention to the unit of measurement timeit is showing you.

Tracing memory allocations
- Sometimes you'll hear people talking about memory leaks and memory efficient programs. How is memory connected to program runtime? There is data in a program. Numbers, lists, objects, et cetera. These are stored in the computer memory, known as the heap. Every time we create a new object, we need to allocate storage for it and this operation takes time. This is one reason for why we care about memory allocation. Another reason is that accessing memory in modern computers is done in layers. We have the CPU, and then we have L1 and L2 caches. Then we have the memory. And the access times are very different. Accessing layer 1 cache is about 0.5 ns, while accessing the main memory is 100 ns. Every time we try to access a piece of data, the CPU will first try to fetch from the cache, then from main memory. Difference, also known as latency, is huge. If we keep our data small, there's a good chance it will fit inside the cache line, and then it will be much faster to access. Sometimes algorithms that seem faster on paper are much slower than dumb cache friendly or sometimes known as cache oblivious algorithms. The last reason is that modern operating systems give us the appearance of having infinite memory. The computer I use right now has 16 GB of memory, but programs can use much more than that. This is done by swapping some section of memory into the hard drive. When we try to access the section memory that is currently swapped to disk, the operating system will pick another section of memory, will write to the disk, and then load the section memory we require into actual memory. This is also known as a page fault, and it's very costly operation. Hardware latency, even solid-state one, is order of magnitude slower than memory. All of this means we'd like to keep our memory small so we don't cause page faults. I hope you're convinced that you need to profile and monitor your memory as well. Tracemalloc is a library that was added in Python 3.4 as a tool to understand memory allocations. Let's look at an example. Assume we have our own encoding scheme for an event object. In reality you should use an existing encoding scheme like JSON, YAML, MessagePack or others. We are creating our own encoding just to demonstrate how to trace memory. Let's take a look at enc.py, which is found in the exercise file. The details are not that important, but in line 39 we have encode_event, which creates an encoder, and then calls the enc.encode method to encode the event. In main.py, we use tracemalloc. We import tracemalloc and we use a temporary file to encode the data. In line 50 to line 55, we create some sample data to test our code. And in line 59, we start the tracemalloc. In line 61 and 62, we encode all these events, and from line 64 onwards, we tell tracemalloc to print the statistics. Let's run this code. We're going to switch to the terminal, and run python enc.py. And we get some measurements about where the memory was allocated. If we look closely, we see that line 41 produces data. If we go back to our code, this is the line where we create a new object for encoding. One usual optimization to this kind of problems is to use an object pool. If you suspect that you have a memory issue in your program, tracemalloc is a good tool to have in your arsenal.

memory_profiler
- If you're running five from two and also need memory profile or need some memory user statistics, you can use memory profiler. Memory profiler is not in the stomach library. And we need to install it before we can use it. You want to compute the sum of differences of values in a list. The file is called sos.py and can find it in the exercise file. Once you have your function, you need to add the profile decorator before the function so memory profile will know to profile this function. At profile, and save the file. And now we switch the terminal and run python dash M memory profiler and our code sos.py. And we got some out. As we can see, line seven is the one that generates most of the memory. We can easily fix this by looking over intercedes and not over values, thus avoiding the location of vials to farming. When we install memory profiler, we also get amplify utility, which is time based profiling. Time based profiling will help us see if our memory continues to grow. We have the usual wavelike pattern of our location and garbage collection. Let's go back to our code. We remove the decorator and I'm going to increase the number from one million to 100 million. And save the file. Now we go back to the terminal and you run mprof run sos.py. Mprof generated a profile date. We can view this file after installing the Macbook library with mprof, plot, and the name of the file. And we get the graph and we see that our memory here continues to grow. This is just one function call so we should be fine. Usually you want to measure loops over time with mprof. Memory profiler also add a magic mp run command to python and others. Check the commentation for details.

### Picking the Right Data Structure

Big-O notation
- You'll often hear the term Big O in relation to performance. Big O Notation basically means in the worst-case, how long will our code run given a specific input? How long usually means the number of operation or, if you're looking at the memory, the size of memory. For example, let's say we'd like to know if a given number is inside a list. Ideally, it will be the first number on the list and we'll do one comparison. In the worst-case, the number won't be on the list and will iterate over the whole list to find out it's not there. Assuming the size of the list is N, we say the complexity is O of N. This is a very simple case and for more complex algorithms determining the complexity can be very difficult task. Lucky for us, someone else already figure out the complexity of most of the algorithms and data structures in use. For example, sorting is O of N times log N where log stands for logarithm. The nice folks at Python created a list on the complexity of various operations in the most common data structures. For example, appending to a list is O of one and sorting the list is O of N times log N. One question you should always ask is what does N mean? In our search example, it means the length of the list, however, if you're looking for a string in a list and strings there might be very long, N might be the length of the longest string in the list. And, sometimes, the complexity can depend on both the length of the list and the length of the longest string within it. Here a chart that shows how functions behave when N grows bigger. As we can see, 2 to power of N is growing much bigger than N squared. Big O ignores constants. For example, O of one means constant time. And, O of 1,000 is also considered to be O of one. Theoretically, this makes sense. But, in practice, if our code run a thousand times slower, it makes a difference. How does this help us? Knowing the complexity of operations is important for choosing the right data structure for the job. For example, if you'd like to know if an item has been seen, we'll pick a set or a digit which have O of one lookup over a list or a toppal which have O of N lookup time. If you know the theoretical limitation of an operation, we won't waste time trying to find a better solution that is better than this limit.

bisect
- Say you'd like to convert numerical scores to grades in the US system, where A is above 90, B is between 80 to 90, and so forth until the failed score below 60, which is F. Let's start with a simple implementation. I'll place this in a file called grades.py. As you see, we have a list called cutoffs and the names, and then a function on line six called grade, which just goes over the cutoffs, and once it find the grade, which is below the cutoff, returns the name. And we also have a test in line 17, which make sure that our code is working as expected. Once we have a test that passes, we can measure our code. We're going to use timeit module with ipython. So, we'll start ipython and we'll run the code grades.py. This will also run the test and since there are no errors, we're good. And now, let's time our code, timeit grade and let's say 74. This was 486 nanoseconds. If this time is acceptable, you should stop and work on something else. However, if you identified that this code is a bottleneck and we need to get faster, optimization is a good idea. Searching in a sorted list like cutoffs can be much faster. This is known as a binary search and its complexity is O of log N, which is much smaller than the O of N we do currently. And since we're using Python, we don't need to call this ourselves. We can use the bisect module. Bisect has a function called bisect that given a sorted list and a value, will return the index in the list where we should insert the value to keep the list sorted. Let's use it in the code. Before making changes, it's a good idea to commit the current point of code to our source control, Git, for example. I usually start with keeping the original code around and then creating a new one. This helps me compare runtime on different scenarios. So in our code now, we import bisect and I wrote a function called grade2, which just uses bisect. In line 24, I am asking bisect what is the index where we should insert this score, and I'm getting it from the names list. Also changed a little bit is the test. Now it gets the function to call and make sure that the function on the score is meeting the expectation. And now we run two tests, one for grade and one for grades2. Let's go back to our code. Don't forget to save. So, we're going to run grades again and since there's no errors, there's no output, which is good. And now, we can time our new function. Time grade2 of 74 and it's in 309 nanoseconds. We got about 33% speedup and as a bonus, shorter code. If you're not using ipython, you can use the timeit module from Python. Don't forget to divide the runtime you get by the number of loops timeit runs, which by default is one million.

deque
- Say we have requests coming in and we'd like to process them in order. For this we'll use a queue, also known as FIFO, first in first out data structure. Here's our initial implementation with a test. We'll place it in tasks.py. In line three I define a class called TaskQueue. In line six I use a list for the tasks, and in line nine, whenever someone pushes a task, I inserted it to the beginning of the list, and in line 12 when someone pops a task, we'll just tasks.pop, which would pop it from the end of the list, and then in line 18 I have a test for that to make sure that everything works. Let's run it. So ipython, and then run tasks.py, and the tests are passing since there is no output, which is a good start. Now let's measure. We're going to use our test function as the benchmark. So timeit test_queue with let's say 1000. So 1.3 milliseconds. If this bit is good enough, you should stop here. Python lists are optimized to work as a stack, also known as LIFO, last in first out. Items are added or removed from the right side. When changes are made to the left side of the list, the whole list is reallocated in memory. To solve this problem we have collections.deque, short for double ended queue, which behaves like a list but is optimized to work on both sides. The down sides of using deque is that item access time might be slower. Let's change our implementation to use deque. I'm going to leave the original code for comparison, and we'll replace the old code once I'm sure it's correct and faster. Here is the new code. I'm importing deque from collections model in line two, and then in line 18 I define DTaskQueue, which uses deque, and the main difference is that in line 21 we create a deque instead of a list. In line 24 we append, and in line 27 we do a popleft, which is a method of deque. I also changed the test. So now it get a class as the argument, and it test this class for correction. In the main part at line 46 and 47 I'm testing both classes. Don't forget to save the new code and let's run it again. So we'll do run tasks.py, and again there's no output, which mean the tests are passing, and now let's try again. We'll test the original queue, and now we'll benchmark the new queue. So we'll say that the class is the DTaskQueue. So 1.14 milliseconds versus 1.42 milliseconds. We got about 20% speed up.

