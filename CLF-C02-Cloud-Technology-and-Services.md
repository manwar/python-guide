## CLF-C02 - 3 Cloud Technology and Services

https://www.linkedin.com/learning/aws-certified-cloud-practitioner-clf-c02-cert-prep-3-cloud-technology-and-services/

### Deploying and Operating in AWS Cloud

Technology highlights
- You probably heard of "the cloud" in the past few years referring to ambiguous things that no one quite seems to define. You might have also heard of Amazon Web Services. Perhaps your company is considering utilizing it, or you're looking to find out more about this cloud computing platform that's taking the world by storm for your own career advancement. Whatever the reason may be that got you to click on this course, I'm glad you're here. I want to help you start from what even is the cloud to getting excited about Amazon Web Services, cloud computing, and potentially even consider taking the AWS Certified Cloud Practitioner Exam. Hi, I'm Hiro Nishimura and I'm a special education teacher turned systems administrator, turned technical writer. And my mission is to introduce cloud computing and Amazon Web services to people with non-traditional technical backgrounds. An introduction to AWS for non-engineers three core services. We will be making a dive into dozens of services AWS offers by learning about the services that make up the core services. This course is also a vital part of your exam prep. If you're thinking about taking the AWS Certified Cloud practitioner exam, as we will go over concepts like deploying and operating in the AWS Cloud and AWS's global infrastructure. I can't wait to begin our cloud journey. Let's get started.

AWS Certified Cloud Practitioner exam
- The AWS Certified Cloud Practitioner exam is the only Foundational level AWS certification exam available from Amazon Web Services. It serves to validate a candidate's foundational knowledge of the AWS cloud. In September 2023, the exam was updated from exam code CLF-C01 to CLF-C02. With the exam code change came some substantial updates that we will go over in this four-part course series. The AWS CLF-C02 is a multiple-choice or multiple-response question exam that is pass or fail. The exam is catered towards cloud beginners and those who don't necessarily have a background in IT. It covers AWS concepts, security and compliance, core AWS services, and the economics of the AWS cloud. There is no prior experience in the AWS cloud required for someone to prepare for and sit for this exam. The AWS Certified Cloud Practitioner exam is divided into four domains: cloud concepts, which makes up 24% of the exam; security and compliance, which makes up 30% of the exam; cloud technology and services, which makes up 34% of the exam; and billing, pricing, and support, which makes up 12% of the exam. Each of the four courses in this series maps to a specific domain in the exam. Let's dig deeper into the specific domain this course maps to.

Cloud Technology and Services domain
- In the AWS Certified Cloud Practitioner exam, there are four domains. They are cloud concepts, security and compliance, cloud technology and services, billing, pricing, and support. The four introductory AWS courses I teach on LinkedIn Learning follow each of these four domains. This course mainly covers the Cloud Technology and Services domain, which makes up 34% of the exam. This domain still remains the largest portion of the AWS Certified Cloud Practitioner Exam, with hundreds of services and dozens of categories and ever expanding. The Cloud Technology and Services domain may seem daunting. They even have satellite as a service. Thankfully, you don't need to know most of the services to do well on this exam, as there is a certain subset of services that are often considered core services by AWS, which doesn't change too much from year to year. Their top product categories are compute, storage, database, networking, content delivery, analytics, machine learning, and security, identity and compliance. To do well on this domain, there are a few concepts and services you need to understand. You'll need to define the AWS global infrastructure as well as methods of deploying and operating in the AWS Cloud. As for the services, you'll need to be able to identify AWS compute services, database services, network services, storage services, artificial intelligence and machine learning services and analytics services, and services from other in-scope AWS service categories. Identifying services from other in-scope AWS service categories is where you would need to be aware of service categories for certain services, which helps you identify what these services may do, but not necessarily have too much background knowledge. If you're interested in the sheer number of services offered by Amazon Web Services, check out aws.amazon.com/products. This page allows you to search for services in alphabetical order, by product category, by launch date, by their free tier type, or which recent reinvent they were announced at. Chances are it's littered with services even your organization's most experienced AWS expert has never heard of. It seems like a lot, but I promise, we'll help you break it down. Ready to get started, let's go.

Interacting with the AWS Cloud
- In this video, we're going to take a break from all the services and discuss an important topic when it comes to creating and maintaining your IT resources on the AWS Cloud. Deploying onto AWS Cloud. AWS users can manage and deploy resources into the AWS Cloud in three ways. By utilizing AWS Management Console, Command Line Interface or CLI and Software Development Kits or SDKs. They all reference the AWS API to help you deploy and manage your AWS Cloud infrastructure. The AWS Management Console is a graphical interface that supports the majority of AWS services. You can think of this as a web portal that you log into as you would your social media account to see everything that's offered on that website. Through it, you can look at your billing statements, launch new services, and find out how your apps are doing, all while using an interface that feels like you're browsing through another website. If you don't have much familiarity with utilizing command lines or SDKs, the AWS Management Console is extremely user-friendly and easy to navigate. The AWS Command Line Interface or CLI allows you to access services via the command line. The command line is a way to access and change resources with text-based command entry. You can tell an EC2 service to shut down or add a new file to an S3 bucket all by typing in some commands into the AWS CLI. It is programming language agnostic and allows you to create scripts to run on AWS. The AWS Software Development Kits or SDKs lets you incorporate connectivity and functionality of a wide range of AWS cloud services into your code, helping you deploy AWS services and resources using a variety of popular programming languages like Node.js, C++, Java, Ruby and PHP. SDK allows you to use AWS Cloud resources in existing applications. You can use just one, two, or all three ways of deployment to AWS Cloud. AWS CLIs and SDKs allow you to create tools that are specific to your organization and help create an environment that utilize infrastructure as code. With infrastructure as code, it can write code that describes the configurations for specific AWS Cloud services and they can be deployed for you by AWS. It helps to speed up the deployment process and removes the risk of human error when spinning up new resources. Some AWS Cloud services that utilize infrastructure as code are Elastic Beanstalk, AWS Lambda and AWS CloudFormation.

Connecting with the AWS Cloud
- While we talk about building on AWS Cloud and interacting with AWS resources, there's not much we can do to utilize AWS if we aren't able to actually connect to the resources hosted on the AWS Cloud. Connectivity options are ways you can establish network connections to transfer data and information back and forth between your local device and the AWS cloud. In this video, we'll learn about a few connectivity options that will help you in your quest to pass the AWS Certified Cloud Practitioner Exam. Virtual Private Network or VPN, AWS Direct Connect, and the public internet. When you create an AWS account, AWS provides you with an Amazon Virtual Private Cloud or Amazon VPC to use for your AWS resources. Amazon VPC is basically a private corner of the AWS Cloud for you to begin building your IT infrastructure isolated from other users and organizations. You can visualize your Amazon VPC instance like your in-home wifi network. It's a little isolated corner of the interwebs where you can connect your computers, your smartphones, your printer, and your smart devices, so they can connect with the internet and with each other. Ideally, it's secured and password protected so that it's private and isolated. Armed with this little corner of the Amazon web services, you would then be able to connect to it from your local computer or server. This is where the connectivity options come into play. The concept of virtual private network has been around since the mid 1990s when a Microsoft employee created a peer-to-peer tunneling protocol that paved the way for a more secure network connection between a local device and the internet. VPNs work by creating an encrypted private network connection between your local device, which is say, your computer or your server, and the resource you're accessing remotely or online. AWS has its own VPN service aptly named, drum roll, please. AWS VPN, which as you may imagine, helps you establish a more secure and private connection between your local devices or offices and your AWS Cloud infrastructure. AWS Direct Connect directly connects your local network to the AWS Cloud by creating a secure, private connection between your local network and the AWS. Connecting your local network directly to AWS bypasses the public internet and provides a more predictable and low latency network performance, as well as reduces bandwidth costs. We'll discuss different cloud deployment models in the upcoming few videos, but this method of directly connecting your local network to the AWS Cloud infrastructure may be a great way to build and operate a hybrid deployment environment. Utilizing AWS Direct Connect is considered the shortest path between your local network and the AWS Cloud, so is uniquely suited for large scale data transfers, rapid data backups, and broadcast media processing. If you're scrolling on your smartphone using a mobile data plan from a wireless carrier to load your favorite social network, chances are you're utilizing the public internet. Unlike utilizing VPN or AWS Direct Connect, the network connection you establish in a public internet is not private or secure. It's not encrypted during transit, and you cannot hide who you are or what you are doing, which is ripe for picking for bad actors who might be snooping around. If you're building on AWS, you should probably not be using the public internet to keep your data and resources secured. Instead, you can rely on services like AWS VPN to create a private network within the public internet to connect to the AWS Cloud.

Cloud deployment models
- We've learned about different ways to deploy your resources onto the cloud in the beginning of this section, like utilizing programmatic access and infrastructure as code, or IAC. There are also three core cloud deployment models. Basically, you can think of these models as where your IT infrastructure resides. These models are cloud, or cloud native deployment, hybrid, and on-premises deployment. When an organization utilizes cloud deployment, it means that all parts of their IT infrastructure reside and run on the cloud. All applications were either migrated to, or created in the cloud, and the organization completely relies on the internet and their cloud computing service providers to service their computational and IT requirements. Many small startups utilize this model as it allows them to be flexible and scalable in their resources while removing the roadblock of costly and time consuming procurement and management processes for physical IT infrastructure. They may use services like Office 365 for emails and Microsoft Office products, Skype for Business for an on-demand communication and Microsoft Azure for their app development and hosting. All resources in a cloud deployment infrastructure live on the cloud. Hybrid deployment is, as the name suggests, a hybrid of infrastructure living on the cloud and living on an on-premises data center. Hybrid deployment connects on-premises tech with cloud-based resources. This is a very common setup for many established companies that already have their on-premises data centers and are in the process of migrating over to the cloud. Hybrid deployment allows organizations to extend and scale their infrastructure into the cloud, while still maintaining access to on-premises resources living on their onsite servers. Another common use case is to use the cloud deployment as a backup and disaster recovery solution. An organization can maintain a working copy on premises, but make sure they have durable backup in the cloud. Because migration of existing IT systems take a long time and is costly, hybrid deployment is a very effective in-between as resources are migrated to the cloud. Finally, we have the other side of the spectrum from the cloud deployment, on-premises deployment. Organizations use virtualization to deploy resources in their on-premises data centers. In many cases, the execution of on-premises deployment looks like traditional IT infrastructure with its servers and network cables and data center management. And the setup does not provide many of the benefits of cloud computing. The resources are not accessed using the internet because they're on site. This means resources hosted in this model have very low latency or lag when you load them because all the data is on site and nothing has to be uploaded or downloaded using the internet. However, it could utilize application management and virtualization technologies to increase efficiency of the available resources, such as by deploying virtual machines and internet resources behind the firewall. On-premises deployment provides dedicated resources, which means that the organization is not sharing any part of their resources with another organization. This may be a requirement for certain industries that take data privacy very seriously, such as the medical field. Flexibility, scalability, and finding your perfect fit are the features of cloud computing that shines when considering which cloud computing deployment model is the best fit for your organization. For organizations that don't have very many IT resources deployed yet, cloud, or cloud native deployment, would allow them to utilize the complete flexibility and affordability, which are signatures of cloud computing services. For those who need all their data secured and on-premises, either due to retrieval speed needs or security requirements, on-premises deployment utilizing virtualization of legacy resources may be a good fit. For companies with legacy IT resources that would take a long time to upload to the cloud, but would like to extend their compute and storage capacities, try it again, for companies with legacy IT resources that would take a long time to upload to the cloud, but would like to extend their compute and storage capacity economically, hybrid deployment might be preferable. Many companies use hybrid deployment have quick access to on-premises resources, but have very safe backups in case of an emergency.

AWS global infrastructure
- Amazon Web Services has a global footprint. It serves millions of customers with data centers around the world who rely on its global infrastructure to help their businesses succeed and grow. Let's take a quick peek at what makes up these infrastructures that are responsible for securely and reliably hosting IT resources for so many businesses around the world. AWS has data centers around the world called Availability Zones or AZs. Each availability zone is independent of each other and network and power source and separate by a meaningful distance. Each AZ being separated by a meaningful distance and not relying on the same network or power source means that they do not share a single point of failure, if natural or manmade disaster struck or if one location lost power. Hosting your resources in multiple availability zones or even regions helps create what is known as high availability. The ability to provide uninterrupted performance even during natural disasters is called fault tolerance When disaster strikes, by having the capacity to recover from these scenarios quickly, the system is set to have resiliency. Having multiple copies of your data in different data centers is called having redundancy. By architecting your cloud infrastructure across multiple availability zones, you can prepare for events like power failures, natural disasters, and other potential operational mishaps to make sure that your web applications and resources don't experience extended downtimes when disaster strikes. Two or more availability zones make up a region. Currently, there are more than 100 availability zones within over 30 regions globally, with more being added all the time. Each availability zone may have one or more data centers and all availability zones within a single region are interconnected with high speed bandwidth, low latency networking, which is to say data transfer between AZs within the same region is lightning fast. Some regions have more AWS Cloud services than others. When a brand new service is introduced, it's generally first released in few specific regions as opposed to the whole entire world. Some of the regions that tend to receive new services earlier are US east, US west, some Asia Pacific regions like Singapore, Xinyan, Tokyo, and some areas in the European Union like Frankfurt and Ireland. This fact may influence which region you decide to use to host your infrastructure. Generally, you would choose a region closest to your physical location to host your AWS Cloud infrastructure because you can reduce network latency for your end users. For example, if your company is based out of Washington DC, you might pick the region US East one, which is based out of North Virginia in the United States. Some regions cost more than others, and service level agreements or SLAs also vary by region. You may also have compliance requirements to meet, which may require you to host your resources in specific regions or multiple regions. The idea that data is subject to laws of the nation where they're collected is called data sovereignty, and if you serve customers in certain areas or countries, you need to be cognizant of their laws and regulations. Organizations may also host their resources in multiple regions for disaster recovery, business continuity, or to have low latency for end users by having their resources physically closer to them. Sometimes an AWS region isn't close enough to your end users when running highly demanding applications that require extremely quick resource access. AWS has introduced AWS local zones, a new type of AWS infrastructure deployment that helps you run your latency sensitive applications by placing compute, storage, database and other selected AWS resources close to large populations and business centers. Because AWS local zones are connected with high bandwidth secure network connections to local AWS regions, you can also access the full range of end region services provided there. AWS Wavelength Zones is also a new AWS resource that helps you provide ultra low latency user experience for your application and users by embedding AWS Compute and storage services within 5G Networks. This provides mobile edge computing infrastructure to develop, deploy and scale ultra low latency applications. We went through a lot of new concepts that fueled AWS global infrastructure like availability zones, regions, AWS local zones and AWS wavelength zones. Further along in this course, we will also learn about other edge location technologies like Edge locations, Amazon Cloud Front, and AWS Global Accelerator in the network services section.

Study break: Deploying and operating in AWS
- We will soon be learning about dozens of core AWS services that provide multitudes of great services to millions of users around the world in upcoming sections of this course. However, none of this would be possible without the Global AWS Infrastructure that exists to support all these resources. While cloud computing makes this seem like everything is in the clouds, the Global AWS Infrastructure is built with physical data centers and servers existing on land. To effectively utilize and take advantage of AWS, you must deploy and operate your IT resources on AWS Cloud, as well as connect to the AWS Cloud. Deploying an IT refers to how the IT resources and infrastructure are brought into action. In terms of cloud computing, it often refers to the IT infrastructure being built up in the cloud computing platform, and then being put into action. Operating in IT refers to the daily actions and activities associated with maintaining the IT resources. The IT resources are first deployed, and then operated until they're no longer needed and are shut down. You can deploy and manage your IT infrastructure in AWS Cloud in three ways, by using AWS Management Console, programmatic access via command-line interface, or CLI, and software development kits, or SDKs. You can connect to the cloud in many ways, and in this section, we discussed how you can connect to AWS Cloud by using virtual private network, or VPN, AWS Direct Connect, and the public internet. We also learned about the cloud deployment models, which are cloud or cloud native, hybrid, and on-premises deployment. Cloud deployment models tell you where your IT infrastructure is residing. Finally, we went over the AWS Global Infrastructure, which is the backbone of AWS Cloud. There are physical data centers and infrastructures that power this behemoth of an infrastructure. Availability Zones are discrete data centers around the world, and two or more Availability Zones make up a region. Having your resources in multiple Availability Zones creates high availability. If an AWS region isn't close enough to your end users who require extremely low latency, you can utilize AWS Local Zones. For applications requiring ultra-low-latency user experiences, you can utilize AWS Wavelength Zones that embed AWS Compute and storage services within 5G networks. If you need a refresher about any of these concepts or resources, feel free to pause and go back to rewatch these videos. See you in the next section!

### Compute Services

Amazon EC2
- Your manager comes by and taps you on the shoulder. "Hey, we need to get a new server for a new application the dev team is creating. Can you get a setup for them?" "Sure," you say enthusiastically. After all, you love researching and buying new computers. How hard could this be? You soon realize that there's a lot more moving parts when purchasing a server for your company. You need to know how much storage it needs, how much memory the dev team requires, the operating systems, the bells and whistles. Even once you manage to find a server that fits all the requirements posted by the dev team, you then have to get it through all the bureaucratic ladders up and down the command chain, and your team, and the engineering team. And once you thought you were finally ready to get it purchased, you find out that you have to push it through the finance department too. And the finance department is not happy with a huge unexpected cost of buying an expensive server. You spend a little while convincing the finance department to approve the cost and you order it. By this point, it's already been a month since your manager initially asked you to purchase this, and the dev team is getting antsy. Now you wait for it to be delivered in a week, but wait, you aren't done just because it's delivered. Now you have to set it up and make it ready for the dev team's use. All in all, it could take you a few months to get that server up and running that the dev team wanted months ago, and by the time you deliver to them, they have one thing to say. "Oh yeah, we've been going further into the development process and turns out we need double the storage we initially requested." There's an easier way to do this instead of spending months of your time drowning in bureaucratic headaches. You can log into AWS and spin up a virtual server, called an instance, in seconds with the exact specifications that your dev team requested. And if their needs change, you can easily adjust the existing server or just spin up a new one. Each instance is configurable by the amount of virtual CPUs, gigabytes of RAM, size and types of storage, and network speed. You're only charged for what you use and when you use it, so there's no huge upfront cost for the finance department to argue over, and you can easily create an extremely scalable and reliable virtual server or multiple servers for your dev team almost as quickly as they could rattle off the bells and whistles they want. This service is called Amazon Elastic Compute Cloud or Amazon EC2, and it's one of the most widely used services in AWS. Amazon EC2 allows you to launch applications and servers when you need them without the upfront financial commitments. It's integrated with many other AWS services, is reliable and secure, and allows you or your company to quickly and inexpensively spin up instances of virtual servers for all your different needs. When you launch Amazon EC2, you select an instance type, which determines the hardware of the host server used for your virtual machine. Each type offers different compute memory and storage capabilities, and are evolving as technology evolves. Currently, the Amazon EC2 instance types as of Winter 2024 are: general purpose, compute optimized, memory optimized, storage optimized, and accelerated computing. Let's learn a little bit about each instance type. General purpose instances, as you may expect, provide a good balance of compute memory and networking resources that cater to diverse or general workloads. These are ideal for applications like web servers and code repositories that utilize resources in equal proportions. Compute optimized instances are ideal for applications that benefit from high performance compute processors, such as media transcoding, scientific modeling, and gaming servers. Memory optimized instances are designed to deliver fast performance to process large data sets and memory. Storage optimized instances are ideal for workloads requiring high sequential read and write access to very large data sets on local storage. Finally, accelerated computing instances utilize code processors or hardware accelerators to perform certain functions like floating point number calculations, graphics processing, or data pattern matching more efficiently than possible with software running on CPUs. As you can see, you can more or less intuit the high level use case of each type from its name.

Container services
- In the real world, you would have boxes to keep things inside and organized. In the cloud world, you have containers to package code in a single project isolated from other components and objects. You can think of them like meal kits you order to make one specific meal. Everything you need to create that meal is packaged inside, pre-portioned, and all you have to do is execute the recipe to have a delicious meal. Everything you need to deploy and manage an application in the cloud can be packaged in a container all ready to go. Containers are similar to virtual machines with its own file system, CPU, memory, and more. They're completely decoupled or independent of the underlying IT infrastructure, and are therefore portable. A virtual portable container filled with an application. For this video, we'll discuss two major container services on AWS. Amazon Elastic Container Service or Amazon ECS, and Amazon Elastic Kubernetes Service or Amazon EKS. Amazon Elastic Container Service or Amazon ECS helps you run, you guessed it, containers on the cloud. Amazon ECS is a fully-managed container orchestration service that helps you run highly-secure, reliable, and scalable containers. Deploy, manage, and scale containerized applications quickly, both on premises and on the cloud. It seamlessly integrates with the rest of the AWS platform. You describe your application and the resources required, and Amazon ECS will do the heavy lifting, launching, monitoring, and scaling your application with automatic integration with other supporting AWS services. Amazon Elastic Kubernetes Services or Amazon EKS is a managed service that helps you run Kubernetes on AWS Cloud without installing or operating your own Kubernetes clusters, saving you time and money. In a nutshell, Kubernetes is an open source platform for managing containerized workloads and services. Containers provide you with ingredients to bundle and run your applications. Kubernetes then helps you build a framework to run distributed systems like containers resiliently, by taking care of scaling and failover management of your applications. If a container goes down, another one needs to start to make sure your infrastructure doesn't experience downtime. Kubernetes can handle that process automatically for you. There's a whole ecosystem behind Kubernetes in how it works, so we won't get too deep into it here, but if you're interested, check out kubernetes.io. It has wonderful, beginner-friendly documentation to learn all about it. If Amazon EKS comes up, know that the K is for Kubernetes, and it's a managed service that helps you run Kubernetes without having to install, operate, or maintain your own Kubernetes control plane. Amazon ECS and Amazon EKS, yet another two services with names that describe their uses.

Elastic Beanstalk
- Do you find yourself doing more administration of infrastructure than coding, even though you're a programmer? Are you spending more time managing your platform, provisioning, and load-balancing than developing? Do you wish you could worry about your code instead of whether or not your application stack can handle your app? Or wish you could provide this environment for your team's developers? AWS Elastic Beanstalk could be your solution. Elastic Beanstalk is an easy-to-use AWS service to help you deploy and scale web applications by simply uploading your code. Elastic Beanstalk handles the deployment process, including the capacity provisioning, load-balancing, autoscaling, and application health monitoring. You can upload services developed using Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker. And you retain full control over the underlying resources at all times. It's free to use, and you only pay for the AWS resources needed to store and run the web applications you've deployed. You never have to worry about outgrowing the resources Elastic Beanstalk provisioned for you, because it automatically scales your applications up and down, based on its specific needs. You also have complete freedom to select the AWS resources, such as the EC2 instance type that you want to use for your application. If you decide that you want to take over the manual management of the infrastructure, you can do so at any time. Elastic Beanstalk provisions and operates the infrastructure for you, so that you can focus on coding. If you are constantly frustrated by the amount of time you spend managing, configuring servers, databases, firewalls, and networks, perhaps it's time you give AWS Elastic Beanstalk a try.

Elastic Load Balancing
- Have you ever experienced a situation where suddenly everyone around you at work seems to want something from you? And now, you probably got completely overwhelmed, and maybe found yourself unable to do any of the tasks requested, much less, all of them. When a server is overloaded with requests, it reacts similarly. It becomes unable to send out the responses to the paralyzing amount of traffic. It becomes completely overwhelmed, and you might notice that the website is loading very slowly or goes down completely. Let's think about fashion company A. Fashion company A decided to have a sale to celebrate their first anniversary, all shirts are 50% off. Their marketing department did an amazing job, and everyone is talking about it on social media. As soon as the sale starts, boom, their site goes down. Uh-oh, what happened? Chances are, the infrastructure team wasn't prepared for the crushing amount of traffic that suddenly inundated the company website's server, which quickly became overloaded. Now, the whole company is running around trying to get the website back up and communicate with angry customers. How could fashion company A have prepared better for the sale, so that their website didn't go down from too many excited customers accessing it at the same time? They could have utilized Elastic Load Balancing to automatically distribute incoming traffic across multiple servers. This means that the infrastructure department could have set up multiple replicated servers, which means multiple servers with the same content on them, and placed Elastic Load Balancer in front of them to distribute the traffic to multiple servers. That way, each server is only taking care of a fraction of the overall traffic. You can think of Elastic Load Balancers like air traffic controllers telling incoming airplanes to go to different runways. They make sure that any one runway doesn't get overwhelmed with airplanes to cause delays by equally utilizing all available runways for landing. Elastic Load Balancers help your applications achieve fault tolerance by ensuring scalability, performance, and security. They can also monitor the health of your servers. If a server goes down, the load balancers can send the traffic to the remaining healthy servers. Elastic Load Balancers are highly available, secure, flexible, and monitorable, allowing you to glean robust information about your traffic, as well as providing you the confidence that your applications are up at all times.

Serverless services
- Serverless or just someone else's servers. The jokes write themselves, and I'm not here to start the debate. But in an extremely high level, serverless helps developers build and run applications without having to manage their own servers, physical or virtual. The servers do exist, likely in the cloud computing platform's data centers, but they're so abstracted away from the application development that the developer does not have to worry about the provisioning, maintaining, or scaling the infrastructure. AWS has two major serverless compute services available to their users, AWS Lambda and AWS Fargate. Let's quickly take a look at both. AWS Lambda is probably one of the most popular services on AWS. It allows you to run code without provisioning or managing servers. It's an event-driven, pay-as-you-go compute service. So when an event triggers the code to run, that's the only time it's running, and that's the only time you're getting charged. An example of it in action could be your mobile application user uploading a profile photo. Once the user uploads the profile photo, a Lambda function can be automatically triggered to execute, to resize the image, and publish it on their profile page. AWS Fargate is a serverless compute engine for containers. Remember container services from a few videos ago? It allows you to focus on building applications without having to manage the underlying servers. Helping developers accelerate the process of going from idea to production as well as saving money. It is compatible with both Amazon ECS and Amazon EKS, the two container services we learned about recently. Bottom line, serverless is still someone's server, but as a developer building on AWS, it's not your server to have to deal with.

Amazon Lightsail
- Love the idea of having your website database or application running on AWS, but just don't have the energy or technical know-how to get it done? Need a quick development or test environment spun up? AWS has a service that'll do just a trick, Amazon Lightsail. Amazon Lightsail is perfect for simpler workloads, quick deployments, and getting started on AWS. It's a snap to get going while still being designed to scale with you as you grow. You can use it to deploy simple web applications, create websites, run your business's software or spin up developer or test environments while maintaining cost-effective monthly fees. There are many pre-configured and ready to use operating systems, web apps, and development stacks. Some of the more popular resources available are WordPress, Windows OS, Ubuntu, and Node dot js. They are one click to launch services so getting started is a breeze. You can quickly deploy projects ranging from creating your first WordPress blog to running a database with just a few clicks. If you are considering using AWS to quickly spin a projects or resources but don't have the time or engineering know how to deploy full on services, consider trying out Amazon Lightsail.

Study break: Reviewing Compute Services
- In the Compute services section of this course, we went over a few major compute services hosted on AWS, Amazon EC2, Amazon ECS, Amazon EKS, Elastic Beanstalk, Elastic Load Balancing, AWS Lambda, AWS Fargate, and Amazon Lightsail. Some have fairly obvious naming conventions and others not so much. Let's quickly review them to refresh your memory. Amazon Elastic Compute Cloud, more commonly referred to as Amazon EC2, is a virtual server hosted on AWS Cloud. You can instantly launch applications and servers whenever you want with an extremely versatile range of capabilities. It is one of the most widely used services in AWS, and you can spin up an instance with no upfront financial commitments. We learn about two container services, Amazon ECS, and Amazon EKS, Amazon ECS, or Amazon Elastic Container Service is a fully managed container orchestration service that helps you run containers on the AWS Cloud, Amazon EKS, or Amazon Elastic Kubernetes Service is a fully managed service that helps you run Kubernetes on AWS without having to install or operate your own Kubernetes clusters. AWS Elastic Beanstalk helps you deploy and scale web applications by simply uploading your code. It handles the deployment process like capacity provisioning, load balancing, auto-scaling and application health monitoring, so you and your team can focus on coding. You can upload code in many of the popular programming languages like PHP, Node.js, and Python, and you retain full control over the underlying resources at all times. Elastic Load Balancing helps your applications achieve fault tolerance by ensuring scalability, performance, and security. You can monitor the health of your servers, and if one goes down, it can reroute incoming web traffic to healthy servers. Elastic Load balancers are highly available, secure, flexible, and monitorable, which means you can feel confident that your applications are up at all times and get insight for information about web traffic going to your applications. We learned about two serverless services, AWS Lambda and AWS Fargate. AWS Lambda is an event driven service that allows you to run code without provisioning or managing your own servers. AWS Fargate is a serverless compute engine for containers and is compatible with Amazon ECS and Amazon EKS, the two container services we discussed earlier. Finally, you can think about Amazon Lightsail as EC2 on bumper bowling mode, AWS provides many pre-configured and ready to use operating systems, web applications, and development stacks to help you get your websites and applications running with minimal configurations.

### Storage Services

Types of storage
- In everyday objects, there are different types of storage for different materials and uses. For liquid, there are jugs. For sandwiches, there are Ziploc bags. For your Christmas ornaments, there are plastic boxes with lids. For your kids' used diapers, there are odor locking diaper pails. In IT, there are also different types of storage for different types of, well, things that need storing. As you may have surmised, AWS's storage services help you store data on the AWS cloud. There are three types of data that AWS stores. Object storage, file storage, and block storage. Let's start with object storage. Files are broken down into pieces called objects, which are then placed in buckets. Yes, they are really buckets filled with objects. Object storage allows you to store massive amounts of unstructured data like photos or videos, and it's best suited for static data, which is data that doesn't change or evolve. This is because once it's placed there in the bucket, unless you write over it, it can't be changed. Each object has three components. The data itself, metadata, which is data about the data that helps you search and index that data, and the identifier, which is the unique address given to that unit of data so you can locate it within the hard drive or cloud storage service. Amazon Simple Storage Service, or Amazon S3, is an example of an object storage service. Next up is file storage. File storage is something many of us are familiar with. Data is stored as pieces of information inside a folder like you would store your documents and video files in your folders on your desktop. File storage has hierarchy unlike object storage, and folders can be inside other folders. While your laptop or desktop's file storage or Google Drive's file storage may be what most of us are familiar with, an example of Amazon's file storage service is Amazon Elastic File System, or Amazon EFS. Finally, we have block storage. Block storage stores data and units called blocks! Like object storage, block storage utilize unique identifiers so you can search efficiently for that block that you're looking for. Blocks of data are distributed and stored in multiple places, and when that data is requested, the blocks are reassembled for the user. One way to visualize block storage could be the USB thumb drive or the external hard drive. We save data onto these devices and then can connect these devices to whatever computer or server we're using to access that block of data. On AWS, an example of a service utilizing block storage is Amazon Elastic Block Store, or Amazon EBS. Let's spend the next few videos learning about few of the more popular AWS storage services available.

Amazon S3
- I have my files, photos, and videos backed up to the cloud everywhere, Dropbox, Google Drive, you name it. I probably have some files stored there. Most of the time it's because I've run out of storage in one service, so I move to another. But this makes it so difficult to keep tabs on all my files and I could lose important documents if I can't log into my account or remember which account I stored them in. Many of us have haphazard file storage situations when it comes to backing up to the cloud. The storage service might go down. We might be wasting money by paying for storage space for subscription service that we may not end up using. Also, when we upload the files to the cloud, it's difficult to hot link to it, meaning you can't use an image you upload it on a service like Dropbox or Google Drive and embed it into your webpage. But fear not, AWS has a solution to all your static file storage called Amazon Simple Storage Service or Amazon S3. Amazon S3 is an object storage service, which means that you're storing each file as an entity called an object. It offers industry leading data availability, security, performance, and scalability. Scalability refers to the way you scale your usage up or down with extreme flexibility and be charged only for what you use. You can upload files of all sizes to serve a wide variety of needs, such as websites, mobile apps, backup and archiving, enterprise applications, IOT services, and big data analytics. Amazon is three boasts easy to use management features to fine tune access controls for your organization's specific compliance requirements. It is designed for 99.999999999%, or also known as 11 nines of durability, which means that there's almost no chance of the data becoming corrupted. There are many storage classes available with Amazon S3, which supports different data access levels at corresponding pricing. You can even set up S3 lifecycle policies, which would automatically transfer files from one storage class to a cheaper one after a certain number of days. These options range from using S3 standard class to store your frequently accessed files, to using S3 Glacier Deep Archive to store backup data that are rarely accessed for very cheap rates. While for the AWS Certified Cloud Practitioner exam, you need to be able to recognize the different storage classes, you'll likely not need to know too much about each specific one in detail unless you're going to be actively evaluating and utilizing them. The Amazon S3 storage classes are S3 Intelligent-Tiering, S3 Standard, S3 Standard-Infrequent access, or S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, S3 Glacier Deep Archive, S3 One Zone-Infrequent Access or S3 One Zone-IA, and S3 on Outposts. S3 Intelligent Tiering provides automatic cost savings by automatically moving objects between cost optimized access tiers. S3 Standard is a good general purpose storage for frequently accessed data requiring millisecond access. S3 Standard-Infrequent access or S3 Standard-IA provides lower cost storage for data that is accessed monthly with milliseconds retrieval. S3 Glacier Instant Retrieval offers low cost storage for rarely accessed long-term archive data with milliseconds retrieval. S3 Glacier Flexible Retrieval provides low cost storage with low retrieval fees for archived or backed up data with 1-5 minutes, 3-5 hours, or 12 hour retrieval times. S3 Glacier Deep Archive provides lowest cost storage for long-term archives with 12 or 48 hour retrieval time. S3 One Zone- Infrequent Access or S3 One Zone-IA provides low cost storage for infrequently accessed objects with rapid retrieval. This may be ideal for secondary backups that don't require multi availability zone redundancy. Finally, S3 on Outposts provide object storage for on-premises AWS Outpost environments. I highly recommend you take a peek at the storage classes infographic offered at aws.amazon.com/S3/storage-classes-infographic. I have the URL on the slide for you to copy. It will show you which storage class provides you the lowest cost storage for different access patterns and use cases you might have. Whatever your needs for object-based file storage may be, ranging from using photographs for your website or spending as little money as possible to back up your organization's files, Amazon S3 would have an option that works for your budget and needs.

Amazon Elastic Block Store
- You've spun up a virtual server on AWS using Amazon EC2 to run a database, but you notice that you are running out of space on your virtual machine. What should you do so that you can continue making your databases larger without impacting the virtual machine's performance? AWS has a solution for you called Amazon Elastic Block Store, which allows you to add extra block stores to your EC2 instance, and you don't even have to reboot your server. Amazon Elastic Block Store, or Amazon EBS, behaves like raw, unformatted block devices, which can be mounted or attached to your EC2 instance to expand your server's storage. You can add multiple volumes to the same instance, and you can use these volumes as file systems or hard drives. You can dynamically change the configurations of a volume attached to an instance, which means you can change the settings and sizes with just a few clicks on the management console. These volumes are automatically replicated within their availability zone, making them highly available and durable. Many organizations use EBS to host their huge databases. There are different EBS storage types available to fit your needs and budgets, as well as the option to encrypt the volumes for compliance. EBS provides persistent block storage volumes, which means that they don't disappear when EC2s are rebooted. They also exist independently of the virtual servers they're mounted onto and can therefore be moved around to other EC2 instances. You can think of EBS volumes as external hard drives for your virtual servers. Taking advantage of scalable, durable, and reliable storage options using Amazon EBS will make scaling your IT operations a breeze.

AWS Snow Family
- I had a lot of conflict about just where to place the AWS Snow Family in my courses. It just does too many things. I still don't really know if the storage services section of this course is the correct home for it, but it'll have to remain here for now while I continue to think about it. The AWS Snow Family is what AWS refers to as AWS hybrid cloud service, which extends AWS infrastructure and services into the edge where their customers physically are. Rare for a cloud computing service, these are hardware devices that get physically shipped to you from AWS. Currently, the AWS Snow Family is made up of AWS Snowcone, AWS Snowball, and AWS Snowmobile. AWS Snowcone is the tiniest at just 4.5 pounds and provides edge computing, data storage, and data transfer services and environments with little or no internet connectivity. You can utilize it to collect and process data, transfer data to AWS using the internet by utilizing AWS data sync, or ship the device back to AWS for rapid offline data transfer. You can utilize eight terabytes of HDD storage or 10 terabytes of SSD storage with four gigabytes of memory. AWS Snowball Edge is a medium sized solution and has 80 terabytes to 210 terabytes of storage capabilities, and 80 gigabytes to 416 gigabytes of usable memory. The device is larger than AWS Snowcone coming in at just under 50 pounds. You have the option of choosing compute optimized or storage optimized versions, and you can transfer data to it to ship back to AWS or utilize its compute capabilities to do local processing. The largest option for data transfer with the Snow Family is to utilize the AWS Snowmobile. It's an exabyte scale data migration device, and you can migrate up to 100 petabytes of data in a 45 foot long shipping container pulled by a semitrailer truck. Both AWS Snowball Edge and AWS Snowmobile are HIPAA compliant. Whether you need a solid compute solution in a jungle or metal of the ocean, or transfer petabytes of data, AWS Snow Family could have a solution for you.

AWS Storage Gateway
- So this idea of using cloud computing to keep your cost down sounds like a great idea, but your organization uses the data a lot, so you also don't want to sacrifice latency or the length of time it takes for your resources to be accessible. Going up to the cloud to download a 500-megabyte file every time you want to make an edit sounds like a horrible idea when your whole company is sharing a single circuit. What should you do to maintain very low latency, but still take advantage of the cost and time-saving benefits of cloud computing? AWS Storage Gateway may be the best-of-both-worlds solution you're looking for. It connects your on-premises storage with AWS Cloud Storage, providing a hybrid storage solution for your IT infrastructure. The service seamlessly integrates on-premises enterprise applications and corporate workflows with AWS's Cloud Storage Services through the use of a virtual machine installed onto an on-premises data center's host server. Basically, it creates a gate that connects your onsite users and devices to the resources stored in AWS Cloud with minimal latency. AWS offers three types of storage solutions to fit your needs, file-based, volume-based, and tape-based. Files backed up using the file gateway are stored as objects in S3. There is a one-to-one representation of each file backed up to the cloud in S3 and the Gateway asynchronously updates the objects in S3 as local files are updated. Local cache is maintained to provide low-latency access to recently accessed resources. The Volume Gateway, on the other hand, uploads files and blocks as opposed to single files. You can think of the Volume Gateway as backing resources up as a virtual hard disk instead of individual files. These blocks can be asynchronously backed up as point-in-time snapshots and stored as Elastic Block Store or EBS snapshots. There are two types of Volume Gateways available, stored volume and cache volume. The major difference is where the complete copy of your data is stored. Stored volume keeps the complete copy on premises while sending snapshots or incremental backups to AWS. Cached volume keeps only the most recently accessed data on premises and keeps the complete copy on the cloud. The last type of storage gateway is the Tape Gateway, which utilizes virtual tapes. You can use your existing tape-based backup infrastructure to backup data onto virtual tapes on S3. You can think about Tape Gateway as taking backups on physical tapes, except instead of physical tapes, they are digital tape cartilages stored on S3. Data is stored locally, then asynchronously uploaded to S3. The data can then be archived using Amazon Glacier, which is like sending your physical tape backups to an offsite tape holding facility like Iron Mountain. You pay for storage and data retrieval. The quicker you can access the backup data, the more expensive the solution becomes. For example, data stored via Tape Gateway is much cheaper saved to S3 Glacier Deep Archive than S3 Glacier because the data retrieval takes a longer period of time. Depending on where you want to store the complete copy of your data and how you would like to back up your data, there are multiple options available for utilizing AWS Cloud as a backup and storage resource for your frequently accessed data through AWS Storage Gateway.

AWS Backup
- Need to back up your data? Well, guess what? There's an AWS service for that. Okay. There are quite a few AWS services for that, but the one we're going to discuss here is, drum roll, please. AWS Backup. AWS, perhaps have jokingly, calls AWS Backup a backup as a solution, BaaS, and it is a fully managed policy-based backup service that automatically protects your backup data across all your AWS services and hybrid environments according to your backup policies and settings. Simplify backup protection at exabyte scale. That's a billion gigabytes. AWS backup provides data protection, ransomware recovery capabilities, and compliance insights for your data protection policies and operations.

Study break: Reviewing storage services
- In this section, we learned about different types of storage and different storage services available in AWS. AWS offers three types of storage, object storage, file storage, and block storage. Amazon's Simple Storage Service, or Amazon S3, is one of the most popular services on AWS and is an object storage service. It boasts robust storage class options to meet your budget and retrieval needs. Amazon Elastic Block Store, or Amazon EBS, behaves like unformatted block device that can be mounted to your Amazon EC2 instance to expand their virtual service storage capabilities. The AWS Snow Family is a suite of AWS hybrid cloud services that does a lot of many things, including providing edge computing, storage, and data transfer services through physical devices. The family is currently made up of AWS Snowcone, AWS Snowball, and AWS Snowmobile. AWS Storage Gateway connects your on-premises storage with AWS's cloud storage, providing a hybrid storage solution for your IT infrastructure, working as a gate that connects your onsite users and devices to resources stored in the AWS Cloud. And AWS Backup provides backup services for all your AWS services and hybrid environments at exabyte scale. If any of these concepts or services need a bit of a refresher, feel free to pause and go back to the videos. Otherwise, see you in the next section.

### Database Services

Database services
- There are a few different types of databases available to suit your data needs. The ones we're going to learn about in this video are relational databases, NoSQL databases, and in-memory databases. We'll also learn about database migration tools to help get your databases on the AWS Cloud. The first set of databases we will discuss are relational databases. Relational databases store and organize data and tables that are related to each other. This type of database is a collection of data with predefined relationships between them. An example of a relational database may be a company directory that lists the employee names and departments they belong to. A commonly used interface to communicate with relational databases is Structured Query Language, or SQL. AWS's powerful relational database solution is Amazon Relational Database Service, or Amazon RDS. It's a collection of managed services to help you set up, operate, and scale databases with just a few clicks. Amazon RDS supports six different types of database engines. You can also deploy on-premises with Amazon RDS on AWS Outposts. Amazon Aurora is a fully managed database engine supported by Amazon RDS. It is fully managed by AWS, which relieves engineers of having to engage in time-consuming and administrative work, like provisioning, database setup, and maintenance. Instead, it allows them to focus on developing their products and innovating. You can monitor performance using various AWS monitoring and alerting services, so you can quickly detect performance issues. If you already have existing databases, you can use AWS Database Migration Service to migrate or replicate them into Amazon RDS as well. You may have a database on a schema that's not supported by AWS's Database Services. In that case, AWS Schema Conversion Tool, or AWS SCT, can help you convert your existing database schema from one database engine to another. The next type of database we're going to learn about are NoSQL databases. NoSQL databases are known as non-relational databases, and it's built for lots and lots of data. As opposed to relational databases that normalizes data into tables with relationships between pieces of data elements, NoSQL databases provide a variety of data models like key-value, document, and graph, and are built for large data volume, low latency, and flexible data models. AWS's NoSQL database service is called Amazon DynamoDB, and it is a fully managed, serverless key-value database that helps you run high-performance applications at any scale. Amazon DynamoDB is serverless, which means that you don't have to provision, patch, or manage any servers. AWS automatically scales your tables up or down to adjust for capacity and to maintain performance, as well as maintaining stability with redundancies and fault tolerance. You can focus on getting rid of that pesky bug from your new superstar mobile application, instead of provisioning and maintaining the databases. Finally, let's discuss in-memory databases. There are a few terminologies that refer to this concept: memory database, or MMDB; in-memory database system, or IMDS; and real-time database system, or RTDB. In-memory databases are databases that rely on internal memory or random access memory, commonly referred to as RAM, for data storage, which is ideal for applications that require microsecond response times or have large spikes in traffic. This type of database may be a great choice for banking, telecommunications, and gaming industries that require low latency, high throughput, and highly scalable applications and databases. An example of an in-memory database solution offered at AWS are Amazon MemoryDB for Redis, which provides super quick reads and writes along with scalability that can help power your web and mobile applications. Another is Amazon DynamoDB Accelerator, or DAX, which is managed in-memory cache for Amazon DynamoDB, the NoSQL database service we learned about earlier. Amazon DynamoDB already provides milliseconds access by utilizing Amazon DynamoDB accelerators, and memory cache can make it even faster, designed for latency-sensitive and high-read workloads. Another is Amazon ElastiCache, which is a fully managed Redis and MIM cache compatible service that is ideal for high-performance use cases, like data caching, web, mobile apps, and gaming. In a nutshell, in-memory databases offer blazing-fast response times by relying on internal memory.

Evaluate: Hosted or managed database?
- When deciding on how to host your database on AWS Cloud, you have two options: spinning up an Amazon EC2 instance and hosting your own database on your virtual server, or utilizing one of AWS's managed databases we just learned about in the previous video. The main consideration when deciding whether to go through the process of spinning up your own virtual machine, managing it, patching it, and operating it under your database, or utilizing a managed database like Amazon RDS, will come down to how much control you want over your infrastructure, how much you want to automate, and how much you want to save time or cost. The whole point of managed services is that you can utilize just the service part of the offering, like creating and managing a database, rather than having to worry about managing the underlying infrastructure because AWS will be doing the grunt work for you. If you would like more control, flexibility, and choice in how your database is run and how the underlying infrastructure is managed, you'd want to choose to run your database on Amazon EC2. If you're willing to give up some control and flexibility in exchange for focusing on your other tasks rather than the day-to-day administration/management of your databases, you can choose to utilize one of AWS's managed database services.

Study break: Reviewing database services
- In this chapter, we went over four of the major database services in AWS, Amazon DynamoDB, Amazon Relational Database Service or RDS, Amazon Aurora and Amazon Redshift. Let's quickly review all of them to make sure we've got the fundamental concepts down before moving on. Amazon DynamoDB is a fast, flexible, fully managed and secure non-relational or noSQL database that can handle more than 10 trillion requests per day and support peaks of more than 20 million requests per second. It's serverless, so you don't have to provision, patch, or manage any servers, and it automatically scales up or down to adjust for capacity. Instead of worrying about managing your database, you can just worry about scaling your application. Amazon Relational Database Service or Amazon RDS is a fully managed relational database. Because it's fully managed, like Amazon DynamoDB, you don't have to provision or manage any servers. Instead of spending your time doing administrative tasks, you can devote your time to working on your products. You have six database engines to choose from. Amazon Aurora, PostgreSQL, mySQL, MariaDB, Oracle Database and SQL server. We just mentioned Amazon Aurora as one of the database engines you can use with Amazon RDS. Amazon Aurora is fully managed by Amazon RDS, and it's my SQL and PostgreSQL compatible. You can get the same security, availability and reliability of commercial databases, but for faster and cheaper. Amazon Redshift is a cloud-based, fully managed petabyte-scale data warehouse service that's faster and cheaper than other data warehouse providers. Data warehouses store extremely large amounts of data collected from a wide range of sources to analyze. It is quick to set up and easy to scale, and its encryption is compliant with many industry regulations, Databases, databases. You may not be very familiar with databases which could make trying to decipher these options a little more difficult. Amazon DynamoDB is a non-relational or noSQL database, whereas Amazon RDS and Amazon Aurora are relational databases. Amazon Redshift is a data warehouse for lots and lots of data. All are scalable, secure, and less expensive than industry alternatives. If you want to review any of the databases, feel free to take a few minutes going over the videos.

### Network Services

Amazon VPC
- Think about your wifi setup at home. That's a private network. You likely have a cable that runs into your house from the street, which connects your home's private network to your ISP, like Comcast or Verizon. That cable is connected to your modem, which is your connection to the internet, also thought of as a gateway. The modem is connected to a router or a switch via another cable, which routes traffic between devices in the network and also the internet. You connect your devices like your laptop and tablet to the router using the wireless network. Your home wifi setup is a private network where you can create your own ecosystem for connecting devices and resources. And a private network in the cloud is what AWS calls Amazon Virtual Private Cloud, more commonly referred to as Amazon VPC. Amazon VPC creates a logically isolated section in the cloud where you can provision your AWS resources. Think of it like your corner of the cloud, where you define what comes in, what goes out, and what lives inside. Amazon VPC is very flexible and secure, allowing you to control almost every aspect of your virtual network. It's completely scalable, allowing you to instantly scale your resources up or down. It also boasts advanced security features like security groups and network access control lists to help you filter inbound and outbound traffic at the instance level and subnet level. When you sign up for an AWS Cloud account, you automatically get a VPC provision to you along with automatically configured subnets, IP ranges, route tables, insecurity groups to help you get started. Going back to the analogy of your home network, the virtual private cloud is your home network. The modem is the internet gateway. The router is the route table and your network's firewall is the network access control list. Your laptops and tablets are resources like your EC2 instances that are launched inside your VPC or private network. When you create your first AWS Cloud account, you will be creating a logically isolated corner in this vast realm of the cloud where you are free to create and scale resources for your organization.

Amazon CloudFront
- Faster, faster, faster, that seems to be the trend with everything these days. Back in the days we were happy just to have objects we ordered online reliably delivered to us, but then Amazon's two-day shipping happened and suddenly waiting a week for your shoes to arrive became almost intolerable. In some cities, Amazon even has one-hour shipping for that emergency toilet paper you just can't seem to leave the house to buy. In the digital space, we went from having to buffer a 10-minute YouTube video to being able to watch a whole 4K movie with no apparent lag on Netflix. Gone are the days when you had to wait for images to load slice by slice or a whole web page to load a few rows of texts at a time. We want things fast, digital or physical, and we want them now. On the internet, content delivery networks, or CDNs, are working behind the scenes to deliver your content faster and faster. Amazon's global CDN service that securely delivers data, applications, and APIs is called Amazon CloudFront. CloudFront seamlessly integrates with many AWS services to provide optimal performance and security, including AWS Shield for DDoS mitigation and Amazon EC2 as origins for your applications. A CDN is a system of distributed servers around the world that delivers website and application content to end users based on a few factors. These factors are location of the user, origin of the website or application, and the location of the content delivery server. The main purpose of CDNs is to make loading websites and applications for end users faster, and Amazon CloudFront does this by using edge locations to cache files and resources for quicker retrieval. Imagine your favorite fruit. Most of us go to the grocery store to pick up an apple or a watermelon. Not very many of us live close enough to a farm to directly buy a fruit from a farmer. Instead, there are distribution networks set up between us and the farmer, which simplified bring the fruit to a grocery store nearby. All we have to do is pick up the fruit from the local grocery store where a truck traveled for days to bring the fruits from the farm. We have the convenience of driving 15 minutes as opposed to hours or maybe even days to buy some fruit. The farm is the origin, which on AWS could be S3 bucket, EC2 instance, or Elastic Load Balancer, amongst a few other services. The truck then takes the apples to a grocery store where they are left to be sold to consumers or in web terms cached. In AWS, files and data are cached at edge locations. Once the data is downloaded to an edge location, it stays there for a certain period of time, at which point users near the data center can retrieve the webpages or application resources from the edge location close to their location rather than having to go all the way to the origin, which could even be on a different continent. This allows for data to be retrieved faster with the best possible performance because users are not going all the way back to the origin server to download the resources, but rather accessing a location close to themselves. CloudFront is scalable, allowing you to start small and scale up as traffic to your application or webpage increases. It automatically manages traffic load without any intervention from you and utilizes application acceleration and optimization. There is no minimum commitment or fixed-term contract, and you only pay for content delivered using the service. Amazon CloudFront acts as a supermarket in a busy city, making cache data quickly accessible to users around the world using edge locations.

AWS Global Accelerator
- When we have a destination to drive to, we open up Google Maps and try to locate the best way to end up there. Most of the time, the decision on how we drive will be based on the time it takes, but sometimes there are tall roads to take into account that could significantly decrease the delay, distance, or both, but may cost you a few dollars. This in the AWS Cloud Computing world, is the feature of the AWS Global Accelerator. You can utilize AWS Global Accelerator to direct your web traffic over the AWS Global network instead of the public networks to substantially shorten the amount of time necessary for your customers to load your web applications. You can visualize this process like your customer's regular requests using the local roads and highways during rush hour, where some networks may be quicker than others. But by utilizing AWS Global Accelerator, you have access to the toll road that allows you to bypass the heavy traffic by throwing your customer requests through AWS's dedicated super fast global network. Usually when a customer sits at their computer and makes a request, which could be accessing a website or clicking buy on a web shopping cart, their request is routed through many different networks before they reach the data centers housing the web application. The servers then send back requests to respond to the initial customer request, which then goes through another set of networks to reach back to the customer. Every network that requests hit results in latency or lag. AWS Global Accelerator speeds up the content delivery process by utilizing AWS edge locations and the high speed congestion free AWS Global Network directing web traffic over the AWS Global Network to endpoints in the nearest region to the customer, so the request spends as little time as possible on the slower public networks. By utilizing AWS Global Accelerator, availability and performance of data delivery can go up by 60%.

Amazon Route 53
- If you've ever set up a website with yourpersonaldomain.com, you probably used a domain name registrar to purchase and set up your domain. You might have used something like GoDaddy or Namecheap to name a few popular commercial domain name registrars. AWS, of course, has its own service where you can purchase and set up domain names, but it can do so much more. It's called Amazon Route 53, and it's a highly scalable cloud domain name system, or DNS. It allows you to reliably and cost effectively route your end users to your internet applications. It can connect user requests to infrastructure running on AWS, like an EC2 instance, or an S3 bucket. It can also route users to infrastructure outside of AWS, acting as a DNS service for domains purchased at other domain name registrars. Route 53 is designed to be integrated with other AWS services, like mapping your domain names to your EC2 instance or S3 bucket. It's simple to set up, fast, secure, and cost effective. You are charged only for what you use without any upfront fees or minimum usage commitments. It's also designed to automatically scale to handle large query volumes. Route 53's basic functions are domain registration, domain name system, or DNS service, health checking of web application accessibility and auto-naming for service discovery. Utilizing the more robust features of Route 53 allows you to create websites and applications with high availability by automatically rerouting traffic, catered to demand, and integrating with services that send alerts when downtimes occur. One of the ways you could use Route 53 is to purchase yourpersonaldomain.com. Then to route users who visit yourpersonaldomain.com to a static website hosted on your S3 bucket. When your visitors type in yourpersonaldomain.com into a browser, they will be able to load your static website because Route 53 does the legwork of routing the user to the resources you identified. You can think of Route 53 like a telephone operator. Back in the days you would call the telephone operator so you could speak to your grandmother. When the telephone operator got your request, he or she would use a switchboard to connect you to your grandmother. Route 53 works in similar ways except with routing internet traffic.

Study break: Network services
- In this section, we went over four major network services in AWS, Amazon Virtual Private Cloud, or Amazon VPC, Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53. Amazon Virtual Private Cloud or Amazon VPC is an isolated corner of the AWS Cloud made just for you. You can provision your AWS resources in a virtual network that you define with complete control of your virtual networking environment from IP address range to configurations of route tables and network gateways. It's free and is automatically created for you when you create your AWS account. Inside your very own Amazon VPC, you can create and scale your AWS Cloud resources to your heart's content. Amazon CloudFront is a content delivery network, or CDN. The main purpose of CDNs is to make sure websites and applications load faster. Amazon CloudFront achieves this by using edge locations all around the world to cache files and resources for quicker retrieval. By caching, say, a video at an edge location in Sydney, Australia, someone who lives in Australia can stream that video much quicker than if there was no content delivery network, because otherwise, they would have to download the video all the way from the content origin, which could be anywhere in the world. Amazon CloudFront sees where you're based and routes your traffic to the closest cached location so you can enjoy the content without having to wait. AWS Global Accelerator is cloud toll road, allowing your customer request to take the super high speed AWS global network or toll roads toward its destination, instead of spending time on the slower public networks or the local roads and normal highways. AWS Global Accelerator speeds up the content delivery process by utilizing AWS Edge locations and the high-speed congestion free AWS Global Network, directing web traffic over the AWS Global Network to endpoints in the nearest region to the customer, so the request spends as little time as possible on the slower public networks. Amazon Route 53 sounds like a highway, and in a sense, it kind of is like a highway, if highways help take you from here to there. It is a highly scalable Domain Name System, or DNS. It allows you to route your users to your internal applications. This could be in form of your users accessing infrastructure running on AWS, like an EC2 instance. Its basic functions are domain registration, domain name system, or DNS, health checking of web applications accessibility, and auto-naming for service discovery. It helps route your users to the appropriate resources you want them to access. AWS's network services mainly occupy themselves with helping you create secure networks for your resources to live within and to route traffic to proper services. If any of these services we talked about needs a bit of a review, feel free to go check out the videos again.

## Artificial Intelligence, Machine Learning and Analytics Services

AI/ML services
- AI/ML buzzwords. We love buzzwords, but we can't really dismiss AI and ML as mere buzzwords because their implications reach all of us who rely on technology day to day. Let's first begin with what AI and ML even stand for. They stand for artificial intelligence and machine learning, which go hand in hand and are often lumped together as AI/ML. AWS helps users find business solutions with AI, add AI to business applications, or choose machine learning models to train or utilize for different purposes. Let's go over some major artificial intelligence and machine learning AWS services: Amazon SageMaker, Amazon Lex, and Amazon Kendra. First up is Amazon SageMaker, which is a machine learning service. It helps you build, train, and deploy machine learning models for whatever use case you may have. You still get to take advantage of the fully managed infrastructure, tools, and workflows to speed up your processes. You may utilize Amazon SageMaker to develop real-world applications, like product recommendation features, robotics, and voice-assisted devices. You may have noticed these conversational chatbots answering your questions, sometimes helpfully, sometimes infuriatingly incorrectly, on many websites recently. These are powered by conversational AI chatbots, and they may very well be powered by Amazon Lex. Amazon Lex is an artificial intelligence service that helps you build bots with sophisticated voice and text conversational AI. Integrate voice assistants and chatbots to provide self-service customer service to your users. Everyone's collecting data, data, data, but then you have to do something with that massive amounts of data. You may have experienced a panic of looking for that one specific photo of that one specific location. Oh, a few years ago in your camera reel, scrolling endlessly trying to locate it. Amazon Kendra utilizes machine learning to help find answers faster with intelligent search. You can imagine being able to input Cafe, Paris, Croissant, River to find that photo of the delicious croissant you had at a cafe next to a river in Paris, instead of getting a bunch of photos of some rivers, some croissants, some in Paris, you get the gist. Amazon Kendra helps customers and employees find the information they need when they need it quickly by utilizing natural language processing, or NLP, which saves a lot of time.

Data analytics services
- Data is like currency these days, and there are many companies all over the globe clamoring to collect as much data as possible. Data helps you sell, data helps you find trends, and then the data makes you the world... Uh, try this again. Data helps you sell, data helps you find trends, and data makes the world go round. And so AWS offers many services that help you then store, move, analyze, and utilize the data collected to fulfill business needs. Some data analytics services that, as the name suggests, helps you analyze loads of data are Amazon Athena, AWS Glue, Amazon Kinesis, and Amazon QuickSight. Have you heard of a Data Lake? A Data Lake is like a lake filled with data. Ha, okay, I kid, but I'm kind of serious. It's a repository that allows you to store all your structured and unstructured data and loads and loads of it. Though you can store your data as is without having to structure it beforehand, you can still run analytics on it to provide data processing, real-time analytics, machine learning, and much more. AWS recommends using Amazon S3, a storage service, as your data lake function, and then to utilize analytics services to help you clean, analyze, and utilize the set data. Amazon Athena allows you to easily analyze data directly in Amazon S3 using standard SQL. AWS Glue helps you integrate data from over 70 diverse data sources and prepare the data for analytics by cleaning it up for further analysis. Amazon Kinesis helps organizations collect, process, and analyze data like audio, video, application logs, and much more in real time at any scale, which you can then derive insights from in minutes. Finally, Amazon QuickSight is a business intelligence service with machine learning integrations that help you quickly build visualizations and perform ad hoc analysis on your data to obtain business insights.

Study break: Reviewing AI/ML and analytics services
- In this section, we learned about a few artificial intelligence and machine learning and data analytics services AWS offers. Let's go through a quick review to jog our memories. AI/ML stands for artificial intelligence and machine learning, and AWS definitely has quite a few services and resources to help you utilize these emerging technologies for your business. From helping you build and train machine learning models to launching customized chat bots for your businesses. We learned about Amazon SageMaker, Amazon Lex, and Amazon Kendra. AWS offers many services that help you store, move, analyze, and utilize data for business purposes. Many of these are data analytics services, helping you, surprise, analyze loads and loads of data. We can collect all the data we want, but if we don't know how to analyze it or look at it, data can be quite useless. A few data analytics services that we learned about were Amazon Athena, AWS Glue, Amazon Kinesis, and Amazon QuickSight. As always, if any of these concepts or services seems like it needs a bit of a refresher, feel free to pause and go back to watch any previous video. See you in the next section.

### Management Tools

AWS CloudFormation
- You've built an awesome system in AWS using many different services in many different settings. Together, they work great, and you'd like to replicate the setup for a new project. Unfortunately, setting it up took days, and you don't exactly remember every step you took. Worse yet, some resources have to be provisioned before others for the system to work. We love recipes when we cook, because they tell us what materials to buy and when to do what for that perfect meal. In tech, we also love recipes. Better yet, we love recipes that cook themselves and present us with the finished dish. That's what Amazon CloudFormation does for your IT infrastructure hosted on the cloud. You create templates like recipes for your resources to be set up a certain way, and you can run it over and over to provision and deploy fully configured infrastructure. Best yet, unlike all those cookbooks filled with your favorite recipes in the bookstores, using CloudFormation is free. You just pay for the resources you use when you run the service like the EC2 instances or S3 bucket storage. With CloudFormation, you can provision anything ranging from a simple EC2 instance to a multi-region, multi-tier application quickly and efficiently using a simple text file written in JSON or YAML. You can update or manage the templates, referred to as stacks, at any point using the AWS Management Console, command line, or software development kit, commonly known as SDK. Basically, you can change up the recipe whenever you see fit, even making different versions for different uses. Version control is always available so you can revert back to previous settings whenever you want. AWS CloudFormation brings to life what is known as infrastructure as code, where you can deploy IT infrastructure based on a text file filled with code that specifies resources and configurations you need for each service you want to deploy. With CloudFormation, you can bring orderly and predictable back into resource deployment, no longer leaving things up to human error or chance.

AWS CloudTrail
- Your AWS IT infrastructure, like any IT infrastructure, needs to be monitored and audited to make sure the resources remain compliant with any government, industry, or company policies. In addition to compliance, the AWS CloudTrail service helps to track user activity and API usage, which allows for operational and risk auditing of your AWS infrastructure. With CloudTrail, you can log and monitor account activities, provide event history of account activities, simplify compliance audits, discover and troubleshoot security and operational issues, provide visibility into user and resource activities, and track and automatically respond to security threats within your AWS infrastructure. For example, you can utilize CloudTrail to automatically respond to security vulnerabilities. You can create a workflow to add a specific policy to an S3 bucket when CloudTrail finds an API call that made the bucket public. You can track many account activities, including actions taken through the AWS Management Console, AWS SDKs and command-line tools. You can review logs using CloudTrail event history, have the reports delivered to S3 buckets, or send reports to CloudWatch logs and events for more granular monitoring of AWS resources. You can view, filter, and download account activities for the most recent 90 days for free. You can also set up a trail that delivers a copy of management events in every region free of charge. However, the data is sent to S3, so you will be charged for storage usage. Data events, which are operations performed on or within the resource itself, also have very small charges. AWS CloudTrail is an invaluable resource in simplifying event security analysis and troubleshooting for your AWS cloud IT infrastructure.

Amazon CloudWatch
- You've built your app or your infrastructure, but now you need to actively monitor it, and collect metrics and react to any events. Unfortunately, you can't be up 24/7 monitoring and neither can your team. Thankfully, Amazon CloudWatch is a monitoring and management system built for developers, system administrators, site reliability engineers, and IT managers. Natively integrated with over 70 AWS services, CloudWatch helps you gain system-wide visibility into resource utilization, application performance, and operational health. It collects monitoring and operational data as logs, metrics and events to provide insight into your application performances. You can collect and track metrics in real time or have it send off notifications when an event occurs. You can even set up CloudWatch alarms to automatically make changes using predefined triggers so you don't have to lift the finger to fix common issues. CloudWatch employs a pay as you go model, so you only pay for what you use with no upfront commitment. Keep tabs on your applications hosted on AWS Cloud with Amazon CloudWatch so you and your teammates can get a good night's sleep instead of holding round the clock monitoring visuals.

Study break: Reviewing management tools
- In this chapter, we went over three of the major management tools in AWS, AWS CloudFormation, AWS CloudTrail, and Amazon CloudWatch. Let's quickly review all of them to make sure we've got the fundamental concepts down before moving on. AWS CloudFormation allowed you to create a recipe for spinning up identical setups for a collection of resources and services for your IT infrastructure. It's free to use and you only pay for the resources you utilize by building a project on CloudFormation. It utilizes infrastructure as code, and you can deploy IT infrastructure based on a text file filled with code that specifies configurations for all of your services and resources. Once that's created, CloudFormation does the actual configurations and deployment for you. You can continue to build out your resources without having to worry about human error and configurations. AWS CloudTrail can log and monitor account activities, provide event history of account activities, simplify compliance audits, discover and troubleshoot security and operational issues, provide visibility into user and resource activities, and track and automatically respond to security threats within your AWS infrastructure. In a nutshell, it's an event tracker and security analysis tool that helps you keep your AWS Cloud infrastructure compliant and secure. Amazon CloudWatch helps you gain system-wide visibility into resource utilization, application performance, and operational health. It collects monitoring and operational data as logs, metrics, and events, and provides insight into your application performance. You can even set up CloudWatch alarms to automatically make changes using predefined triggers to automatically solve common issues. It's integrated with nearly 70 AWS services, helping your team keep comprehensive monitoring data 24/7. Now, you might be thinking, "CloudTrail, CloudWatch, what's the difference?" AWS CloudTrail audits logs. Amazon CloudWatch monitors and can react to changes. Need access logs because someone did something they shouldn't have? CloudTrail. Need to know how much CPU and EC2 instance is using? CloudWatch. Imagine a detective trailing a trail of footprints for CloudTrail. CloudWatch is watching or monitoring to make sure your resources are functioning as they should be. AWS' management tools help you build and manage your AWS Cloud infrastructure. If any of the services we talked about needs a bit of a review, feel free to go check out the videos again.

### Other Services

Not-so-short list of services to keep in mind
- Here's a tiny bit of a doozy that we can't run away from. With the update of the exam from CLF-C01 to CLF-C02, a new task statement was added to the third domain, which is identify services from other in-scope AWS service categories. In this section, AWS listed a bunch of service categories with a few services in each category that were previously largely out of scope for the cloud practitioner exam. Well, there's no point in wallowing in self pity, through. Let's get cracking. The different service categories that are now in-scope are application integration services, business application services, customer engagement services, developer tool services, end user computing services, front end web and mobile services, and IOT services. Services that are in scope for the business application services are Amazon Connect and Amazon Simple Email Service or Amazon SES. Business application services, well, meet business application needs. These services help organizations meet customer needs, providing services like help centers with Amazon Connect or managed email service. Imagine Gmail managed by Google Workspace with Amazon Simple Email Service or Amazon SES. Services that are in scope in the customer engagement services are AWS Activate for Startups, AWS IQ, AWS Managed Services or AMS, and AWS Support. Customer engagement services at AWS engage with their customers throughout their lifecycle to provide the best customer service experience possible. AWS Activate for Startups provides smaller and more advanced startups free tools and resources to get up to speed quickly on AWS including AWS credits, support credits and AWS solution templates tailored to startup use cases. AWS IQ links you up with AWS certified Experts pre-vetted by AWS, providing on-demand access to experts to help you complete projects faster. AWS Managed Services, or AMS, provides infrastructure operations management for your AWS infrastructure with full lifecycle services to provision, run and support your infrastructure. We learn about AWS Support and customer service resources are also a big part of AWS's customer engagement ecosystem with general support services, multiple self-service resources and AWS support plans amongst many other resources to help you ask questions, troubleshoot and help build infrastructure aligned with best practices. Services that are in scope in the developer tool services and capabilities are AWS AppConfig, AWS Cloud9, AWS Cloud Shell, AWS CodeArtifact, AWS CodeBuild, AWS CodeCommit, AWS CodeDeploy, AWS CodePipeline, AWS CodeStar, and AWS X-Ray. Developer tool services and capabilities are what they sound like, tools and resources that allow IT professionals and developers to rapidly and safely and deliver software. These services help you version control your source code and build test and deploy your applications to AWS or your on-premises environment. The services that are in scope in the end-user computing services are AWS AppStream 2.0. Amazon Workspaces and Amazon Workspaces Web. End-user computing services allow employees to securely access desktops and other applications from a variety of devices. You might have heard of or used applications that allow you to remotely access your work computer, sometimes called remote desktops or virtual desktop infrastructure or VDI. Maybe you are especially tech savvy and use remote desktop applications to access your expensive gaming desktop from your laptop to take advantage of the bigger capacity of your desktop while retaining the mobility of a laptop. End-user computing services on AWS help organizations do similar things with their office computers, servers, and applications. The services that are in scope in the front-end web and mobile services are AWS Amplify and AWS AppSync. Front-end web and mobile services on AWS are tools that are built on top of AWS so that you benefit from the reliability of AWS and build web and mobile apps faster with the most well-known service being AWS Amplify. They make it simple to innovate with end-to-end solutions to develop, test, and monitor your applications, as well as scale seamlessly across the globe. Services that are in scope for IOT services are AWS IoT Core and AWS IoT Greengrass. What could IoT services on AWS do? Well surprise, they help you connect and manage IoT devices and data. What is IoT? IoT is short for Internet of Things, which you might recognize if I say things like Amazon Alexa, Tesla, or the Ring Doorbell. They're physical objects that exchange data with other devices and systems of their internet to provide value to users. They might send you notifications as with the Ring Doorbell or allow you to control aspects of your technologies or devices like playing your Spotify playlists or controlling your smart lights with Amazon Alexa. Before sitting in for your exam, I recommend you poke around aws.amazon.com/products to search for the services and categories mentioned in this video to at least familiarize yourself with the names and their basic functions. You shouldn't need to be extremely knowledgeable about it, but at least be able to know the types of service category AWS CodeStar belongs in and what their high level functions could be.

### Conclusion

Study break: Exam tips and resources
- Ah, the most important video for any exam taker, the exam tips video. If you're considering taking the AWS Certified Cloud Practitioner exam, I think this video will be beneficial to help you sort out some of the services we went over. After all, we went over quite a lot of information in such a short period of time. As always, feel free to pause at any point to go back to previous videos and review if you think you need a bit of a refresher. In the AWS Certified Cloud Practitioner exam, there are four domains. They are cloud concepts, security and compliance, cloud technology and services, and billing, pricing, and support. This course mainly focused on the cloud technology and services domain, which makes up 34% of the exam. You'll need to define the AWS global infrastructure, as well as methods of deploying and operating in the AWS cloud. And don't forget the AWS services. And oh boy, there are quite a few services to know. You'll need to be able to identify AWS compute services, database services, network services, storage services, artificial intelligence and machine learning services and analytics services, and services from other in scope AWS service categories. But we've come this far together, so we'll get to the end of the race together. Let's dive in for the review. There are three ways to connect your AWS cloud resources that AWS wants you to know about. Virtual private network, AWS Direct Connect, and the public internet. The number three seems to be a magic number here. There are also three cloud deployment models, cloud or cloud native deployment, hybrid deployment, and on-premises deployment. You can deploy and manage your IT infrastructure in AWS cloud by using AWS management console, the command line interface or CLI, and software development kits or SDKs. The AWS global infrastructure is the backbone of AWS cloud. These are the physical data centers and infrastructure that powered this behemoth of an infrastructure. Availability zones are discrete data centers around the world, and two or more availability zones make up a region. Having your resources in multiple availability zones creates high availability. Edge locations help your customers download your resources quicker by caching data to an edge location physically closest to them. If an AWS region isn't close enough to your end users who require extremely low latency, you can utilize AWS local zones. For applications requiring ultra-low latency user experiences, you can utilize AWS Wavelength Zones that embed AWS compute and storage services within 5G networks. We learned about a few compute services offered by AWS cloud. First up, Amazon Elastic Compute Cloud or Amazon EC2 is a virtual server hosted on AWS cloud with a versatile range of capabilities to meet virtually any use case. Two container services we learned about were Amazon Elastic Container Service or Amazon ECS, which is a fully managed container orchestration service on AWS cloud. Amazon Elastic Kubernetes Service or Amazon EKS is a fully managed service that helps you run Kubernetes on AWS cloud. AWS Elastic Beanstalk helps you deploy and scale web applications by simply uploading your code and Elastic Load Balancing helps your applications achieve fault tolerance by ensuring scalability, performance and security. Two serverless services we learned about are AWS Lambda and AWS Fargate. AWS Lambda allows you to run code without provisioning or managing servers and is an event driven pay as you go compute service. AWS Fargate is a serverless compute engine for containers. Finally, Amazon LightSail provides pre-configured and ready to use operating systems, web applications and development stacks to help get your applications and websites up and running quickly with minimal configurations on your end. AWS offers three different types of storage, object storage, file storage and block storage. Amazon Simple Storage Service or Amazon S3 is an object storage service with robust storage class options to meet your budget and data retrieval needs. Amazon Elastic Block Store or Amazon EBS is a block storage service that acts like unformatted block storage you can mount to your Amazon EC2 instances. The AWS Snow Family is a suite of AWS hybrid cloud services that does a lot of many things including providing edge computing, storage and data transfer services through physical devices. The family is currently made up of AWS Snowcone, AWS Snowball, and AWS Snowmobile. AWS Storage Gateway connects your on-premises storage with AWS's cloud storage, providing hybrid storage solutions for your IT infrastructure. And AWS Backup provides backup service for all your AWS service in hybrid environments at exabyte scale. We learned about three different types of database services available from AWS, relational databases, NoSQL databases and in-memory databases. We also learned about two database migration tools, AWS Database Migration Service and AWS Schema Conversion Tool. AWS's fully managed relational database service is called Amazon Relational Database or Amazon RDS. You can also deploy on-premises with Amazon RDS on AWS Outposts. AWS's NoSQL database service is Amazon DynamoDB and is a fully managed serverless key value database. Finally, some of AWS's in-memory database solutions are Amazon MemoryDB for Redis and Amazon ElastiCache, both of which rely on internal memory to provide microsecond response times. In this course, we learn about four major network services on AWS. Amazon Virtual Private Cloud or Amazon VPC is an isolated corner of AWS cloud made just for you to provision your AWS resources in and a virtual network that you define with complete control over virtual networking environment. Amazon CloudFront is a content delivery network or CDN. The main purpose of CDN is to make websites and applications load faster. AWS Global Accelerator is a cloud toll road allowing your customer requests to take the super high speed AWS global network towards its destination instead of spending time on a slower public networks. Amazon Route 53 is a highly scalable domain name system or DNS. It allows you to route your users to your internal applications. Moving on to AI ML and data analytics services. Let's start with the artificial intelligence and machine learning services. Amazon SageMaker helps you build, train and deploy machine learning models to create real world applications and products. Amazon Lex helps you launch sophisticated voice and text conversational chatbots with AI. Finally, Amazon Kendra helps you find answers quickly with intelligence search powered by machine learning. Now onto data analytics services. Amazon Athena helps you easily analyze data directly in Amazon S3 using standard SQL. AWS Glue helps you integrate data from diverse data sources and prepare them from analytics. Amazon Kinesis helps organizations collect, process and analyze data like audio, video, application logs and much more in real time at any scale. You can then derive insights from them in minutes. Finally, Amazon QuickSight is a business intelligence service that helps you quickly build visualizations and perform ad hoc analysis on your data with machine learning. For AWS management tools, we learned about AWS CloudFormation, AWS CloudTrail and AWS CloudWatch. AWS CloudFormation allows you to create a recipe for automatically spinning up identical setups for a collection of resources and services for your IT infrastructure. AWS CloudTrail can log and monitor account activities, provide event history of account activities, simplify compliance audits, discover and troubleshoot security and operational issues, provide visibility into user and resource activities and track and automatically respond to security threats within your AWS infrastructure. Amazon CloudWatch helps you gain system-wide visibility into resource utilization, application performance and operational health by collecting and monitoring operational data. There was a huge bunch of other in-scope service categories that we just learned about. You don't need to get as much in-depth knowledge of these services, but you do need to be aware of them. These categories are application integration services, business application services, customer engagement services, developer tool services, end user computing services, front-end web and mobile services and IoT services. You can look up any of these and other of-interest services and service categories at aws.amazon.com/products. And wow, that was a lot. I hope this course helped shed some light on many of the core concepts and services AWS Cloud offers us as customers, and the almost limitless potential building and innovating on the cloud. And if you're looking to sit in for the AWS Certified Cloud Practitioner exam, I hope this course, along with its three sibling courses, will help you along in that journey as well. Wishing you the best of luck in your AWS Cloud exploration and, if applicable, on exam taking. See you in the other courses.

Next steps
- Well, that was a lot of information in such a short amount of time. I'm so glad you stuck with me to the end, I hope you not only learned a few things, but enjoyed the process too. If you are interested in learning more about Amazon Web Services and even potentially taking the AWS Certified Cloud Practitioner exam, please check out the rest of the introduction to AWS for Non-Engineer series here at LinkedIn Learning. The courses cover the four domains of the AWS Certified Cloud Practitioner exam, which are Cloud concepts, security, technology, which we refer to as core services, and billing and pricing. If you have questions or want to learn more about Cloud computing and potential careers that work with or in Cloud computing, please come visit Cloud Newbies, a community of Cloud Newbies and season pros where we learn about Cloud computing and study for certifications together. You can visit us at cloudnewbies.com. If you are looking for a resource website while you're beginning your research into Amazon Web Services, you can visit me at awsnewbies.com where I introduce Cloud computing and AWS in a jargon free way. Thanks again for watching, and I hope to see you again in one of my other courses or resources. Good luck.
