https://www.linkedin.com/learning/cloud-native-projects-aws-serverless

Flexibility with Lambda
- If you work with cloud native technologies, you cannot escape hearing about serverless. The concept of paying for the operations you use versus the infrastructure at 24 hours a day, seven days a week, appeals in many specific use cases in software engineering today. Many people talk about serverless. However, not everyone has a clear picture of what it truly is. This course will not only explain what serverless is but also answer the benefits and negatives of this development toolkit. We will look at specific use cases for AWS's implementation of serverless, called Lambda, and how to leverage this with several languages, to not only help you make appropriate decisions on how but also when to use this technology. Hi, I'm Frank Moley. I'm a cloud-native developer, architect, and manager working in both public and private clouds throughout my career. I have consumed and even helped build serverless technologies. I am passionate about technology and sharing that technology with others. So let's get ready to learn about cloud native challenges, focusing on AWS Lambda as a serverless technology.

What you need to know
- There is some initial setup on your machine that is needed in order to be successful in this course. First and foremost, let's talk about the language support that we're going to use throughout this course. All of these are OS-dependent installations. And in order to install them on your machine, whether it's Linux or Windows, you're going to want to consult the documentation of that language for your operating system. We're going to use Java, the JDK and specifically, you're going to use 14. Any of them after 11 should work for this course. And you're also going to need Maven to handle some dependency management because that's how I'm going to set up the project. We're also going to use Golang. I'm going to be using 1.14 but you can use any modern version of Go that is compatible with AWS Lambda. And finally, we're going to use Python. I'm going to use the 3.8 interpreter. Use any of the dot versions of Python 3 that you wish to use with AWS Lambda. Again, ensure that there is support within AWS. But they're all going to be specific to your operating system, so pay attention to that. Now, an IDE is something that many developers use on any given day. And while I'm going to use very specific IDEs, the IDE itself is not important and I don't want you to get mired in the IDE selection itself. You really should use an IDE that is comfortable for you. And that really brings us to the point of knowing your IDE. Whether it's within this course or anything within development, understanding your IDE is critical to making you most performant in your day-to-day activities. So if you choose to use any IDE other than the one I'm using, that's perfectly fine and that's what you should do, especially if it's an IDE that you know. Now, I'm going to recommend that you use language-specific IDEs or plugins, so when we're doing Java work, choose an IDE that is specifically for Java or has a plugin for that. The same way with Go or Python. Now, I'm going to reiterate this because oftentimes, this gets confusing to people. I'm going to be using JetBrains products. What is important is that you are familiar with your IDE, not that you're using the same one that I am using. Familiarity with the IDE is the most important aspect of choosing an IDE for not only success in this course but your day-to-day activities. Now, we will be using the command line. And you need to understand the command line for your operating system. Now, understanding command line is very important to being successful as a developer in my opinion. I use macOS X and as such, I often use the Z shell within macOS X to do operations. Now, you may not have Z shell, you may be using Bash in Linux. That's perfectly fine. As long as you know your command line. And the same will transpire for Windows. Its command line has options to either use Bash if you're on Windows 10 Pro or other command line technology that will allow you to accomplish the same things. Now, if you're in a Windows environment, you may have to look some stuff up on the Amazon website but what is important is that you know your command line for your operating system. Now, again, if you're on Windows, you may need a little bit more in-depth knowledge when targeting Linux backends. And oftentimes when you package your Lambda for AWS, this becomes critical. The good thing is AWS has provided a lot of the documentation, and much of it can be found very easily with how to use Windows and install to a Linux backend through AWS. And you're going to need to know your toolkits, specifically the AWS CLI that is a Python program that runs on your operating system. Now, AWS itself. We're going to be using Amazon Web Services and Lambda functionality in order to use this course. First and foremost, you're going to need an account. The good thing is you can get an account for free and all of the examples that we're going to do within this course should be able to leverage a free account. Now, you will need to know the basics of AWS. You don't need a lot of in-depth knowledge but you need to know how to get around in the console, and understand some of the various parts that we're going to target. More importantly than knowing AWS is be able to read their documentation. AWS has compiled an amazing set of documentation and I'm going to encourage you to leverage it often if you get a little bit confused about the direction that we're going with AWS Lambda.

Introducing AWS Lambda
- Lambda is a compute service from Amazon AWS that allows you to run code called Lambda functions without provisioning or managing virtual servers. This runtime is scalable and can be multi-tenant without you having to manage that aspect of the environment. AWS Lambda functions offer a robust set of language support, making this technology more open for many users. AWS Lambda natively supports some of the most popular languages used in cloud native computing today, including Go, Java, and Python that we'll look at in this course. There is also native support for some other very popular languages like Ruby, C#, Node.js, and even PowerShell. In addition, Lambda has a runtime API that you can use for languages that are not natively supported to keep with the polyglot theme. Now one of the beauties of Lambda, in my opinion, is the hooks to the Lambda framework are simple. There are no complex APIs to learn or deal with for very simple or even more complex executions within the framework. Lambda as a serverless technology in the AWS ecosystem plays very nicely with other AWS offerings. Lambda can be used to process data or binary files through simple triggers in AWS. Changes in S3, RDS, or Dynamo, for instance, can trigger a Lambda function to do some meaningful work. Lambda can also be used to respond to web requests through API Gateways or CloudFront acting as web applications or web services. Data streams are a powerful option in asynchronous processing through integrations with SQS or Kinesis. And of course, we wouldn't be talking about AWS if we didn't mention IoT. And IoT is another powerful integration point with things like Alexa Skills or IoT, Iot Events, within AWS, and that isn't it. You can integrate Lambda with other Lambda functions and various other AWS services. Now AWS has provided many operational tools around Lambda to make it a truly effective system for application development. Out of the box, the console itself provides base but rich monitoring of your Lambda executions. You can also build customized dashboards if your operational needs demand it. Lambda comes with a testing framework, REI, that can provide a way to test your serverless functions without doing deployments. And when it comes time to deploy, Lambda provides many different paths, some of which we will look at through this course, including console command line and container deployments if that fits your needs. Now that we have seen what Lambda offers, let's take a look at Serverless and what it offers as a technology to developers.

Understanding serverless
- Before we get too deep, let's take a look at what serverless is and how it operates at a conceptual level. Now, many times you may hear the word serverless, and immediately think, well, there's no server. Well, that's not true. There obviously is a server. The difference here is that you are paying for operations, not for the server itself. And there's a reason for this. This allows the cloud provider to aggregate a whole bunch of functions instead of you dedicating that VM hardware for downtime. So again, you are not paying to run a VM, you're paying for the operations. But there is still a server, even though this technology name is serverless. Another aspect is you are not managing the runtime of VM. Now, even in cloud where the VM is controlled by cloud infrastructure, you still have maintenance of a running VM, whatever form of compute it is, so that is where the benefit comes from. It's not only are you not paying 24 hours a day, 7 days a week for the VM, but you're not managing it, dealing with upgrades, dealing with nodes dying, dealing with any of the other aspects that come with management of the VM, even in cloud infrastructure. Now, this may get a little bit confusing as to why this is offered. And the key here is that it provides maximum utilization of the compute time itself. A cloud provider can take and pair cyclical workloads, such that the maximum capacity of the VM is achieved throughout the entire day, as opposed to you owning the VM, where it's only achieving maximum throughput at given times of the day. Now, this allows the provider to leverage many workloads on a group of VMs instead of you getting the leverage of that group yourself. This allows them to maximize the value again and do so in a way that is cost effective for you and for them. Now, there are many, many workloads that this applies to that make it very attractive. Now I want to take a look at this concept of cyclical workloads. Now, I'm going to give you a very exaggerated view of a cyclical workload, and I have never seen a real one that is this cyclical, but I have seen workloads that mimic this. It's just not a pure sine wave, which is what I'm going to provide. Now, the idea here is that we have a single process that is running, and you'll see throughout the day, throughout the week, throughout the year, that you have peak times and valleys within this workload. Now again, this varies by provider and it varies by situation, but you'll see these peaks in the valleys throughout the life cycle of an application running on a single VM. Now, in the serverless world, we can take and add another cyclical workload on this series of VMs. And what you see here is that even though we still have peaks in the valleys of a single flow, we've now filled those peaks and valleys with another process such that the peak of one may mimic the valley of another, and we're getting more utilization on this VM pool. Now, if we add yet another workflow, you will see that we've reduced the time where we're not under peak workload and we've reduced the time where we're not under a valley to be as small as possible. Now, obviously, a VM is not going to just run three serverless functions. Ideally it's going to run hundreds or thousands in a pool of VMs and this allows them to get maximum throughput across the workload the entire time. Now, you may step back and question this whole idea of cyclical workloads, but I encourage you to take a look at what actually happens on your services. If you run chron operations, you'll see that they peak and then they go dormant for a period of time. You can see this same utilization across industries. Look at something like e-commerce, where during the holiday shopping season, you see a ramp up in technology usage and then a ramp down after the holiday season is over. And what often happens, in a traditional environment where you're running VMs is that you will actually scale your entire infrastructure for the peaks, so that you can handle the peak workload. And the rest of the time when you're in the valley it becomes wasted space. So this is where serverless can start to offer some value as we maximize the utilization on peaks and decrease the utilization on these valleys.

Benefits of serverless
- We have now seen how serverless works at a very high level, so let's take a look at why one would want to do this. First and foremost is cost savings. With serverless, you get the ability to scale based on your need, but instead of scaling for your highest demand, you're scaling by the operation and you're paying for just the operation. And by doing so, you are not paying for your peak usage throughout the year as I previously stated. You also get benefits from reduced maintenance. So even though a VM may be in the cloud, you still have maintenance that you have to do on it. You've got to upgrade your VM, you've got to patch components and with serverless, you simply deploy your code. You don't do any of those maintenance activities on your VM itself. Now, separation is another key benefit of using serverless. There is no longer a need to cram disjointed operations into a single VM in order to maximize its value. You get some security and permissions benefits as well because usually with serverless, these functions are operating in their own little sandbox and that sandbox has its own security and its own permissions associated with it, and you're not crossing threads with other functions that may be operating in the same VM pool. Again, something that you don't get when you're trying to cram multiple operations into a single VM pool itself. Another benefit of this separation is it solves many of the async patterns that we have by reducing the waste associated with them. Let me give you an example. Traditionally, when we write a message listener, we run that on a server that is actively pulling against the messaging system to determine if there is a message to act on. And in doing so, it's using resources, but there's a lot of waste associated with that. With serverless, we actually trigger the serverless function based on the presence of a message, and this again reduces our waste, but allows us a just in time operation that reduces overall cost since we're only paying for the operation itself. Now, let's talk a little bit more about asynchronous operations, because most, if not all serverless functions tend to end up being some form of an asynchronous process. Even if it's running a website, a lot of times we will front that with some sort of CDN that actually makes the operation itself async. Now, not always, but async is a very strong pattern and it's worth talking about. The serverless functions can be a key player in asynchronous operations, especially as you move into a public cloud ecosystem. Again, it reduces the waste of waiting. That waiting operation is nothing but a wasteful event when you're dealing with operations of scale on a VM consumption model. It's also much easier to isolate and respond in a serverless function type behavior because each listener will respond based on a function itself as opposed to a listener pool that responds to many different queues. It also offers a lot of flexibility in that you can scale multiple operations within a serverless function by chaining them together instead of tying them all to individual listeners. A lot of times we would do things in async where the first listener would do some operation and put a message back on a queue so that another listener could pick it up. Well, with serverless functions, we can simply trigger the next in the chain without the intermediate cue that was really a fancy way of doing piping across a large scale operation. Now, let's talk about some of the ilities that we get to solve with serverless functions. The first is scalability. Because we're dealing with an individual function and we're paying for the operation of that function, we can have as many of them running at any given point in time that we need to, and each of them is usually within the cloud provider isolated into its own run time and own sandbox, even if it's the same function running 10, 20, 30 times in a given period of time. Serverless functions also give us maintainability. Because each distinct function only does one task, we're not deploying a pool of operations to do many different tasks. This increases our maintainability because when we have to modify an individual function, we're only impacting that function, of course, reducing the risk associated with regressions across a pool of operations. Serverless functions tend to be very easy to deploy, especially within cloud operations. Because of their ease in deployability and maintainability, they give us a speed to market function that comes along with it, and that's another ability that kind of comes behind the scenes from maintainability and deployability. And obviously with serverless functions, you get a lot of flexibility. You can do just about anything you can imagine in a serverless function, and by only paying for the operation, you get all of the benefits without a lot of the pain that is associated with deploying large scale operations to handle a multitude of single functions.

Downfalls of serverless
- As with all technology, there is no silver bullet. Serverless has some risks that need to be evaluated before making the jump to it as a technology choice for your institution. First and foremost, we need to talk about cost. Now, heavy workloads in a serverless environment can actually cost more than running a VM even when that VM has those peaks and valleys that we talked about. It's hard sometimes to get a true picture of cost effectiveness in a serverless model because of this. Now, designing with serverless may hide some efficiencies that you could gain from a cost perspective. So let's take a look at the example of where we're running multiple serverless functions against a single trigger. Could we achieve the same thing by fanning that trigger out and actually reducing our costs in a traditional VM environment? Another risk that comes with serverless is the inability to do proper debugging. Now, I'm not saying it's impossible, but it sometimes becomes very difficult to debug operations, especially when those serverless functions are running in a cloud environment. You cannot simply connect to running servers many times, which makes it difficult to do real time debugging against a running process. Now, when you start chaining serverless functions, that debugging becomes even more complex because now we have serverless functions that trigger other functions and we now have to figure out how to get output and debug against multiple functions at one time. I often find that logging is the most effective way to do debugging, which is kind of an old school way of doing debugging, but sometimes it's your only choice. By putting little out statements that say look here, or I'm here, we can get a better indication of where we're at by reducing the complexity associated with connecting to debugging. But of course, every time you're doing that, you have to trigger a function, which then costs you more money. Now, another hard part is testing. Testing can be a struggle with serverless, and that's simply because the emulators from vendors that help aren't an exact replica of what happens in a real run time. In addition, sometimes you run against serverless technologies that don't have an emulator at all, so it's almost impossible to test against them in a live environment. It also can be very hard to integration test serverless functions because you have to be able to run them in your environment, but that environment isn't something that you run. It runs in a cloud provider. So that adds to the complexity of actually effectively testing it, and oftentimes you end up doing mocks around those serverless functions so that you don't have to deal with the cloud provider itself. But of course, those are mocks and not true tests of the running system as a whole. Now, one of the biggest problems that I've seen with serverless functions is this concept of sprawl. As you start to write more and more functions, your function base grows, and as with any growth, individual elements become harder to control when you're running multiple, especially when you start interacting with triggers from the cloud provider or from various technologies and chaining events. That sprawl can be almost unimaginable to deal with when you first start running serverless functions. And again, triggers and chains just expand this pain beyond belief because now you have different ways of calling functions. It's not uncommon for a function to have two or three triggers or two or three chains or sometimes a mixture of them, and that complexity that gets added to your environment makes maintaining that that much more difficult. Now, I don't want to discourage you from serverless. I think it's a fantastic technology if used in the proper use case. So let's take a look at some of the use cases next.

Serverless use cases
- So let's take a look at a couple of the use cases for serverless technologies. Autoscaling web apps is definitely a very niche proposal within serverless for many people, but it is a use case that can be leveraged. Autoscaling is the primary benefit of this. By building a web application that responds to a traditional HTTP request, you get autoscaling essentially for free with the serverless footprint on many cloud providers. Therefore, as your request volume increases serverless can really start to shine as you don't have to build your app per se to be fault tolerant and resilient to the scaling need, because serverless takes care of much of that for you. Another benefit here is that there's minimal initial setup to get this scaling. You simply write your web application and then as it becomes more and more popular, serverless will handle the scaling needs of your application. Now, I do want to point out, once again, that you need to be careful as sustained volume increases. If you find that your application becomes so popular that it's used constantly throughout the day at a high volume, you may actually start to get benefits from moving away from serverless into a more traditional VM environment. Now, event streaming is a use case that I most often use for serverless in a cloud native world. Much of event streaming is based on asynchronous actions and we've already talked about how serverless can really shine in asynchronous actions. Events through your streaming model can trigger these functions to operate and manipulate data as needed. As always, with serverless, you get built in scalability, because of the way that the framework operates. Now, this can be very useful in observability, processing, and transformation of that data. In fact, a large majority of the use cases that I have used serverless technologies for, wrap around the use of observability, specifically, around security patterns within an application space. Now, be careful with this use case, as sprawl and dependent triggers starts to increase, it can make your serverless model that much more complex. Now, file manipulation is another one of those use cases that just naturally works. Anytime you upload or manipulate files, videos, or images, those can be triggers in many cloud providers naturally for serverless functions. Many times these occur on save or delete actions, but they could also occur with moves, because those are just saves and deletes in many patterns. Again, we're dealing with asynchronous processing. As we save the file, this trigger occurs after that point, therefore it becomes an asynchronous action. You can also use this in many use cases where you're pushing data to live data stores based on some action. Again, it's giving you this opportunity to do something after the fact and push data, or manipulate data as you need it. Now, once again, be careful here when using this use case. Intense processing needs can start to overrun the serverless benefits. And of course there's debugging, which often has to happen with these large complex tasks that tend to get associated with file manipulation, especially around videos and imaging. Now, another one of the primary use cases are around connectors within a cloud native environment. Cloud providers often provide connectors to serverless functions. These give you remote system operations, not just within your cloud provider, but truly with SaaS models where you need to execute some action based on a remote system trigger. These third party operations, in addition to leveraging the benefits of the cloud provider, can give you an asynchronous gate, but one that's air gaped away from your primary run time. And serverless tends to fit really well in this model. These disjointed applications don't really need live connectivity, but need a way to communicate with each other over some sort of a triggered mechanism. And I've said this word triggered a lot, because a lot of times that's how we think of serverless functions. Some trigger execute some function, and it continues down this path. And we see this a lot with third party operations, where we're interacting with the SaaS provider and either they're setting us event or we're setting them an event, and at some point these serverless functions tend to fit the bill. These SaaS models are all over the place, things like Salesforce, and things like Stripe and Split that are very prevalent within the ecosystem allow you to subscribe and also push triggers to their systems. Now, one thing to be careful, and something that's gotten me in the past, is that some of these third party products, especially in the SaaS space, can be very noisy and push lots of events, because they tend to err on the side of caution, send you too much data, and you deal with what you want. In doing so, you can actually get a lot of processing that's really nothing more than waste, so you do want to be careful on how much you allow them to impact your costs through these remote connectors.

Lambda with Python
- So it's time to start our code journey with Lambda and we're going to do that by using the Python language. What I'd like you to do is to log in to your AWS Management Console and let's go to Lambda. Once Lambda loads, you'll be presented with one of the dashboards, either the Raw Functions or the Dashboard itself. Let's take a look real quickly at the Dashboard itself. You'll see that it gives us a listing of the number of Lambda functions that we're running, how much concurrence do we have, how much is unreserved, and you'll see all of our metrics about our functions themselves. So let's jump in and we're going to create a new function. Now we're going to do this from scratch, and we will call this LinkedIn Learning Demo, and we will select Python 3.8 as the interpreter. And for the purposes of this demo, we don't need to worry about any of the permission changes or any of the advanced settings, we'll allow it to create default. So go ahead and click Create Function. This may take a little bit of time, but what you'll see is we're brought into the Dashboard of our existing function that we just created. Now with the Python language, you're going to get a code editor inline with the function itself. So let's go ahead and jump into the code editor that is presented as part of the AWS Console. And we're going to click on our Lambda function. Now you'll see here that they've given us the basic skeleton. All Python Lambda function should follow a skeleton similar to this where we've declared our function name, we accept an event and any context, we then have a to-do section, and ultimately we'll return a 200, if everything is successful, and, if not, an appropriate status code. So for this example, let's remove the to-do, and we're going to put in some very, very simple functions here. So we're going to do a print, and we are going to print our request using JSON.dumps, and the event itself. Now what this will do is this will take our request, and actually dump it to the screen. So let's go ahead and run just a little bit of code here, so we're going to do I equals zero, we'll do a while loop, while I is less than event, and we're going to pull from that body a count, and once we are in there, we will simply print the event message. And then of course we're going to increment I, so we'll do an I plus equals one, and we'll give it a carriage return. And that's all we actually need to do to implement a Lambda. Now, obviously this is a very, very simplistic example, and that's fine for the purposes of this course, because what's important here is you've created a Lambda. We can test this and we're going to do that in the next video. We can execute against it. We can tie a trigger to some other event in AWS that would allow this Lambda to be executed. All of the logic is really depending on your business cases. This is all you need to do, outside of implementing your core logic, to get a Lambda running using the Python language. It really is that simple. So stay tuned next and I'll show you how to test this.

Deploying and running a Python Lambda function
- In our last video, we created a very simple, yet ultimately powerful lambda function that allowed us to iterate over some request data and spit it out to the console. So now let's take a look at how to run that lambda in the absence of some external trigger. And this is really a great way for you to test your lambdas within the console if you don't have access to any of Amazon's testing tools that you can install locally on your machine. So let's navigate back to the AWS Management Console, and we're going to click on lambda and we'll go into our lambda functions and we'll click our LinkedIn Learning demo. And you'll see, that we have our function that we created earlier. Now, from right here, we can click the test function. Now what this allows us to do is we can create a new test event or edit an existing saved test event. We'll go ahead and create a new one. The event template doesn't really matter because all we're doing is giving it some sort of a structure and all we need is a JSON payload for this test. So we'll name this testinglil, and we will replace all of this JSON payload. So if you remember, our function took a message, and for this message, we will simply do, "Hello LinkedIn Learning". And we're also going to send it a count and we'll do it five times. Now, make sure your JSON is structured correctly and now we will simply create it. So now that we're back at our code source, you'll see that there's a warning here that says change is not deployed. We need to actually commit our code to our lambda and we can do that within the console by simply clicking deploy. So now that our changes have been deployed, we can run this function. So let's select our testinglil and run it. And what you see here is we get an execution result based on our function itself. So the first thing you'll notice here, in the first few lines, is we got the response payload that came back from our lambda and that was something that was defaulted in from the code editor in AWS. But you'll also notice that we get all of our log output. And you'll see the log messages start with our request, because we asked it to spit our request out. And that request came in just like we entered it, as part of the test, as we might expect. But then we get into the loop that we created, and that loop executed five times, and it printed out, "Hello LinkedIn Learning". You'll see the next section here of our execution results actually list the function logs, and this is the log output that we told our application to produce. So we start with getting the environment variables that are passed by default to our lambda, and in this case, it's the request id. Then we start into the log messaging that we produced. You'll see that we have the request that we asked it to spit out, and then we have a loop of five times that we see that message and that's based on the code that we produced. Then you'll see that we get an in message on that request and then we get a final report. And it tells us about the duration, how much we were billed, and the memory size that we consumed. Now a lot of this stuff is configurable, but within this case you'll see we have 128 megabytes. The maximum we use for interpreter, listed up here towards the top was 51. We're in good shape. Now, as you're building bigger and bigger lambdas, you can use this test data to actually size your lambda appropriately, knowing that the larger the size, the larger the cost that you may be paying. But for the purposes of what we've just done, we've written a lambda function, and we did so in the embedded console. We executed that lambda function after deploying it, and now we can analyze the results. Now the sky's the limits from here. One thing I will tell you from experience is make sure in real systems that you use structured logging. Instead of just printing out data to the console, you're going to want to go ahead and create some form of structured logging. It will make it easier to troubleshoot live production systems.

Challenge: Python
- Now it's time for our first challenge exercise for this course. We're going to be using the Python language for this challenge and you're going to create a function that actually does some work and use it as a Lambda function that you deploy. Now, part of the steps that I'm going to ask you to implement as part of this challenge are as follow. So I want you this time to start with the local Python file instead of the code editor from AWS Console. Now, there's nothing wrong with the code editor, it's just a little bit more realistic to store your code in source control and to do that locally as opposed to doing it in their code editor. Now, in order to actually upload your zip file that you're going to create, you need to follow the instructions linked here that will tell you how to create a role in your AWS account. Now, I'm assuming that you have AWS set up with credentials. If you do not, please stop and go do that first. But what you're going to do is you're going to create a zip file after you create a role, and then you're going to upload that zip file using the CLI. You're going to create a coin-flip program. The coin-flip program is going to input a number from the JSON payload for the number of flips to execute. You will then randomize those results and generate either a heads or a tails based on that randomization. And then at the end, in the actual response for the Python, I want you to spit out the number of flips and the results of those flips themselves. So in addition to doing any logging that you want to do, I actually want you to turn this into a response that you sent outbound. So you're on your own. On the next video, I'm going to show you my solution to this problem.

Solution: Python
- And now it's time for me to show you my solution to this challenge exercise. Now I am going to walk you through the actual uploading steps that I took when I actually tested this program so that you can match it to what you did. So you'll see at this point, I have deleted the Python module that I created, and I just have that original demo version and that's on purpose. Again, I want to show you what goes on, and you cannot actually create a module, again, that's already been created. Okay, so here's how I implemented my Python function. I imported json and randon, I created the actual handler. I set the count equal to the event count. I set heads equal to zero and then tails equal to zero. Then I went through a range function, just another for loop within Python and I created a random integer with zero or one as my seed values, and then I flipped. So if I flipped it and got a zero, I considered that heads, I went ahead and printed that out. I incremented my heads count at that point. If I got one, I considered that tails, incremented the tails count after printing it out. Then at the end, I set a dictionary equal to my results, which had count, which was the original count, heads and tails, and then I output that as a return statement. So let's jump into the console here so I can show you how to upload this and then test this through the CLI. So the first thing that I'm going to do is I'm going to create a zip called chal.zip, and I'm going to put into it, my Python challenge actual file. So now that my zip is created and this is all predicated on two key aspects. First of all, that my AWS config is set. And second, that I have created a role in my account according to the instructions from AWS. All right, so now I'm going to do an AWS command using the CLI, lambda create-function, - -function-name cointoss --zip-file will be fileb://chal.zip. That's what I named my zip file. Now the handler is going to be python chal.lambda_handler, and this comes from the name of my file and then the lambda handler itself. Now I'm going to set my run time to Python 3.8 and the role is going to be the role that I created. Now, that role that I created is based on that example documentation that I gave you. So I need to put in the ARN for that role. Now there's a couple ways you can get this ARN. One is to jump into the console and go to IAM, go to rolls, select the roll, and you can copy the roll ARN here, or it follows a pattern and that pattern embeds your account number. So now that I've copied that, let's paste that here. And I need to pass a profile because I actually run multiple AWS accounts off this machine and I have a profile set up specifically for this course. And now we get the output of this. Output tells us all about the ARN of our function and some details about it. Now, one of the cool things about the AWS CLI is we actually can test this right here. So let's go ahead and do that. AWS lambda invoke, and again, I'm going to set my profile equal to LinkedIn Learning or LIL is how I've called it, pass it the function name, which in this case is cointoss. And now let's send it a payload. We'll do a single tick, curly brace, double quote, count, 10, curly brace, single tick. Now I'm using the version two of the CLI so I need to pass in cli-binary-format raw-in-base64-out. If you are not using CLI two, you don't need to do that. And I'm going to pass it a file to put my output into. So you'll see I got a status code of 200 and if I cat that file out, you'll see that I get the JSON body itself. And in this case, I flipped it 10 times and I got exactly five heads and five tails. If I run that same element again, and then look at the output, this time I got six heads and four tails. Now the final thing I want to show you is back into the console. Let's go back to Lambda. There's my function defined. You'll see that it's deployed. I can pull the code up in here because it's Python. This doesn't work for every language, but Python as we've seen, does have the ability to use the code editor in line. And I can test it right here, much the same way that I did from the console. So again, it's really your choice on how to test these things. I find that I work better in the console so I use that functionality more often than not. But again, your mileage may vary. You use what makes sense to you. Go ahead and create a body and now test it. And this time, I got seven heads and eight tails. So relatively simple program, but it does give you a feel for the way you would do this in a little bit more professional manner by creating the code on your machine itself, uploading it to AWS, and then executing it there. Now in the real world, I don't use zip files for this. I actually build container images, store them in ECR and then pull them out of there but that's outside the scope of this course. What's really important here is that we can package it to zip, upload it and move forward.
