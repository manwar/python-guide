https://www.linkedin.com/learning/cloud-native-projects-aws-serverless

Flexibility with Lambda
Selecting transcript lines in this section will navigate to timestamp in the video
- [Frank] If you work with cloud native technologies, you cannot escape hearing about serverless. The concept of paying for the operations you use versus the infrastructure at 24 hours a day, seven days a week, appeals in many specific use cases in software engineering today. Many people talk about serverless. However, not everyone has a clear picture of what it truly is. This course will not only explain what serverless is but also answer the benefits and negatives of this development toolkit. We will look at specific use cases for AWS's implementation of serverless, called Lambda, and how to leverage this with several languages, to not only help you make appropriate decisions on how but also when to use this technology. Hi, I'm Frank Moley. I'm a cloud-native developer, architect, and manager working in both public and private clouds throughout my career. I have consumed and even helped build serverless technologies. I am passionate about technology and sharing that technology with others. So let's get ready to learn about cloud native challenges, focusing on AWS Lambda as a serverless technology.

What you need to know
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] There is some initial setup on your machine that is needed in order to be successful in this course. First and foremost, let's talk about the language support that we're going to use throughout this course. All of these are OS-dependent installations. And in order to install them on your machine, whether it's Linux or Windows, you're going to want to consult the documentation of that language for your operating system. We're going to use Java, the JDK and specifically, you're going to use 14. Any of them after 11 should work for this course. And you're also going to need Maven to handle some dependency management because that's how I'm going to set up the project. We're also going to use Golang. I'm going to be using 1.14 but you can use any modern version of Go that is compatible with AWS Lambda. And finally, we're going to use Python. I'm going to use the 3.8 interpreter. Use any of the dot versions of Python 3 that you wish to use with AWS Lambda. Again, ensure that there is support within AWS. But they're all going to be specific to your operating system, so pay attention to that. Now, an IDE is something that many developers use on any given day. And while I'm going to use very specific IDEs, the IDE itself is not important and I don't want you to get mired in the IDE selection itself. You really should use an IDE that is comfortable for you. And that really brings us to the point of knowing your IDE. Whether it's within this course or anything within development, understanding your IDE is critical to making you most performant in your day-to-day activities. So if you choose to use any IDE other than the one I'm using, that's perfectly fine and that's what you should do, especially if it's an IDE that you know. Now, I'm going to recommend that you use language-specific IDEs or plugins, so when we're doing Java work, choose an IDE that is specifically for Java or has a plugin for that. The same way with Go or Python. Now, I'm going to reiterate this because oftentimes, this gets confusing to people. I'm going to be using JetBrains products. What is important is that you are familiar with your IDE, not that you're using the same one that I am using. Familiarity with the IDE is the most important aspect of choosing an IDE for not only success in this course but your day-to-day activities. Now, we will be using the command line. And you need to understand the command line for your operating system. Now, understanding command line is very important to being successful as a developer in my opinion. I use macOS X and as such, I often use the Z shell within macOS X to do operations. Now, you may not have Z shell, you may be using Bash in Linux. That's perfectly fine. As long as you know your command line. And the same will transpire for Windows. Its command line has options to either use Bash if you're on Windows 10 Pro or other command line technology that will allow you to accomplish the same things. Now, if you're in a Windows environment, you may have to look some stuff up on the Amazon website but what is important is that you know your command line for your operating system. Now, again, if you're on Windows, you may need a little bit more in-depth knowledge when targeting Linux backends. And oftentimes when you package your Lambda for AWS, this becomes critical. The good thing is AWS has provided a lot of the documentation, and much of it can be found very easily with how to use Windows and install to a Linux backend through AWS. And you're going to need to know your toolkits, specifically the AWS CLI that is a Python program that runs on your operating system. Now, AWS itself. We're going to be using Amazon Web Services and Lambda functionality in order to use this course. First and foremost, you're going to need an account. The good thing is you can get an account for free and all of the examples that we're going to do within this course should be able to leverage a free account. Now, you will need to know the basics of AWS. You don't need a lot of in-depth knowledge but you need to know how to get around in the console, and understand some of the various parts that we're going to target. More importantly than knowing AWS is be able to read their documentation. AWS has compiled an amazing set of documentation and I'm going to encourage you to leverage it often if you get a little bit confused about the direction that we're going with AWS Lambda.

Introducing AWS Lambda
Selecting transcript lines in this section will navigate to timestamp in the video
- [Narrator] Lambda is a compute service from Amazon AWS that allows you to run code called Lambda functions without provisioning or managing virtual servers. This runtime is scalable and can be multi-tenant without you having to manage that aspect of the environment. AWS Lambda functions offer a robust set of language support, making this technology more open for many users. AWS Lambda natively supports some of the most popular languages used in cloud native computing today, including Go, Java, and Python that we'll look at in this course. There is also native support for some other very popular languages like Ruby, C#, Node.js, and even PowerShell. In addition, Lambda has a runtime API that you can use for languages that are not natively supported to keep with the polyglot theme. Now one of the beauties of Lambda, in my opinion, is the hooks to the Lambda framework are simple. There are no complex APIs to learn or deal with for very simple or even more complex executions within the framework. Lambda as a serverless technology in the AWS ecosystem plays very nicely with other AWS offerings. Lambda can be used to process data or binary files through simple triggers in AWS. Changes in S3, RDS, or Dynamo, for instance, can trigger a Lambda function to do some meaningful work. Lambda can also be used to respond to web requests through API Gateways or CloudFront acting as web applications or web services. Data streams are a powerful option in asynchronous processing through integrations with SQS or Kinesis. And of course, we wouldn't be talking about AWS if we didn't mention IoT. And IoT is another powerful integration point with things like Alexa Skills or IoT, Iot Events, within AWS, and that isn't it. You can integrate Lambda with other Lambda functions and various other AWS services. Now AWS has provided many operational tools around Lambda to make it a truly effective system for application development. Out of the box, the console itself provides base but rich monitoring of your Lambda executions. You can also build customized dashboards if your operational needs demand it. Lambda comes with a testing framework, REI, that can provide a way to test your serverless functions without doing deployments. And when it comes time to deploy, Lambda provides many different paths, some of which we will look at through this course, including console command line and container deployments if that fits your needs. Now that we have seen what Lambda offers, let's take a look at Serverless and what it offers as a technology to developers.

Understanding serverless
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] Before we get too deep, let's take a look at what serverless is and how it operates at a conceptual level. Now, many times you may hear the word serverless, and immediately think, well, there's no server. Well, that's not true. There obviously is a server. The difference here is that you are paying for operations, not for the server itself. And there's a reason for this. This allows the cloud provider to aggregate a whole bunch of functions instead of you dedicating that VM hardware for downtime. So again, you are not paying to run a VM, you're paying for the operations. But there is still a server, even though this technology name is serverless. Another aspect is you are not managing the runtime of VM. Now, even in cloud where the VM is controlled by cloud infrastructure, you still have maintenance of a running VM, whatever form of compute it is, so that is where the benefit comes from. It's not only are you not paying 24 hours a day, 7 days a week for the VM, but you're not managing it, dealing with upgrades, dealing with nodes dying, dealing with any of the other aspects that come with management of the VM, even in cloud infrastructure. Now, this may get a little bit confusing as to why this is offered. And the key here is that it provides maximum utilization of the compute time itself. A cloud provider can take and pair cyclical workloads, such that the maximum capacity of the VM is achieved throughout the entire day, as opposed to you owning the VM, where it's only achieving maximum throughput at given times of the day. Now, this allows the provider to leverage many workloads on a group of VMs instead of you getting the leverage of that group yourself. This allows them to maximize the value again and do so in a way that is cost effective for you and for them. Now, there are many, many workloads that this applies to that make it very attractive. Now I want to take a look at this concept of cyclical workloads. Now, I'm going to give you a very exaggerated view of a cyclical workload, and I have never seen a real one that is this cyclical, but I have seen workloads that mimic this. It's just not a pure sine wave, which is what I'm going to provide. Now, the idea here is that we have a single process that is running, and you'll see throughout the day, throughout the week, throughout the year, that you have peak times and valleys within this workload. Now again, this varies by provider and it varies by situation, but you'll see these peaks in the valleys throughout the life cycle of an application running on a single VM. Now, in the serverless world, we can take and add another cyclical workload on this series of VMs. And what you see here is that even though we still have peaks in the valleys of a single flow, we've now filled those peaks and valleys with another process such that the peak of one may mimic the valley of another, and we're getting more utilization on this VM pool. Now, if we add yet another workflow, you will see that we've reduced the time where we're not under peak workload and we've reduced the time where we're not under a valley to be as small as possible. Now, obviously, a VM is not going to just run three serverless functions. Ideally it's going to run hundreds or thousands in a pool of VMs and this allows them to get maximum throughput across the workload the entire time. Now, you may step back and question this whole idea of cyclical workloads, but I encourage you to take a look at what actually happens on your services. If you run chron operations, you'll see that they peak and then they go dormant for a period of time. You can see this same utilization across industries. Look at something like e-commerce, where during the holiday shopping season, you see a ramp up in technology usage and then a ramp down after the holiday season is over. And what often happens, in a traditional environment where you're running VMs is that you will actually scale your entire infrastructure for the peaks, so that you can handle the peak workload. And the rest of the time when you're in the valley it becomes wasted space. So this is where serverless can start to offer some value as we maximize the utilization on peaks and decrease the utilization on these valleys.

Benefits of serverless
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] We have now seen how serverless works at a very high level, so let's take a look at why one would want to do this. First and foremost is cost savings. With serverless, you get the ability to scale based on your need, but instead of scaling for your highest demand, you're scaling by the operation and you're paying for just the operation. And by doing so, you are not paying for your peak usage throughout the year as I previously stated. You also get benefits from reduced maintenance. So even though a VM may be in the cloud, you still have maintenance that you have to do on it. You've got to upgrade your VM, you've got to patch components and with serverless, you simply deploy your code. You don't do any of those maintenance activities on your VM itself. Now, separation is another key benefit of using serverless. There is no longer a need to cram disjointed operations into a single VM in order to maximize its value. You get some security and permissions benefits as well because usually with serverless, these functions are operating in their own little sandbox and that sandbox has its own security and its own permissions associated with it, and you're not crossing threads with other functions that may be operating in the same VM pool. Again, something that you don't get when you're trying to cram multiple operations into a single VM pool itself. Another benefit of this separation is it solves many of the async patterns that we have by reducing the waste associated with them. Let me give you an example. Traditionally, when we write a message listener, we run that on a server that is actively pulling against the messaging system to determine if there is a message to act on. And in doing so, it's using resources, but there's a lot of waste associated with that. With serverless, we actually trigger the serverless function based on the presence of a message, and this again reduces our waste, but allows us a just in time operation that reduces overall cost since we're only paying for the operation itself. Now, let's talk a little bit more about asynchronous operations, because most, if not all serverless functions tend to end up being some form of an asynchronous process. Even if it's running a website, a lot of times we will front that with some sort of CDN that actually makes the operation itself async. Now, not always, but async is a very strong pattern and it's worth talking about. The serverless functions can be a key player in asynchronous operations, especially as you move into a public cloud ecosystem. Again, it reduces the waste of waiting. That waiting operation is nothing but a wasteful event when you're dealing with operations of scale on a VM consumption model. It's also much easier to isolate and respond in a serverless function type behavior because each listener will respond based on a function itself as opposed to a listener pool that responds to many different queues. It also offers a lot of flexibility in that you can scale multiple operations within a serverless function by chaining them together instead of tying them all to individual listeners. A lot of times we would do things in async where the first listener would do some operation and put a message back on a queue so that another listener could pick it up. Well, with serverless functions, we can simply trigger the next in the chain without the intermediate cue that was really a fancy way of doing piping across a large scale operation. Now, let's talk about some of the ilities that we get to solve with serverless functions. The first is scalability. Because we're dealing with an individual function and we're paying for the operation of that function, we can have as many of them running at any given point in time that we need to, and each of them is usually within the cloud provider isolated into its own run time and own sandbox, even if it's the same function running 10, 20, 30 times in a given period of time. Serverless functions also give us maintainability. Because each distinct function only does one task, we're not deploying a pool of operations to do many different tasks. This increases our maintainability because when we have to modify an individual function, we're only impacting that function, of course, reducing the risk associated with regressions across a pool of operations. Serverless functions tend to be very easy to deploy, especially within cloud operations. Because of their ease in deployability and maintainability, they give us a speed to market function that comes along with it, and that's another ability that kind of comes behind the scenes from maintainability and deployability. And obviously with serverless functions, you get a lot of flexibility. You can do just about anything you can imagine in a serverless function, and by only paying for the operation, you get all of the benefits without a lot of the pain that is associated with deploying large scale operations to handle a multitude of single functions.

